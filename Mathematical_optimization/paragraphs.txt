Inmathematics,computerscienceandoperationsresearch,mathematicaloptimizationormathematicalprogramming,alternativelyspelledoptimisation,istheselectionofabestelement(withregardtosomecriterion)fromsomesetofavailablealternatives.[1]\nInthesimplestcase,anoptimizationproblemconsistsofmaximizingorminimizingarealfunctionbysystematicallychoosinginputvaluesfromwithinanallowedsetandcomputingthevalueofthefunction.Thegeneralizationofoptimizationtheoryandtechniquestootherformulationsconstitutesalargeareaofappliedmathematics.Moregenerally,optimizationincludesfinding"bestavailable"valuesofsomeobjectivefunctiongivenadefineddomain(orinput),includingavarietyofdifferenttypesofobjectivefunctionsanddifferenttypesofdomains.\nAnoptimizationproblemcanberepresentedinthefollowingway:\nSuchaformulationiscalledanoptimizationproblemoramathematicalprogrammingproblem(atermnotdirectlyrelatedtocomputerprogramming,butstillinuseforexampleinlinearprogramming\xe2\x80\x93seeHistorybelow).Manyreal-worldandtheoreticalproblemsmaybemodeledinthisgeneralframework.Problemsformulatedusingthistechniqueinthefieldsofphysicsandcomputervisionmayrefertothetechniqueasenergyminimization,speakingofthevalueofthefunction\n\n\n\nf\n\n\n{\\displaystylef}\n\nasrepresentingtheenergyofthesystembeingmodeled.\nTypically,\n\n\n\nA\n\n\n{\\displaystyleA}\n\nissomesubsetoftheEuclideanspace\n\n\n\n\n\nR\n\n\nn\n\n\n\n\n{\\displaystyle\\mathbb{R}^{n}}\n\n,oftenspecifiedbyasetofconstraints,equalitiesorinequalitiesthatthemembersof\n\n\n\nA\n\n\n{\\displaystyleA}\n\nhavetosatisfy.Thedomain\n\n\n\nA\n\n\n{\\displaystyleA}\n\nof\n\n\n\nf\n\n\n{\\displaystylef}\n\niscalledthesearchspaceorthechoiceset,\nwhiletheelementsof\n\n\n\nA\n\n\n{\\displaystyleA}\n\narecalledcandidatesolutionsorfeasiblesolutions.\nThefunction\n\n\n\nf\n\n\n{\\displaystylef}\n\niscalled,variously,anobjectivefunction,alossfunctionorcostfunction(minimization),[2]autilityfunctionorfitnessfunction(maximization),or,incertainfields,anenergyfunctionorenergyfunctional.Afeasiblesolutionthatminimizes(ormaximizes,ifthatisthegoal)theobjectivefunctioniscalledanoptimalsolution.\nInmathematics,conventionaloptimizationproblemsareusuallystatedintermsofminimization.\nAlocalminimum\n\n\n\n\n\n\nx\n\n\n∗\n\n\n\n\n{\\displaystyle\\mathbf{x}^{\\ast}}\n\n\nisdefinedasanelementforwhichthereexistssome\n\n\n\nδ\n>\n0\n\n\n{\\displaystyle\\delta>0}\n\nsuchthat\nthatistosay,onsomeregionaround\n\n\n\n\n\n\nx\n\n\n∗\n\n\n\n\n{\\displaystyle\\mathbf{x}^{\\ast}}\n\n\nallofthefunctionvaluesaregreaterthanorequaltothevalueatthatelement.\nLocalmaximaaredefinedsimilarly.\nWhilealocalminimumisatleastasgoodasanynearbyelements,aglobalminimumisatleastasgoodaseveryfeasibleelement.\nGenerally,unlessboththeobjectivefunctionandthefeasibleregionareconvexinaminimizationproblem,theremaybeseverallocalminima.\nInaconvexproblem,ifthereisalocalminimumthatisinterior(notontheedgeofthesetoffeasibleelements),itisalsotheglobalminimum,butanonconvexproblemmayhavemorethanonelocalminimumnotallofwhichneedbeglobalminima.\nAlargenumberofalgorithmsproposedforsolvingnonconvexproblems\xe2\x80\x93includingthemajorityofcommerciallyavailablesolvers\xe2\x80\x93arenotcapableofmakingadistinctionbetweenlocallyoptimalsolutionsandgloballyoptimalsolutions,andwilltreattheformerasactualsolutionstotheoriginalproblem.Globaloptimizationisthebranchofappliedmathematicsandnumericalanalysisthatisconcernedwiththedevelopmentofdeterministicalgorithmsthatarecapableofguaranteeingconvergenceinfinitetimetotheactualoptimalsolutionofanonconvexproblem.\nOptimizationproblemsareoftenexpressedwithspecialnotation.Herearesomeexamples:\nConsiderthefollowingnotation:\nThisdenotestheminimumvalueoftheobjectivefunction\n\n\n\n\nx\n\n2\n\n\n+\n1\n\n\n{\\displaystylex^{2}+1}\n\n,whenchoosingxfromthesetofrealnumbers\n\n\n\n\nR\n\n\n\n{\\displaystyle\\mathbb{R}}\n\n.Theminimumvalueinthiscaseis\n\n\n\n1\n\n\n{\\displaystyle1}\n\n,occurringat\n\n\n\nx\n=\n0\n\n\n{\\displaystylex=0}\n\n.\nSimilarly,thenotation\nasksforthemaximumvalueoftheobjectivefunction2x,wherexmaybeanyrealnumber.Inthiscase,thereisnosuchmaximumastheobjectivefunctionisunbounded,sotheansweris"infinity"or"undefined".\nConsiderthefollowingnotation:\norequivalently\nThisrepresentsthevalue(orvalues)oftheargument\n\n\n\nx\n\n\n{\\displaystylex}\n\nintheinterval\n\n\n\n(\n−\n∞\n,\n−\n1\n]\n\n\n{\\displaystyle(-\\infty,-1]}\n\nthatminimizes(orminimize)theobjectivefunction\n\n\n\n\nx\n\n2\n\n\n+\n1\n\n\n{\\displaystylex^{2}+1}\n\n(theactualminimumvalueofthatfunctionisnotwhattheproblemasksfor).Inthiscase,theansweris\n\n\n\nx\n=\n−\n1\n\n\n{\\displaystylex=-1}\n\n,since\n\n\n\nx\n=\n0\n\n\n{\\displaystylex=0}\n\nisinfeasible,i.e.doesnotbelongtothefeasibleset.\nSimilarly,\norequivalently\nrepresentsthe\n\n\n\n(\nx\n,\ny\n)\n\n\n{\\displaystyle(x,y)}\n\npair(orpairs)thatmaximizes(ormaximize)thevalueoftheobjectivefunction\n\n\n\nx\ncos\n⁡\n(\ny\n)\n\n\n{\\displaystylex\\cos(y)}\n\n,withtheaddedconstraintthat\n\n\n\nx\n\n\n{\\displaystylex}\n\nlieintheinterval\n\n\n\n[\n−\n5\n,\n5\n]\n\n\n{\\displaystyle[-5,5]}\n\n(again,theactualmaximumvalueoftheexpressiondoesnotmatter).Inthiscase,thesolutionsarethepairsoftheform\n\n\n\n(\n5\n,\n\n2\nk\nπ\n)\n\n\n{\\displaystyle(5,\\,2k\\pi)}\n\nand\n\n\n\n(\n−\n5\n,\n\n(\n2\nk\n+\n1\n)\nπ\n)\n\n\n{\\displaystyle(-5,\\,(2k+1)\\pi)}\n\n,where\n\n\n\nk\n\n\n{\\displaystylek}\n\nrangesoverallintegers.\nOperators\n\n\n\n\na\nr\ng\n\nm\ni\nn\n\n\n\n{\\displaystyle\\operatorname{arg\\,min}}\n\nand\n\n\n\n\na\nr\ng\n\nm\na\nx\n\n\n\n{\\displaystyle\\operatorname{arg\\,max}}\n\naresometimesalsowrittenas\n\n\n\nargmin\n\n\n{\\displaystyle\\operatorname{argmin}}\n\nand\n\n\n\nargmax\n\n\n{\\displaystyle\\operatorname{argmax}}\n\n,andstandforargumentoftheminimumandargumentofthemaximum.\nFermatandLagrangefoundcalculus-basedformulaeforidentifyingoptima,whileNewtonandGaussproposediterativemethodsformovingtowardsanoptimum.\nTheterm"linearprogramming"forcertainoptimizationcaseswasduetoGeorge B.Dantzig,althoughmuchofthetheoryhadbeenintroducedbyLeonidKantorovichin1939.(Programminginthiscontextdoesnotrefertocomputerprogramming,butfromtheuseofprogrambytheUnitedStatesmilitarytorefertoproposedtrainingandlogisticsschedules,whichweretheproblemsDantzigstudiedatthattime.)DantzigpublishedtheSimplexalgorithmin1947,andJohnvonNeumanndevelopedthetheoryofdualityinthesameyear.\nOthermajorresearchersinmathematicaloptimizationincludethefollowing:\nInanumberofsubfields,thetechniquesaredesignedprimarilyforoptimizationindynamiccontexts(thatis,decisionmakingovertime):\nAddingmorethanoneobjectivetoanoptimizationproblemaddscomplexity.Forexample,tooptimizeastructuraldesign,onewoulddesireadesignthatisbothlightandrigid.Whentwoobjectivesconflict,atrade-offmustbecreated.Theremaybeonelightestdesign,onestiffestdesign,andaninfinitenumberofdesignsthataresomecompromiseofweightandrigidity.Thesetoftrade-offdesignsthatcannotbeimproveduponaccordingtoonecriterionwithouthurtinganothercriterionisknownastheParetoset.ThecurvecreatedplottingweightagainststiffnessofthebestdesignsisknownastheParetofrontier.\nAdesignisjudgedtobe"Paretooptimal"(equivalently,"Paretoefficient"orintheParetoset)ifitisnotdominatedbyanyotherdesign:Ifitisworsethananotherdesigninsomerespectsandnobetterinanyrespect,thenitisdominatedandisnotParetooptimal.\nThechoiceamong"Paretooptimal"solutionstodeterminethe"favoritesolution"isdelegatedtothedecisionmaker.Inotherwords,definingtheproblemasmulti-objectiveoptimizationsignalsthatsomeinformationismissing:desirableobjectivesaregivenbutcombinationsofthemarenotratedrelativetoeachother.Insomecases,themissinginformationcanbederivedbyinteractivesessionswiththedecisionmaker.\nMulti-objectiveoptimizationproblemshavebeengeneralizedfurtherintovectoroptimizationproblemswherethe(partial)orderingisnolongergivenbytheParetoordering.\nOptimizationproblemsareoftenmulti-modal;thatis,theypossessmultiplegoodsolutions.Theycouldallbegloballygood(samecostfunctionvalue)ortherecouldbeamixofgloballygoodandlocallygoodsolutions.Obtainingall(oratleastsomeof)themultiplesolutionsisthegoalofamulti-modaloptimizer.\nClassicaloptimizationtechniquesduetotheiriterativeapproachdonotperformsatisfactorilywhentheyareusedtoobtainmultiplesolutions,sinceitisnotguaranteedthatdifferentsolutionswillbeobtainedevenwithdifferentstartingpointsinmultiplerunsofthealgorithm.Evolutionaryalgorithms,however,areaverypopularapproachtoobtainmultiplesolutionsinamulti-modaloptimizationtask.\nThesatisfiabilityproblem,alsocalledthefeasibilityproblem,isjusttheproblemoffindinganyfeasiblesolutionatallwithoutregardtoobjectivevalue.Thiscanberegardedasthespecialcaseofmathematicaloptimizationwheretheobjectivevalueisthesameforeverysolution,andthusanysolutionisoptimal.\nManyoptimizationalgorithmsneedtostartfromafeasiblepoint.Onewaytoobtainsuchapointistorelaxthefeasibilityconditionsusingaslackvariable;withenoughslack,anystartingpointisfeasible.Then,minimizethatslackvariableuntilslackisnullornegative.\nTheextremevaluetheoremofKarlWeierstrassstatesthatacontinuousreal-valuedfunctiononacompactsetattainsitsmaximumandminimumvalue.Moregenerally,alowersemi-continuousfunctiononacompactsetattainsitsminimum;anuppersemi-continuousfunctiononacompactsetattainsitsmaximum.\nOneofFermat\'stheoremsstatesthatoptimaofunconstrainedproblemsarefoundatstationarypoints,wherethefirstderivativeorthegradientoftheobjectivefunctioniszero(seefirstderivativetest).Moregenerally,theymaybefoundatcriticalpoints,wherethefirstderivativeorgradientoftheobjectivefunctioniszeroorisundefined,orontheboundaryofthechoiceset.Anequation(orsetofequations)statingthatthefirstderivative(s)equal(s)zeroataninterioroptimumiscalleda\'first-ordercondition\'orasetoffirst-orderconditions.\nOptimaofequality-constrainedproblemscanbefoundbytheLagrangemultipliermethod.Theoptimaofproblemswithequalityand/orinequalityconstraintscanbefoundusingthe\'Karush\xe2\x80\x93Kuhn\xe2\x80\x93Tuckerconditions\'.\nWhilethefirstderivativetestidentifiespointsthatmightbeextrema,thistestdoesnotdistinguishapointthatisaminimumfromonethatisamaximumoronethatisneither.Whentheobjectivefunctionistwicedifferentiable,thesecasescanbedistinguishedbycheckingthesecondderivativeorthematrixofsecondderivatives(calledtheHessianmatrix)inunconstrainedproblems,orthematrixofsecondderivativesoftheobjectivefunctionandtheconstraintscalledtheborderedHessianinconstrainedproblems.Theconditionsthatdistinguishmaxima,orminima,fromotherstationarypointsarecalled\'second-orderconditions\'(see\'Secondderivativetest\').Ifacandidatesolutionsatisfiesthefirst-orderconditions,thensatisfactionofthesecond-orderconditionsaswellissufficienttoestablishatleastlocaloptimality.\nTheenvelopetheoremdescribeshowthevalueofanoptimalsolutionchangeswhenanunderlyingparameterchanges.Theprocessofcomputingthischangeiscalledcomparativestatics.\nThemaximumtheoremofClaudeBerge(1963)describesthecontinuityofanoptimalsolutionasafunctionofunderlyingparameters.\nForunconstrainedproblemswithtwice-differentiablefunctions,somecriticalpointscanbefoundbyfindingthepointswherethegradientoftheobjectivefunctioniszero(thatis,thestationarypoints).Moregenerally,azerosubgradientcertifiesthatalocalminimumhasbeenfoundforminimizationproblemswithconvexfunctionsandotherlocallyLipschitzfunctions.\nFurther,criticalpointscanbeclassifiedusingthedefinitenessoftheHessianmatrix:IftheHessianispositivedefiniteatacriticalpoint,thenthepointisalocalminimum;iftheHessianmatrixisnegativedefinite,thenthepointisalocalmaximum;finally,ifindefinite,thenthepointissomekindofsaddlepoint.\nConstrainedproblemscanoftenbetransformedintounconstrainedproblemswiththehelpofLagrangemultipliers.Lagrangianrelaxationcanalsoprovideapproximatesolutionstodifficultconstrainedproblems.\nWhentheobjectivefunctionisconvex,thenanylocalminimumwillalsobeaglobalminimum.Thereexistefficientnumericaltechniquesforminimizingconvexfunctions,suchasinterior-pointmethods.\nTosolveproblems,researchersmayusealgorithmsthatterminateinafinitenumberofsteps,oriterativemethodsthatconvergetoasolution(onsomespecifiedclassofproblems),orheuristicsthatmayprovideapproximatesolutionstosomeproblems(althoughtheiriteratesneednotconverge).\nOptimizationalgorithmmachinelearning\nIntroduction\n\nAnoptimizationalgorithmisaprocedurewhichisexecutediterativelybycomparingvarioussolutionstillanoptimumorasatisfactorysolutionwillbefound.OptimizationalgorithmshelpustominimizeormaximizeanobjectivefunctionE(x)whichissimplyamathematicalfunctiondependentontheModel\xe2\x80\x99sinternalparameterswhichareusedincomputingthetargetvalues(Y)fromthesetofpredictors(X)usedinthemodel.TherearetwotypesofoptimizationalgorithmswhicharewidelyusedsuchasZero-orderalgorithms,FirstOrderOptimizationAlgorithmsandSecondOrderOptimizationAlgorithms.[3]\nZero-orderalgorithms[4]\nZero-order(orderivative-free)algorithmsuseonlythecriterionvalueatsomepositions.ItispopularwhenthegradientandHessianinformationaredifficulttoobtain,e.g.,noexplicitfunctionformsaregiven.[5]\n\nFirstOrderOptimizationAlgorithms[6]\nThesealgorithmsminimizeormaximizeaLossfunctionE(x)usingitsGradientvalueswithrespecttotheparameters.MostwidelyusedFirstorderoptimizationalgorithmisGradientDescent.TheFirstorderderivativedisplayswhetherthefunctionisdecreasingorincreasingataparticularpoint.FirstorderDerivativebasicallywillprovideusalinewhichistangentialtoapointonitsErrorSurface.[7]\nExample\nGradientdescent\nItisafirstorderoptimizationalgorithmforfindingtheminimumofafunction.\n\xce\xb8=\xce\xb8\xe2\x88\x92\xce\xb7\xe2\x8b\x85\xe2\x88\x87J(\xce\xb8)\xe2\x80\x8a\xe2\x80\x94it\xe2\x80\x8aistheformulaoftheparameterupdates,where\xe2\x80\x98\xce\xb7\xe2\x80\x99isthelearningrate,\xe2\x80\x99\xe2\x88\x87J(\xce\xb8)\xe2\x80\x99istheGradientofLossfunction-J(\xce\xb8)w.r.tparameters-\xe2\x80\x98\xce\xb8\xe2\x80\x99.\nItisthemostpopularOptimizationalgorithmsusedinoptimizingaNeuralNetwork.NowgradientdescentismajorlyusedtodoWeightsupdatesinaNeuralNetworkModel,i.eupdateandtunetheModel\xe2\x80\x99sparametersinadirectionsothatwecanminimizetheLossfunction.NowweallknowaNeuralNetworktrainsviaafamoustechniquecalledBack-propagation,inwhichpropagatingforwardcalculatingthedotproductofInputssignalsandtheircorrespondingWeightsandthenapplyingaactivationfunctiontothosesumofproducts,whichtransformstheinputsignaltoanoutputsignalandalsoisimportanttomodelcomplexNon-linearfunctionsandintroducesNon-linearitytotheModelwhichenablestheModeltolearnalmostanyarbitraryfunctionalmapping.[8]\n\nSecondOrderOptimizationAlgorithms[9]\nSecond-ordermethodsusethesecondorderderivativewhichisalsocalledHessiantominimizeormaximizethelossfunction.TheHessianisaMatrixofSecondOrderPartialDerivatives.Sincethesecondderivativeiscostlytocompute,thesecondorderisnotusedmuch.Thesecondorderderivativeinformsuswhetherthefirstderivativeisincreasingordecreasingwhichhintsatthefunction\xe2\x80\x99scurvature.ItalsoprovidesuswithaquadraticsurfacewhichtouchesthecurvatureoftheErrorSurface.[10]\nTheiterativemethodsusedtosolveproblemsofnonlinearprogrammingdifferaccordingtowhethertheyevaluateHessians,gradients,oronlyfunctionvalues.WhileevaluatingHessians(H)andgradients(G)improvestherateofconvergence,forfunctionsforwhichthesequantitiesexistandvarysufficientlysmoothly,suchevaluationsincreasethecomputationalcomplexity(orcomputationalcost)ofeachiteration.Insomecases,thecomputationalcomplexitymaybeexcessivelyhigh.\nOnemajorcriterionforoptimizersisjustthenumberofrequiredfunctionevaluationsasthisoftenisalreadyalargecomputationaleffort,usuallymuchmoreeffortthanwithintheoptimizeritself,whichmainlyhastooperateovertheNvariables.\nThederivativesprovidedetailedinformationforsuchoptimizers,butareevenhardertocalculate,e.g.approximatingthegradienttakesatleastN+1functionevaluations.Forapproximationsofthe2ndderivatives(collectedintheHessianmatrix)thenumberoffunctionevaluationsisintheorderofN\xc2\xb2.Newton\'smethodrequiresthe2ndorderderivates,soforeachiterationthenumberoffunctioncallsisintheorderofN\xc2\xb2,butforasimplerpuregradientoptimizeritisonlyN.However,gradientoptimizersneedusuallymoreiterationsthanNewton\'salgorithm.Whichoneisbestwithrespecttothenumberoffunctioncallsdependsontheproblemitself.\nMoregenerally,iftheobjectivefunctionisnotaquadraticfunction,thenmanyoptimizationmethodsuseothermethodstoensurethatsomesubsequenceofiterationsconvergestoanoptimalsolution.Thefirstandstillpopularmethodforensuringconvergencereliesonlinesearches,whichoptimizeafunctionalongonedimension.Asecondandincreasinglypopularmethodforensuringconvergenceusestrustregions.Bothlinesearchesandtrustregionsareusedinmodernmethodsofnon-differentiableoptimization.Usuallyaglobaloptimizerismuchslowerthanadvancedlocaloptimizers(suchasBFGS),sooftenanefficientglobaloptimizercanbeconstructedbystartingthelocaloptimizerfromdifferentstartingpoints.\nBesides(finitelyterminating)algorithmsand(convergent)iterativemethods,thereareheuristics.Aheuristicisanyalgorithmwhichisnotguaranteed(mathematically)tofindthesolution,butwhichisneverthelessusefulincertainpracticalsituations.Listofsomewell-knownheuristics:\nProblemsinrigidbodydynamics(inparticulararticulatedrigidbodydynamics)oftenrequiremathematicalprogrammingtechniques,sinceyoucanviewrigidbodydynamicsasattemptingtosolveanordinarydifferentialequationonaconstraintmanifold[12];theconstraintsarevariousnonlineargeometricconstraintssuchas"thesetwopointsmustalwayscoincide","thissurfacemustnotpenetrateanyother",or"thispointmustalwaysliesomewhereonthiscurve".Also,theproblemofcomputingcontactforcescanbedonebysolvingalinearcomplementarityproblem,whichcanalsobeviewedasaQP(quadraticprogramming)problem.\nManydesignproblemscanalsobeexpressedasoptimizationprograms.Thisapplicationiscalleddesignoptimization.Onesubsetistheengineeringoptimization,andanotherrecentandgrowingsubsetofthisfieldismultidisciplinarydesignoptimization,which,whileusefulinmanyproblems,hasinparticularbeenappliedtoaerospaceengineeringproblems.\nThisapproachmaybeappliedincosmologyandastrophysics.[13]\nEconomicsiscloselyenoughlinkedtooptimizationofagentsthataninfluentialdefinitionrelatedlydescribeseconomicsquascienceasthe"studyofhumanbehaviorasarelationshipbetweenendsandscarcemeans"withalternativeuses.[14]Modernoptimizationtheoryincludestraditionaloptimizationtheorybutalsooverlapswithgametheoryandthestudyofeconomicequilibria.TheJournalofEconomicLiteraturecodesclassifymathematicalprogramming,optimizationtechniques,andrelatedtopicsunderJEL:C61-C63.\nInmicroeconomics,theutilitymaximizationproblemanditsdualproblem,theexpenditureminimizationproblem,areeconomicoptimizationproblems.Insofarastheybehaveconsistently,consumersareassumedtomaximizetheirutility,whilefirmsareusuallyassumedtomaximizetheirprofit.Also,agentsareoftenmodeledasbeingrisk-averse,therebypreferringtoavoidrisk.Assetpricesarealsomodeledusingoptimizationtheory,thoughtheunderlyingmathematicsreliesonoptimizingstochasticprocessesratherthanonstaticoptimization.Internationaltradetheoryalsousesoptimizationtoexplaintradepatternsbetweennations.Theoptimizationofportfoliosisanexampleofmulti-objectiveoptimizationineconomics.\nSincethe1970s,economistshavemodeleddynamicdecisionsovertimeusingcontroltheory.[15]Forexample,dynamicsearchmodelsareusedtostudylabor-marketbehavior.[16]Acrucialdistinctionisbetweendeterministicandstochasticmodels.[17]Macroeconomistsbuilddynamicstochasticgeneralequilibrium(DSGE)modelsthatdescribethedynamicsofthewholeeconomyastheresultoftheinterdependentoptimizingdecisionsofworkers,consumers,investors,andgovernments.[18][19]\nSomecommonapplicationsofoptimizationtechniquesinelectricalengineeringincludeactivefilterdesign,[20]strayfieldreductioninsuperconductingmagneticenergystoragesystems,spacemappingdesignofmicrowavestructures,[21]handsetantennas,[22][23][24]electromagnetics-baseddesign.Electromagneticallyvalidateddesignoptimizationofmicrowavecomponentsandantennashasmadeextensiveuseofanappropriatephysics-basedorempiricalsurrogatemodelandspacemappingmethodologiessincethediscoveryofspacemappingin1993.[25][26]\nOptimizationhasbeenwidelyusedincivilengineering.Themostcommoncivilengineeringproblemsthataresolvedbyoptimizationarecutandfillofroads,life-cycleanalysisofstructuresandinfrastructures,[27]resourceleveling[28]andscheduleoptimization.\nAnotherfieldthatusesoptimizationtechniquesextensivelyisoperationsresearch.[29]Operationsresearchalsousesstochasticmodelingandsimulationtosupportimproveddecision-making.Increasingly,operationsresearchusesstochasticprogrammingtomodeldynamicdecisionsthatadapttoevents;suchproblemscanbesolvedwithlarge-scaleoptimizationandstochasticoptimizationmethods.\nMathematicaloptimizationisusedinmuchmoderncontrollerdesign.High-levelcontrollerssuchasmodelpredictivecontrol(MPC)orreal-timeoptimization(RTO)employmathematicaloptimization.Thesealgorithmsrunonlineandrepeatedlydeterminevaluesfordecisionvariables,suchaschokeopeningsinaprocessplant,byiterativelysolvingamathematicaloptimizationproblemincludingconstraintsandamodelofthesystemtobecontrolled.\nOptimizationtechniquesareregularlyusedingeophysicalparameterestimationproblems.Givenasetofgeophysicalmeasurements,e.g.seismicrecordings,itiscommontosolveforthephysicalpropertiesandgeometricalshapesoftheunderlyingrocksandfluids.\nNonlinearoptimizationmethodsarewidelyusedinconformationalanalysis.\nOptimizationtechniquesareusedinmanyfacetsofcomputationalsystemsbiologysuchasmodelbuilding,optimalexperimentaldesign,metabolicengineering,andsyntheticbiology[30].Linearprogramminghasbeenappliedtocalculatethemaximalpossibleyieldsoffermentationproducts[31],andtoinfergeneregulatorynetworksfrommultiplemicroarraydatasets[32]aswellastranscriptionalregulatorynetworksfromhigh-throughputdata[33].Nonlinearprogramminghasbeenusedtoanalyzeenergymetabolism[34]andhasbeenappliedtometabolicengineeringandparameterestimationinbiochemicalpathways[35].\nMethodstoobtainsuitable(insomesense)naturalextensionsofoptimizationproblemsthatotherwiselackofexistenceorstabilityofsolutionstoobtainproblemswithguaranteedexistenceofsolutionsandtheirstabilityinsomesense(typicallyundervariousperturbationofdata)areingeneralcalledrelaxation.Solutionsofsuchextended(=relaxed)problemsinsomesensecharacterizes(atleastcertainfeatures)oftheoriginalproblems,e.g.asfarastheiroptimizingsequencesconcerns.Relaxedproblemsmayalsopossessestheirownnaturallinearstructurethatmayyieldspecificoptimalityconditionsdifferentfromoptimalityconditionsfortheoriginalproblems.\n