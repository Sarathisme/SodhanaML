Cross-validation,sometimescalledrotationestimation,[1][2][3]orout-of-sampletestingisanyofvarioussimilarmodelvalidationtechniquesforassessinghowtheresultsofastatisticalanalysiswillgeneralizetoanindependentdataset.Itismainlyusedinsettingswherethegoalisprediction,andonewantstoestimatehowaccuratelyapredictivemodelwillperforminpractice.Inapredictionproblem,amodelisusuallygivenadatasetofknowndataonwhichtrainingisrun(trainingdataset),andadatasetofunknowndata(orfirstseendata)againstwhichthemodelistested(calledthevalidationdatasetortestingset).[4],[5]Thegoalofcross-validationistotestthemodel\xe2\x80\x99sabilitytopredictnewdatathatwasnotusedinestimatingit,inordertoflagproblemslikeoverfittingorselectionbias[6]andtogiveaninsightonhowthemodelwillgeneralizetoanindependentdataset(i.e.,anunknowndataset,forinstancefromarealproblem).\nOneroundofcross-validationinvolvespartitioningasampleofdataintocomplementarysubsets,performingtheanalysisononesubset(calledthetrainingset),andvalidatingtheanalysisontheothersubset(calledthevalidationsetortestingset).Toreducevariability,inmostmethodsmultipleroundsofcross-validationareperformedusingdifferentpartitions,andthevalidationresultsarecombined(e.g.averaged)overtheroundstogiveanestimateofthemodel\xe2\x80\x99spredictiveperformance.\nInsummary,cross-validationcombines(averages)measuresoffitnessinpredictiontoderiveamoreaccurateestimateofmodelpredictionperformance.[7]\nSupposewehaveamodelwithoneormoreunknownparameters,andadatasettowhichthemodelcanbefit(thetrainingdataset).Thefittingprocessoptimizesthemodelparameterstomakethemodelfitthetrainingdataaswellaspossible.Ifwethentakeanindependentsampleofvalidationdatafromthesamepopulationasthetrainingdata,itwillgenerallyturnoutthatthemodeldoesnotfitthevalidationdataaswellasitfitsthetrainingdata.Thesizeofthisdifferenceislikelytobelargeespeciallywhenthesizeofthetrainingdatasetissmall,orwhenthenumberofparametersinthemodelislarge.Cross-validationisawaytoestimatethesizeofthiseffect.\nInlinearregressionwehaverealresponsevaluesy1,...,yn,andnp-dimensionalvectorcovariatesx1,...,xn.Thecomponentsofthevectorxiaredenotedxi1,...,xip.Ifweuseleastsquarestofitafunctionintheformofahyperplaney=a+\xce\xb2Txtothedata(xi,yi) 1 \xe2\x89\xa4 i \xe2\x89\xa4 n,wecouldthenassessthefitusingthemeansquarederror(MSE).TheMSEforgivenestimatedparametervaluesaand\xce\xb2onthetrainingset(xi,yi) 1 \xe2\x89\xa4 i \xe2\x89\xa4 nis\nIfthemodeliscorrectlyspecified,itcanbeshownundermildassumptionsthattheexpectedvalueoftheMSEforthetrainingsetis(n − p − 1)/(n + p + 1) < 1timestheexpectedvalueoftheMSEforthevalidationset[8](theexpectedvalueistakenoverthedistributionoftrainingsets).ThusifwefitthemodelandcomputetheMSEonthetrainingset,wewillgetanoptimisticallybiasedassessmentofhowwellthemodelwillfitanindependentdataset.Thisbiasedestimateiscalledthein-sampleestimateofthefit,whereasthecross-validationestimateisanout-of-sampleestimate.\nSinceinlinearregressionitispossibletodirectlycomputethefactor(n − p − 1)/(n + p + 1)bywhichthetrainingMSEunderestimatesthevalidationMSEundertheassumptionthatthemodelspecificationisvalid,cross-validationcanbeusedforcheckingwhetherthemodelhasbeenoverfitted,inwhichcasetheMSEinthevalidationsetwillsubstantiallyexceeditsanticipatedvalue.(Cross-validationinthecontextoflinearregressionisalsousefulinthatitcanbeusedtoselectanoptimallyregularizedcostfunction).\nInmostotherregressionprocedures(e.g.logisticregression),thereisnosimpleformulatocomputetheexpectedout-of-samplefit.Cross-validationis,thus,agenerallyapplicablewaytopredicttheperformanceofamodelonunavailabledatausingnumericalcomputationinplaceoftheoreticalanalysis.\nTwotypesofcross-validationcanbedistinguished,exhaustiveandnon-exhaustivecross-validation.\nExhaustivecross-validationmethodsarecross-validationmethodswhichlearnandtestonallpossiblewaystodividetheoriginalsampleintoatrainingandavalidationset.\nLeave-p-outcross-validation(LpOCV)involvesusingpobservationsasthevalidationsetandtheremainingobservationsasthetrainingset.Thisisrepeatedonallwaystocuttheoriginalsampleonavalidationsetofpobservationsandatrainingset.\nLpOcross-validationrequirestrainingandvalidatingthemodel\n\n\n\n\nC\n\np\n\n\nn\n\n\n\n\n{\\displaystyleC_{p}^{n}}\n\ntimes,wherenisthenumberofobservationsintheoriginalsample,andwhere\n\n\n\n\nC\n\np\n\n\nn\n\n\n\n\n{\\displaystyleC_{p}^{n}}\n\nisthebinomialcoefficient.Forp>1andforevenmoderatelylargen,LpOCVcanbecomecomputationallyinfeasible.Forexample,withn=100andp=30=30percentof100(assuggestedabove),\n\n\n\n\nC\n\n30\n\n\n100\n\n\n≈\n3\n×\n\n10\n\n25\n\n\n.\n\n\n{\\displaystyleC_{30}^{100}\\approx3\\times10^{25}.}\n\n\nLeave-one-outcross-validation(LOOCV)isaparticularcaseofleave-p-outcross-validationwithp = 1.\nTheprocesslookssimilartojackknife;however,withcross-validationonecomputesastatisticontheleft-outsample(s),whilewithjackknifingonecomputesastatisticfromthekeptsamplesonly.\nLOOcross-validationrequireslesscomputationtimethanLpOcross-validationbecausethereareonly\n\n\n\n\nC\n\n1\n\n\nn\n\n\n=\nn\n\n\n{\\displaystyleC_{1}^{n}=n}\n\npassesratherthan\n\n\n\n\nC\n\nk\n\n\nn\n\n\n\n\n{\\displaystyleC_{k}^{n}}\n\n.However,\n\n\n\nn\n\n\n{\\displaystylen}\n\npassesmaystillrequirequitealargecomputationtime,inwhichcaseotherapproachessuchask-foldcrossvalidationmaybemoreappropriate.\nPseudo-Code-Algorithm:\nInput:\nx,{vectoroflengthNwithx-valuesofdatapoints}\ny,{vectoroflengthNwithy-valuesofdatapoints}\nOutput:\nerr,{estimateforthepredictionerror}\nSteps:\nerr\xe2\x86\x900\nfori\xe2\x86\x901,...,Ndo\n//definethecross-validationsubsets\nx_in\xe2\x86\x90(x[1],...,x[i\xe2\x88\x921],x[i+1],...,x[N])\ny_in\xe2\x86\x90(y[1],...,y[i\xe2\x88\x921],y[i+1],...,y[N]\nx_out\xe2\x86\x90x[i]\ny_out\xe2\x86\x90interpolate(x_in,y_in,x_out,y_out)\nerr\xe2\x86\x90err+(y[i]\xe2\x88\x92y_out)^2\nendfor\nerr\xe2\x86\x90err/N\nNon-exhaustivecrossvalidationmethodsdonotcomputeallwaysofsplittingtheoriginalsample.Thosemethodsareapproximationsofleave-p-outcross-validation.\nInk-foldcross-validation,theoriginalsampleisrandomlypartitionedintokequalsizedsubsamples.\nOftheksubsamples,asinglesubsampleisretainedasthevalidationdatafortestingthemodel,andtheremainingk \xe2\x88\x92 1subsamplesareusedastrainingdata.Thecross-validationprocessisthenrepeatedktimes,witheachoftheksubsamplesusedexactlyonceasthevalidationdata.Thekresultscanthenbeaveragedtoproduceasingleestimation.Theadvantageofthismethodoverrepeatedrandomsub-sampling(seebelow)isthatallobservationsareusedforbothtrainingandvalidation,andeachobservationisusedforvalidationexactlyonce.10-foldcross-validationiscommonlyused,[9]butingeneralkremainsanunfixedparameter.\nForexample,settingk = 2resultsin2-foldcross-validation.In2-foldcross-validation,werandomlyshufflethedatasetintotwosetsd0andd1,sothatbothsetsareequalsize(thisisusuallyimplementedbyshufflingthedataarrayandthensplittingitintwo).Wethentrainond0andvalidateond1,followedbytrainingond1andvalidatingon d0.\nWhenk = n(thenumberofobservations),thek-foldcross-validationisexactlytheleave-one-outcross-validation.\nInstratifiedk-foldcross-validation,thefoldsareselectedsothatthemeanresponsevalueisapproximatelyequalinallthefolds.Inthecaseofbinaryclassification,thismeansthateachfoldcontainsroughlythesameproportionsofthetwotypesofclasslabels.\nIntheholdoutmethod,werandomlyassigndatapointstotwosetsd0andd1,usuallycalledthetrainingsetandthetestset,respectively.Thesizeofeachofthesetsisarbitraryalthoughtypicallythetestsetissmallerthanthetrainingset.Wethentrain(buildamodel)ond0andtest(evaluateitsperformance)ond1.\nIntypicalcross-validation,resultsofmultiplerunsofmodel-testingareaveragedtogether;incontrast,theholdoutmethod,inisolation,involvesasinglerun.Itshouldbeusedwithcautionbecausewithoutsuchaveragingofmultipleruns,onemayachievehighlymisleadingresults.One\'sindicatorofpredictiveaccuracy(F*),asnotedbelow,willtendtobeunstablesinceitwillnotbesmoothedoutbymultipleiterations.Similarly,indicatorsofthespecificroleplayedbyvariouspredictorvariables(e.g.,valuesofregressioncoefficients)willtendtobeunstable.\nWhiletheholdoutmethodcanbeframedas"thesimplestkindofcross-validation",[10]manysourcesinsteadclassifyholdoutasatypeofsimplevalidation,ratherthanasimpleordegenerateformofcross-validation.[2][11]\nThismethod,alsoknownasMonteCarlocross-validation,[12]randomlysplitsthedatasetintotrainingandvalidationdata.Foreachsuchsplit,themodelisfittothetrainingdata,andpredictiveaccuracyisassessedusingthevalidationdata.Theresultsarethenaveragedoverthesplits.Theadvantageofthismethod(overk-foldcrossvalidation)isthattheproportionofthetraining/validationsplitisnotdependentonthenumberofiterations(folds).Thedisadvantageofthismethodisthatsomeobservationsmayneverbeselectedinthevalidationsubsample,whereasothersmaybeselectedmorethanonce.Inotherwords,validationsubsetsmayoverlap.ThismethodalsoexhibitsMonteCarlovariation,meaningthattheresultswillvaryiftheanalysisisrepeatedwithdifferentrandomsplits.\nAsthenumberofrandomsplitsapproachesinfinity,theresultofrepeatedrandomsub-samplingvalidationtendstowardsthatofleave-p-outcross-validation.\nInastratifiedvariantofthisapproach,therandomsamplesaregeneratedinsuchawaythatthemeanresponsevalue(i.e.thedependentvariableintheregression)isequalinthetrainingandtestingsets.Thisisparticularlyusefuliftheresponsesaredichotomouswithanunbalancedrepresentationofthetworesponsevaluesinthedata.\nThegoalofcross-validationistoestimatetheexpectedleveloffitofamodeltoadatasetthatisindependentofthedatathatwereusedtotrainthemodel.Itcanbeusedtoestimateanyquantitativemeasureoffitthatisappropriateforthedataandmodel.Forexample,forbinaryclassificationproblems,eachcaseinthevalidationsetiseitherpredictedcorrectlyorincorrectly.Inthissituationthemisclassificationerrorratecanbeusedtosummarizethefit,althoughothermeasureslikepositivepredictivevaluecouldalsobeused.Whenthevaluebeingpredictediscontinuouslydistributed,themeansquarederror,rootmeansquarederrorormedianabsolutedeviationcouldbeusedtosummarizetheerrors.\nSupposewechooseameasureoffitF,andusecross-validationtoproduceanestimateF*oftheexpectedfitEFofamodeltoanindependentdatasetdrawnfromthesamepopulationasthetrainingdata.Ifweimaginesamplingmultipleindependenttrainingsetsfollowingthesamedistribution,theresultingvaluesforF*willvary.ThestatisticalpropertiesofF*resultfromthisvariation.\nThecross-validationestimatorF*isverynearlyunbiasedforEF[13][citationneeded].Thereasonthatitisslightlybiasedisthatthetrainingsetincross-validationisslightlysmallerthantheactualdataset(e.g.forLOOCVthetrainingsetsizeisn − 1whentherearenobservedcases).Innearlyallsituations,theeffectofthisbiaswillbeconservativeinthattheestimatedfitwillbeslightlybiasedinthedirectionsuggestingapoorerfit.Inpractice,thisbiasisrarelyaconcern.\nThevarianceofF*canbelarge.[14][15]Forthisreason,iftwostatisticalproceduresarecomparedbasedontheresultsofcross-validation,itisimportanttonotethattheprocedurewiththebetterestimatedperformancemaynotactuallybethebetterofthetwoprocedures(i.e.itmaynothavethebettervalueofEF).Someprogresshasbeenmadeonconstructingconfidenceintervalsaroundcross-validationestimates,[14]butthisisconsideredadifficultproblem.\nMostformsofcross-validationarestraightforwardtoimplementaslongasanimplementationofthepredictionmethodbeingstudiedisavailable.Inparticular,thepredictionmethodcanbea"blackbox"\xe2\x80\x93thereisnoneedtohaveaccesstotheinternalsofitsimplementation.Ifthepredictionmethodisexpensivetotrain,cross-validationcanbeveryslowsincethetrainingmustbecarriedoutrepeatedly.Insomecasessuchasleastsquaresandkernelregression,cross-validationcanbespedupsignificantlybypre-computingcertainvaluesthatareneededrepeatedlyinthetraining,orbyusingfast"updatingrules"suchastheSherman\xe2\x80\x93Morrisonformula.Howeveronemustbecarefultopreservethe"totalblinding"ofthevalidationsetfromthetrainingprocedure,otherwisebiasmayresult.Anextremeexampleofacceleratingcross-validationoccursinlinearregression,wheretheresultsofcross-validationhaveaclosed-formexpressionknownasthepredictionresidualerrorsumofsquares(PRESS).\nCross-validationonlyyieldsmeaningfulresultsifthevalidationsetandtrainingsetaredrawnfromthesamepopulationandonlyifhumanbiasesarecontrolled.\nInmanyapplicationsofpredictivemodeling,thestructureofthesystembeingstudiedevolvesovertime(i.e.itis"non-stationary").Bothofthesecanintroducesystematicdifferencesbetweenthetrainingandvalidationsets.Forexample,ifamodelforpredictingstockvaluesistrainedondataforacertainfive-yearperiod,itisunrealistictotreatthesubsequentfive-yearperiodasadrawfromthesamepopulation.Asanotherexample,supposeamodelisdevelopedtopredictanindividual\'sriskforbeingdiagnosedwithaparticulardiseasewithinthenextyear.Ifthemodelistrainedusingdatafromastudyinvolvingonlyaspecificpopulationgroup(e.g.youngpeopleormales),butisthenappliedtothegeneralpopulation,thecross-validationresultsfromthetrainingsetcoulddiffergreatlyfromtheactualpredictiveperformance.\nInmanyapplications,modelsalsomaybeincorrectlyspecifiedandvaryasafunctionofmodelerbiasesand/orarbitrarychoices.Whenthisoccurs,theremaybeanillusionthatthesystemchangesinexternalsamples,whereasthereasonisthatthemodelhasmissedacriticalpredictorand/orincludedaconfoundedpredictor.Newevidenceisthatcross-validationbyitselfisnotverypredictiveofexternalvalidity,whereasaformofexperimentalvalidationknownasswapsamplingthatdoescontrolforhumanbiascanbemuchmorepredictiveofexternalvalidity.[16]AsdefinedbythislargeMAQC-IIstudyacross30,000models,swapsamplingincorporatescross-validationinthesensethatpredictionsaretestedacrossindependenttrainingandvalidationsamples.Yet,modelsarealsodevelopedacrosstheseindependentsamplesandbymodelerswhoareblindedtooneanother.Whenthereisamismatchinthesemodelsdevelopedacrosstheseswappedtrainingandvalidationsamplesashappensquitefrequently,MAQC-IIshowsthatthiswillbemuchmorepredictiveofpoorexternalpredictivevaliditythantraditionalcross-validation.\nThereasonforthesuccessoftheswappedsamplingisabuilt-incontrolforhumanbiasesinmodelbuilding.Inadditiontoplacingtoomuchfaithinpredictionsthatmayvaryacrossmodelersandleadtopoorexternalvalidityduetotheseconfoundingmodelereffects,thesearesomeotherwaysthatcross-validationcanbemisused:\nSincetheorderofthedataisimportant,cross-validationmightbeproblematicfortime-seriesmodels.Amoreappropriateapproachmightbetouseforwardchaining.\nCross-validationcanbeusedtocomparetheperformancesofdifferentpredictivemodelingprocedures.Forexample,supposeweareinterestedinopticalcharacterrecognition,andweareconsideringusingeithersupportvectormachines(SVM)ork-nearestneighbors(KNN)topredictthetruecharacterfromanimageofahandwrittencharacter.Usingcross-validation,wecouldobjectivelycomparethesetwomethodsintermsoftheirrespectivefractionsofmisclassifiedcharacters.Ifwesimplycomparedthemethodsbasedontheirin-sampleerrorrates,theKNNmethodwouldlikelyappeartoperformbetter,sinceitismoreflexibleandhencemorepronetooverfitting[citationneeded]comparedtotheSVMmethod.\nCross-validationcanalsobeusedinvariableselection.[19]Supposeweareusingtheexpressionlevelsof20proteinstopredictwhetheracancerpatientwillrespondtoadrug.Apracticalgoalwouldbetodeterminewhichsubsetofthe20featuresshouldbeusedtoproducethebestpredictivemodel.Formostmodelingprocedures,ifwecomparefeaturesubsetsusingthein-sampleerrorrates,thebestperformancewilloccurwhenall20featuresareused.Howeverundercross-validation,themodelwiththebestfitwillgenerallyincludeonlyasubsetofthefeaturesthataredeemedtrulyinformative.\nArecentdevelopmentinmedicalstatisticsisitsuseinmeta-analysis.Itformsthebasisofthevalidationstatistic,Vnwhichisusedtotestthestatisticalvalidityofmeta-analysissummaryestimates.[20]Ithasalsobeenusedinamoreconventionalsenseinmeta-analysistoestimatethelikelypredictionerrorofmeta-analysisresults.[21]\n