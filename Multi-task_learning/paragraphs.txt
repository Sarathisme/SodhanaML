Multi-tasklearning(MTL)isasubfieldofmachinelearninginwhichmultiplelearningtasksaresolvedatthesametime,whileexploitingcommonalitiesanddifferencesacrosstasks.Thiscanresultinimprovedlearningefficiencyandpredictionaccuracyforthetask-specificmodels,whencomparedtotrainingthemodelsseparately.[1][2][3]EarlyversionsofMTLwerecalled"hints"[4][5]\n\nInawidelycited1997paper,RichCaruanagavethefollowingcharacterization:MultitaskLearningisanapproachtoinductivetransferthatimprovesgeneralizationbyusingthedomaininformationcontainedinthetrainingsignalsofrelatedtasksasaninductivebias.Itdoesthisbylearningtasksinparallelwhileusingasharedrepresentation;whatislearnedforeachtaskcanhelpothertasksbelearnedbetter.[3]Intheclassificationcontext,MTLaimstoimprovetheperformanceofmultipleclassificationtasksbylearningthemjointly.Oneexampleisaspam-filter,whichcanbetreatedasdistinctbutrelatedclassificationtasksacrossdifferentusers.Tomakethismoreconcrete,considerthatdifferentpeoplehavedifferentdistributionsoffeatureswhichdistinguishspamemailsfromlegitimateones,forexampleanEnglishspeakermayfindthatallemailsinRussianarespam,notsoforRussianspeakers.Yetthereisadefinitecommonalityinthisclassificationtaskacrossusers,forexampleonecommonfeaturemightbetextrelatedtomoneytransfer.Solvingeachuser\'sspamclassificationproblemjointlyviaMTLcanletthesolutionsinformeachotherandimproveperformance.[6]FurtherexamplesofsettingsforMTLincludemulticlassclassificationandmulti-labelclassification.[7]\nMulti-tasklearningworksbecauseregularizationinducedbyrequiringanalgorithmtoperformwellonarelatedtaskcanbesuperiortoregularizationthatpreventsoverfittingbypenalizingallcomplexityuniformly.OnesituationwhereMTLmaybeparticularlyhelpfulisifthetaskssharesignificantcommonalitiesandaregenerallyslightlyundersampled.[8][6]However,asdiscussedbelow,MTLhasalsobeenshowntobebeneficialforlearningunrelatedtasks.[8][9]\nWithintheMTLparadigm,informationcanbesharedacrosssomeorallofthetasks.Dependingonthestructureoftaskrelatedness,onemaywanttoshareinformationselectivelyacrossthetasks.Forexample,tasksmaybegroupedorexistinahierarchy,orberelatedaccordingtosomegeneralmetric.Suppose,asdevelopedmoreformallybelow,thattheparametervectormodelingeachtaskisalinearcombinationofsomeunderlyingbasis.Similarityintermsofthisbasiscanindicatetherelatednessofthetasks.Forexample,withsparsity,overlapofnonzerocoefficientsacrosstasksindicatescommonality.Ataskgroupingthencorrespondstothosetaskslyinginasubspacegeneratedbysomesubsetofbasiselements,wheretasksindifferentgroupsmaybedisjointoroverlaparbitrarilyintermsoftheirbases.[10]Taskrelatednesscanbeimposedaprioriorlearnedfromthedata.[7][11]Hierarchicaltaskrelatednesscanalsobeexploitedimplicitlywithoutassumingaprioriknowledgeorlearningrelationsexplicitly.[8][12].Forexample,theexplicitlearningofsamplerelevanceacrosstaskscanbedonetoguaranteetheeffectivenessofjointlearningacrossmultipledomains.[8]\nOnecanattemptlearningagroupofprincipaltasksusingagroupofauxiliarytasks,unrelatedtotheprincipalones.Inmanyapplications,jointlearningofunrelatedtaskswhichusethesameinputdatacanbebeneficial.Thereasonisthatpriorknowledgeabouttaskrelatednesscanleadtosparserandmoreinformativerepresentationsforeachtaskgrouping,essentiallybyscreeningoutidiosyncrasiesofthedatadistribution.Novelmethodswhichbuildsonapriormultitaskmethodologybyfavoringasharedlow-dimensionalrepresentationwithineachtaskgroupinghavebeenproposed.Theprogrammercanimposeapenaltyontasksfromdifferentgroupswhichencouragesthetworepresentationstobeorthogonal.Experimentsonsyntheticandrealdatahaveindicatedthatincorporatingunrelatedtaskscanresultinsignificantimprovementsoverstandardmulti-tasklearningmethods.[9]\nRelatedtomulti-tasklearningistheconceptofknowledgetransfer.Whereastraditionalmulti-tasklearningimpliesthatasharedrepresentationisdevelopedconcurrentlyacrosstasks,transferofknowledgeimpliesasequentiallysharedrepresentation.LargescalemachinelearningprojectssuchasthedeepconvolutionalneuralnetworkGoogLeNet,[13]animage-basedobjectclassifier,candeveloprobustrepresentationswhichmaybeusefultofurtheralgorithmslearningrelatedtasks.Forexample,thepre-trainedmodelcanbeusedasafeatureextractortoperformpre-processingforanotherlearningalgorithm.Orthepre-trainedmodelcanbeusedtoinitializeamodelwithsimilararchitecturewhichisthenfine-tunedtolearnadifferentclassificationtask.[14]\nTraditionallyMulti-tasklearningandtransferofknowledgeareappliedtostationarylearningsettings.Theirextensiontonon-stationaryenvironmentsistermedGrouponlineadaptivelearning(GOAL).[15]Sharinginformationcouldbeparticularlyusefuliflearnersoperateincontinuouslychangingenvironments,becausealearnercouldbenefitfrompreviousexperienceofanotherlearnertoquicklyadapttotheirnewenvironment.Suchgroup-adaptivelearninghasnumerousapplications,frompredictingfinancialtime-series,throughcontentrecommendationsystems,tovisualunderstandingforadaptiveautonomousagents.\nTheMTLproblemcanbecastwithinthecontextofRKHSvv(acompleteinnerproductspaceofvector-valuedfunctionsequippedwithareproducingkernel).Inparticular,recentfocushasbeenoncaseswheretaskstructurecanbeidentifiedviaaseparablekernel,describedbelow.ThepresentationherederivesfromCilibertoetal.,2015.[7]\nSupposethetrainingdatasetis\n\n\n\n\n\n\nS\n\n\n\nt\n\n\n=\n{\n(\n\nx\n\ni\n\n\nt\n\n\n,\n\ny\n\ni\n\n\nt\n\n\n)\n\n}\n\ni\n=\n1\n\n\n\nn\n\nt\n\n\n\n\n\n\n{\\displaystyle{\\mathcal{S}}_{t}=\\{(x_{i}^{t},y_{i}^{t})\\}_{i=1}^{n_{t}}}\n\n,with\n\n\n\n\nx\n\ni\n\n\nt\n\n\n∈\n\n\nX\n\n\n\n\n{\\displaystylex_{i}^{t}\\in{\\mathcal{X}}}\n\n,\n\n\n\n\ny\n\ni\n\n\nt\n\n\n∈\n\n\nY\n\n\n\n\n{\\displaystyley_{i}^{t}\\in{\\mathcal{Y}}}\n\n,wheretindexestask,and\n\n\n\nt\n∈\n1\n,\n.\n.\n.\n,\nT\n\n\n{\\displaystylet\\in1,...,T}\n\n.Let\n\n\n\nn\n=\n\n∑\n\nt\n=\n1\n\n\nT\n\n\n\nn\n\nt\n\n\n\n\n{\\displaystylen=\\sum_{t=1}^{T}n_{t}}\n\n.Inthissettingthereisaconsistentinputandoutputspaceandthesamelossfunction\n\n\n\n\n\nL\n\n\n:\n\nR\n\n×\n\nR\n\n→\n\n\nR\n\n\n+\n\n\n\n\n{\\displaystyle{\\mathcal{L}}:\\mathbb{R}\\times\\mathbb{R}\\rightarrow\\mathbb{R}_{+}}\n\nforeachtask:.Thisresultsintheregularizedmachinelearningproblem:\n\n\n\n\n\nmin\n\nf\n∈\n\n\nH\n\n\n\n\n\n∑\n\nt\n=\n1\n\n\nT\n\n\n\n\n1\n\nn\n\nt\n\n\n\n\n\n∑\n\ni\n=\n1\n\n\n\nn\n\nt\n\n\n\n\n\n\nL\n\n\n(\n\ny\n\ni\n\n\nt\n\n\n,\n\nf\n\nt\n\n\n(\n\nx\n\ni\n\n\nt\n\n\n)\n)\n+\nλ\n\n|\n\n\n|\n\nf\n\n|\n\n\n\n|\n\n\n\nH\n\n\n\n2\n\n\n\n\n{\\displaystyle\\min_{f\\in{\\mathcal{H}}}\\sum_{t=1}^{T}{\\frac{1}{n_{t}}}\\sum_{i=1}^{n_{t}}{\\mathcal{L}}(y_{i}^{t},f_{t}(x_{i}^{t}))+\\lambda||f||_{\\mathcal{H}}^{2}}\n\n    (1)where\n\n\n\n\n\nH\n\n\n\n\n{\\displaystyle{\\mathcal{H}}}\n\nisavectorvaluedreproducingkernelHilbertspacewithfunctions\n\n\n\nf\n:\n\n\nX\n\n\n→\n\n\n\nY\n\n\n\nT\n\n\n\n\n{\\displaystylef:{\\mathcal{X}}\\rightarrow{\\mathcal{Y}}^{T}}\n\nhavingcomponents\n\n\n\n\nf\n\nt\n\n\n:\n\n\nX\n\n\n→\n\n\nY\n\n\n\n\n{\\displaystylef_{t}:{\\mathcal{X}}\\rightarrow{\\mathcal{Y}}}\n\n.\nThereproducingkernelforthespace\n\n\n\n\n\nH\n\n\n\n\n{\\displaystyle{\\mathcal{H}}}\n\noffunctions\n\n\n\nf\n:\n\n\nX\n\n\n→\n\n\nR\n\n\nT\n\n\n\n\n{\\displaystylef:{\\mathcal{X}}\\rightarrow\\mathbb{R}^{T}}\n\nisasymmetricmatrix-valuedfunction\n\n\n\nΓ\n:\n\n\nX\n\n\n×\n\n\nX\n\n\n→\n\n\nR\n\n\nT\n×\nT\n\n\n\n\n{\\displaystyle\\Gamma:{\\mathcal{X}}\\times{\\mathcal{X}}\\rightarrow\\mathbb{R}^{T\\timesT}}\n\n,suchthat\n\n\n\nΓ\n(\n⋅\n,\nx\n)\nc\n∈\n\n\nH\n\n\n\n\n{\\displaystyle\\Gamma(\\cdot,x)c\\in{\\mathcal{H}}}\n\nandthefollowingreproducingpropertyholds:\n\n\n\n\n⟨\nf\n(\nx\n)\n,\nc\n\n⟩\n\n\n\nR\n\n\nT\n\n\n\n\n=\n⟨\nf\n,\nΓ\n(\nx\n,\n⋅\n)\nc\n\n⟩\n\n\nH\n\n\n\n\n\n{\\displaystyle\\langlef(x),c\\rangle_{\\mathbb{R}^{T}}=\\langlef,\\Gamma(x,\\cdot)c\\rangle_{\\mathcal{H}}}\n\n    (2)Thereproducingkernelgivesrisetoarepresentertheoremshowingthatanysolutiontoequation1hastheform:\n\n\n\n\nf\n(\nx\n)\n=\n\n∑\n\nt\n=\n1\n\n\nT\n\n\n\n∑\n\ni\n=\n1\n\n\n\nn\n\nt\n\n\n\n\nΓ\n(\nx\n,\n\nx\n\ni\n\n\nt\n\n\n)\n\nc\n\ni\n\n\nt\n\n\n\n\n{\\displaystylef(x)=\\sum_{t=1}^{T}\\sum_{i=1}^{n_{t}}\\Gamma(x,x_{i}^{t})c_{i}^{t}}\n\n    (3)TheformofthekernelΓinducesboththerepresentationofthefeaturespaceandstructurestheoutputacrosstasks.Anaturalsimplificationistochooseaseparablekernel,whichfactorsintoseparatekernelsontheinputspaceXandonthetasks\n\n\n\n{\n1\n,\n.\n.\n.\n,\nT\n}\n\n\n{\\displaystyle\\{1,...,T\\}}\n\n.Inthiscasethekernelrelatingscalarcomponents\n\n\n\n\nf\n\nt\n\n\n\n\n{\\displaystylef_{t}}\n\nand\n\n\n\n\nf\n\ns\n\n\n\n\n{\\displaystylef_{s}}\n\nisgivenby\n\n\n\nγ\n(\n(\n\nx\n\ni\n\n\n,\nt\n)\n,\n(\n\nx\n\nj\n\n\n,\ns\n)\n)\n=\nk\n(\n\nx\n\ni\n\n\n,\n\nx\n\nj\n\n\n)\n\nk\n\nT\n\n\n(\ns\n,\nt\n)\n=\nk\n(\n\nx\n\ni\n\n\n,\n\nx\n\nj\n\n\n)\n\nA\n\ns\n,\nt\n\n\n\n\n{\\textstyle\\gamma((x_{i},t),(x_{j},s))=k(x_{i},x_{j})k_{T}(s,t)=k(x_{i},x_{j})A_{s,t}}\n\n.Forvectorvaluedfunctions\n\n\n\nf\n∈\n\n\nH\n\n\n\n\n{\\displaystylef\\in{\\mathcal{H}}}\n\nwecanwrite\n\n\n\nΓ\n(\n\nx\n\ni\n\n\n,\n\nx\n\nj\n\n\n)\n=\nk\n(\n\nx\n\ni\n\n\n,\n\nx\n\nj\n\n\n)\nA\n\n\n{\\displaystyle\\Gamma(x_{i},x_{j})=k(x_{i},x_{j})A}\n\n,wherekisascalarreproducingkernel,andAisasymmetricpositivesemi-definite\n\n\n\nT\n×\nT\n\n\n{\\displaystyleT\\timesT}\n\nmatrix.Henceforthdenote\n\n\n\n\nS\n\n+\n\n\nT\n\n\n=\n{\n\nPSDmatrices\n\n}\n⊂\n\n\nR\n\n\nT\n×\nT\n\n\n\n\n{\\displaystyleS_{+}^{T}=\\{{\\text{PSDmatrices}}\\}\\subset\\mathbb{R}^{T\\timesT}}\n\n.\nThisfactorizationproperty,separability,impliestheinputfeaturespacerepresentationdoesnotvarybytask.Thatis,thereisnointeractionbetweentheinputkernelandthetaskkernel.ThestructureontasksisrepresentedsolelybyA.Methodsfornon-separablekernelsΓisancurrentfieldofresearch.\nFortheseparablecase,therepresentationtheoremisreducedto\n\n\n\nf\n(\nx\n)\n=\n\n∑\n\ni\n=\n1\n\n\nN\n\n\nk\n(\nx\n,\n\nx\n\ni\n\n\n)\nA\n\nc\n\ni\n\n\n\n\n{\\textstylef(x)=\\sum_{i=1}^{N}k(x,x_{i})Ac_{i}}\n\n.ThemodeloutputonthetrainingdataisthenKCA,whereKisthe\n\n\n\nn\n×\nn\n\n\n{\\displaystylen\\timesn}\n\nempiricalkernelmatrixwithentries\n\n\n\n\nK\n\ni\n,\nj\n\n\n=\nk\n(\n\nx\n\ni\n\n\n,\n\nx\n\nj\n\n\n)\n\n\n{\\textstyleK_{i,j}=k(x_{i},x_{j})}\n\n,andCisthe\n\n\n\nn\n×\nT\n\n\n{\\displaystylen\\timesT}\n\nmatrixofrows\n\n\n\n\nc\n\ni\n\n\n\n\n{\\displaystylec_{i}}\n\n.\nWiththeseparablekernel,equation1canberewrittenas\n\n\n\n\n\nmin\n\nC\n∈\n\n\nR\n\n\nn\n×\nT\n\n\n\n\nV\n(\nY\n,\nK\nC\nA\n)\n+\nλ\nt\nr\n(\nK\nC\nA\n\nC\n\n⊤\n\n\n)\n\n\n{\\displaystyle\\min_{C\\in\\mathbb{R}^{n\\timesT}}V(Y,KCA)+\\lambdatr(KCAC^{\\top})}\n\n    (P)whereVisa(weighted)averageofLappliedentry-wisetoYandKCA.(Theweightiszeroif\n\n\n\n\nY\n\ni\n\n\nt\n\n\n\n\n{\\displaystyleY_{i}^{t}}\n\nisamissingobservation).\nNotethesecondterminPcanbederivedasfollows:\nTherearethreelargelyequivalentwaystorepresenttaskstructure:througharegularizer;throughanoutputmetric,andthroughanoutputmapping.\nRegularizer — Withtheseparablekernel,itcanbeshown(below)that\n\n\n\n\n|\n\n\n|\n\nf\n\n|\n\n\n\n|\n\n\n\nH\n\n\n\n2\n\n\n=\n\n∑\n\ns\n,\nt\n=\n1\n\n\nT\n\n\n\nA\n\nt\n,\ns\n\n\n†\n\n\n⟨\n\nf\n\ns\n\n\n,\n\nf\n\nt\n\n\n\n⟩\n\n\n\n\nH\n\n\n\nk\n\n\n\n\n\n\n{\\textstyle||f||_{\\mathcal{H}}^{2}=\\sum_{s,t=1}^{T}A_{t,s}^{\\dagger}\\langlef_{s},f_{t}\\rangle_{{\\mathcal{H}}_{k}}}\n\n,where\n\n\n\n\nA\n\nt\n,\ns\n\n\n†\n\n\n\n\n{\\displaystyleA_{t,s}^{\\dagger}}\n\nisthe\n\n\n\nt\n,\ns\n\n\n{\\displaystylet,s}\n\nelementofthepseudoinverseof\n\n\n\nA\n\n\n{\\displaystyleA}\n\n,and\n\n\n\n\n\n\nH\n\n\n\nk\n\n\n\n\n{\\displaystyle{\\mathcal{H}}_{k}}\n\nistheRKHSbasedonthescalarkernel\n\n\n\nk\n\n\n{\\displaystylek}\n\n,and\n\n\n\n\nf\n\nt\n\n\n(\nx\n)\n=\n\n∑\n\ni\n=\n1\n\n\nn\n\n\nk\n(\nx\n,\n\nx\n\ni\n\n\n)\n\nA\n\nt\n\n\n⊤\n\n\n\nc\n\ni\n\n\n\n\n{\\textstylef_{t}(x)=\\sum_{i=1}^{n}k(x,x_{i})A_{t}^{\\top}c_{i}}\n\n.Thisformulationshowsthat\n\n\n\n\nA\n\nt\n,\ns\n\n\n†\n\n\n\n\n{\\displaystyleA_{t,s}^{\\dagger}}\n\ncontrolstheweightofthepenaltyassociatedwith\n\n\n\n⟨\n\nf\n\ns\n\n\n,\n\nf\n\nt\n\n\n\n⟩\n\n\n\n\nH\n\n\n\nk\n\n\n\n\n\n\n{\\textstyle\\langlef_{s},f_{t}\\rangle_{{\\mathcal{H}}_{k}}}\n\n.(Notethat\n\n\n\n⟨\n\nf\n\ns\n\n\n,\n\nf\n\nt\n\n\n\n⟩\n\n\n\n\nH\n\n\n\nk\n\n\n\n\n\n\n{\\textstyle\\langlef_{s},f_{t}\\rangle_{{\\mathcal{H}}_{k}}}\n\narisesfrom\n\n\n\n\n|\n\n\n|\n\n\nf\n\nt\n\n\n\n|\n\n\n\n|\n\n\n\n\n\nH\n\n\n\nk\n\n\n\n\n=\n⟨\n\nf\n\nt\n\n\n,\n\nf\n\nt\n\n\n\n⟩\n\n\n\n\nH\n\n\n\nk\n\n\n\n\n\n\n{\\textstyle||f_{t}||_{{\\mathcal{H}}_{k}}=\\langlef_{t},f_{t}\\rangle_{{\\mathcal{H}}_{k}}}\n\n.)\n\n\n\n\n\n\n\n\n‖\nf\n\n‖\n\n\nH\n\n\n\n2\n\n\n\n\n\n=\n\n\n⟨\n\n\n∑\n\ni\n=\n1\n\n\nn\n\n\nγ\n(\n(\n\nx\n\ni\n\n\n,\n\nt\n\ni\n\n\n)\n,\n⋅\n)\n\nc\n\ni\n\n\n\nt\n\ni\n\n\n\n\n,\n\n∑\n\nj\n=\n1\n\n\nn\n\n\nγ\n(\n(\n\nx\n\nj\n\n\n,\n\nt\n\nj\n\n\n)\n,\n⋅\n)\n\nc\n\nj\n\n\n\nt\n\nj\n\n\n\n\n\n⟩\n\n\n\nH\n\n\n\n\n\n\n\n\n\n=\n\n∑\n\ni\n,\nj\n=\n1\n\n\nn\n\n\n\nc\n\ni\n\n\n\nt\n\ni\n\n\n\n\n\nc\n\nj\n\n\n\nt\n\nj\n\n\n\n\nγ\n(\n(\n\nx\n\ni\n\n\n,\n\nt\n\ni\n\n\n)\n,\n(\n\nx\n\nj\n\n\n,\n\nt\n\nj\n\n\n)\n)\n\n\n\n\n\n\n=\n\n∑\n\ni\n,\nj\n=\n1\n\n\nn\n\n\n\n∑\n\ns\n,\nt\n=\n1\n\n\nT\n\n\n\nc\n\ni\n\n\nt\n\n\n\nc\n\nj\n\n\ns\n\n\nk\n(\n\nx\n\ni\n\n\n,\n\nx\n\nj\n\n\n)\n\nA\n\ns\n,\nt\n\n\n\n\n\n\n\n\n=\n\n∑\n\ni\n,\nj\n=\n1\n\n\nn\n\n\nk\n(\n\nx\n\ni\n\n\n,\n\nx\n\nj\n\n\n)\n⟨\n\nc\n\ni\n\n\n,\nA\n\nc\n\nj\n\n\n\n⟩\n\n\n\nR\n\n\nT\n\n\n\n\n\n\n\n\n\n\n=\n\n∑\n\ni\n,\nj\n=\n1\n\n\nn\n\n\nk\n(\n\nx\n\ni\n\n\n,\n\nx\n\nj\n\n\n)\n⟨\n\nc\n\ni\n\n\n,\nA\n\nA\n\n†\n\n\nA\n\nc\n\nj\n\n\n\n⟩\n\n\n\nR\n\n\nT\n\n\n\n\n\n\n\n\n\n\n=\n\n∑\n\ni\n,\nj\n=\n1\n\n\nn\n\n\nk\n(\n\nx\n\ni\n\n\n,\n\nx\n\nj\n\n\n)\n⟨\nA\n\nc\n\ni\n\n\n,\n\nA\n\n†\n\n\nA\n\nc\n\nj\n\n\n\n⟩\n\n\n\nR\n\n\nT\n\n\n\n\n\n\n\n\n\n\n=\n\n∑\n\ni\n,\nj\n=\n1\n\n\nn\n\n\n\n∑\n\ns\n,\nt\n=\n1\n\n\nT\n\n\n(\nA\n\nc\n\ni\n\n\n\n)\n\nt\n\n\n(\nA\n\nc\n\nj\n\n\n\n)\n\ns\n\n\nk\n(\n\nx\n\ni\n\n\n,\n\nx\n\nj\n\n\n)\n\nA\n\ns\n,\nt\n\n\n†\n\n\n\n\n\n\n\n\n=\n\n∑\n\ns\n,\nt\n=\n1\n\n\nT\n\n\n\nA\n\ns\n,\nt\n\n\n†\n\n\n⟨\n\n∑\n\ni\n=\n1\n\n\nn\n\n\nk\n(\n\nx\n\ni\n\n\n,\n⋅\n)\n(\nA\n\nc\n\ni\n\n\n\n)\n\nt\n\n\n,\n\n∑\n\nj\n=\n1\n\n\nn\n\n\nk\n(\n\nx\n\nj\n\n\n,\n⋅\n)\n(\nA\n\nc\n\nj\n\n\n\n)\n\ns\n\n\n\n⟩\n\n\n\n\nH\n\n\n\nk\n\n\n\n\n\n\n\n\n\n\n=\n\n∑\n\ns\n,\nt\n=\n1\n\n\nT\n\n\n\nA\n\ns\n,\nt\n\n\n†\n\n\n⟨\n\nf\n\nt\n\n\n,\n\nf\n\ns\n\n\n\n⟩\n\n\n\n\nH\n\n\n\nk\n\n\n\n\n\n\n\n\n\n\n{\\displaystyle{\\begin{aligned}\\|f\\|_{\\mathcal{H}}^{2}&=\\left\\langle\\sum_{i=1}^{n}\\gamma((x_{i},t_{i}),\\cdot)c_{i}^{t_{i}},\\sum_{j=1}^{n}\\gamma((x_{j},t_{j}),\\cdot)c_{j}^{t_{j}}\\right\\rangle_{\\mathcal{H}}\\\\&=\\sum_{i,j=1}^{n}c_{i}^{t_{i}}c_{j}^{t_{j}}\\gamma((x_{i},t_{i}),(x_{j},t_{j}))\\\\&=\\sum_{i,j=1}^{n}\\sum_{s,t=1}^{T}c_{i}^{t}c_{j}^{s}k(x_{i},x_{j})A_{s,t}\\\\&=\\sum_{i,j=1}^{n}k(x_{i},x_{j})\\langlec_{i},Ac_{j}\\rangle_{\\mathbb{R}^{T}}\\\\&=\\sum_{i,j=1}^{n}k(x_{i},x_{j})\\langlec_{i},AA^{\\dagger}Ac_{j}\\rangle_{\\mathbb{R}^{T}}\\\\&=\\sum_{i,j=1}^{n}k(x_{i},x_{j})\\langleAc_{i},A^{\\dagger}Ac_{j}\\rangle_{\\mathbb{R}^{T}}\\\\&=\\sum_{i,j=1}^{n}\\sum_{s,t=1}^{T}(Ac_{i})^{t}(Ac_{j})^{s}k(x_{i},x_{j})A_{s,t}^{\\dagger}\\\\&=\\sum_{s,t=1}^{T}A_{s,t}^{\\dagger}\\langle\\sum_{i=1}^{n}k(x_{i},\\cdot)(Ac_{i})^{t},\\sum_{j=1}^{n}k(x_{j},\\cdot)(Ac_{j})^{s}\\rangle_{{\\mathcal{H}}_{k}}\\\\&=\\sum_{s,t=1}^{T}A_{s,t}^{\\dagger}\\langlef_{t},f_{s}\\rangle_{{\\mathcal{H}}_{k}}\\end{aligned}}}\n\n\nOutputmetric — analternativeoutputmetricon\n\n\n\n\n\n\nY\n\n\n\nT\n\n\n\n\n{\\displaystyle{\\mathcal{Y}}^{T}}\n\ncanbeinducedbytheinnerproduct\n\n\n\n⟨\n\ny\n\n1\n\n\n,\n\ny\n\n2\n\n\n\n⟩\n\nΘ\n\n\n=\n⟨\n\ny\n\n1\n\n\n,\nΘ\n\ny\n\n2\n\n\n\n⟩\n\n\n\nR\n\n\nT\n\n\n\n\n\n\n{\\displaystyle\\langley_{1},y_{2}\\rangle_{\\Theta}=\\langley_{1},\\Thetay_{2}\\rangle_{\\mathbb{R}^{T}}}\n\n.Withthesquaredlossthereisanequivalencebetweentheseparablekernels\n\n\n\nk\n(\n⋅\n,\n⋅\n)\n\nI\n\nT\n\n\n\n\n{\\displaystylek(\\cdot,\\cdot)I_{T}}\n\nunderthealternativemetric,and\n\n\n\nk\n(\n⋅\n,\n⋅\n)\nΘ\n\n\n{\\displaystylek(\\cdot,\\cdot)\\Theta}\n\n,underthecanonicalmetric.\nOutputmapping — Outputscanbemappedas\n\n\n\nL\n:\n\n\n\nY\n\n\n\nT\n\n\n→\n\n\n\n\nY\n~\n\n\n\n\n\n\n{\\displaystyleL:{\\mathcal{Y}}^{T}\\rightarrow{\\mathcal{\\tilde{Y}}}}\n\ntoahigherdimensionalspacetoencodecomplexstructuressuchastrees,graphsandstrings.ForlinearmapsL,withappropriatechoiceofseparablekernel,itcanbeshownthat\n\n\n\nA\n=\n\nL\n\n⊤\n\n\nL\n\n\n{\\displaystyleA=L^{\\top}L}\n\n.\nViatheregularizerformulation,onecanrepresentavarietyoftaskstructureseasily.\nLearningproblemPcanbegeneralizedtoadmitlearningtaskmatrixAasfollows:\n\n\n\n\n\nmin\n\nC\n∈\n\n\nR\n\n\nn\n×\nT\n\n\n,\nA\n∈\n\nS\n\n+\n\n\nT\n\n\n\n\nV\n(\nY\n,\nK\nC\nA\n)\n+\nλ\nt\nr\n(\nK\nC\nA\n\nC\n\n⊤\n\n\n)\n+\nF\n(\nA\n)\n\n\n{\\displaystyle\\min_{C\\in\\mathbb{R}^{n\\timesT},A\\inS_{+}^{T}}V(Y,KCA)+\\lambdatr(KCAC^{\\top})+F(A)}\n\n    (Q)Choiceof\n\n\n\nF\n:\n\nS\n\n+\n\n\nT\n\n\n→\n\n\nR\n\n\n+\n\n\n\n\n{\\displaystyleF:S_{+}^{T}\\rightarrow\\mathbb{R}_{+}}\n\nmustbedesignedtolearnmatricesAofagiventype.See"Specialcases"below.\nRestrictingtothecaseofconvexlossesandcoercivepenaltiesCilibertoetal.haveshownthatalthoughQisnotconvexjointlyinCandA,arelatedproblemisjointlyconvex.\nSpecificallyontheconvexset\n\n\n\n\n\nC\n\n\n=\n{\n(\nC\n,\nA\n)\n∈\n\n\nR\n\n\nn\n×\nT\n\n\n×\n\nS\n\n+\n\n\nT\n\n\n\n|\n\nR\na\nn\ng\ne\n(\n\nC\n\n⊤\n\n\nK\nC\n)\n⊆\nR\na\nn\ng\ne\n(\nA\n)\n}\n\n\n{\\displaystyle{\\mathcal{C}}=\\{(C,A)\\in\\mathbb{R}^{n\\timesT}\\timesS_{+}^{T}|Range(C^{\\top}KC)\\subseteqRange(A)\\}}\n\n,theequivalentproblem\n\n\n\n\n\nmin\n\nC\n,\nA\n∈\n\n\nC\n\n\n\n\nV\n(\nY\n,\nK\nC\n)\n+\nλ\nt\nr\n(\n\nA\n\n†\n\n\n\nC\n\n⊤\n\n\nK\nC\n)\n+\nF\n(\nA\n)\n\n\n{\\displaystyle\\min_{C,A\\in{\\mathcal{C}}}V(Y,KC)+\\lambdatr(A^{\\dagger}C^{\\top}KC)+F(A)}\n\n    (R)isconvexwiththesameminimumvalue.Andif\n\n\n\n(\n\nC\n\nR\n\n\n,\n\nA\n\nR\n\n\n)\n\n\n{\\displaystyle(C_{R},A_{R})}\n\nisaminimizerforRthen\n\n\n\n(\n\nC\n\nR\n\n\n\nA\n\nR\n\n\n†\n\n\n,\n\nA\n\nR\n\n\n)\n\n\n{\\displaystyle(C_{R}A_{R}^{\\dagger},A_{R})}\n\nisaminimizerforQ.\nRmaybesolvedbyabarriermethodonaclosedsetbyintroducingthefollowingperturbation:\n\n\n\n\n\nmin\n\nC\n∈\n\n\nR\n\n\nn\n×\nT\n\n\n,\nA\n∈\n\nS\n\n+\n\n\nT\n\n\n\n\nV\n(\nY\n,\nK\nC\n)\n+\nλ\nt\nr\n(\n\nA\n\n†\n\n\n(\n\nC\n\n⊤\n\n\nK\nC\n+\n\nδ\n\n2\n\n\n\nI\n\nT\n\n\n)\n)\n+\nF\n(\nA\n)\n\n\n{\\displaystyle\\min_{C\\in\\mathbb{R}^{n\\timesT},A\\inS_{+}^{T}}V(Y,KC)+\\lambdatr(A^{\\dagger}(C^{\\top}KC+\\delta^{2}I_{T}))+F(A)}\n\n    (S)Theperturbationviathebarrier\n\n\n\n\nδ\n\n2\n\n\nt\nr\n(\n\nA\n\n†\n\n\n)\n\n\n{\\displaystyle\\delta^{2}tr(A^{\\dagger})}\n\nforcestheobjectivefunctionstobeequalto\n\n\n\n+\n∞\n\n\n{\\displaystyle+\\infty}\n\nontheboundaryof\n\n\n\n\nR\n\nn\n×\nT\n\n\n×\n\nS\n\n+\n\n\nT\n\n\n\n\n{\\displaystyleR^{n\\timesT}\\timesS_{+}^{T}}\n\n.\nScanbesolvedwithablockcoordinatedescentmethod,alternatinginCandA.Thisresultsinasequenceofminimizers\n\n\n\n(\n\nC\n\nm\n\n\n,\n\nA\n\nm\n\n\n)\n\n\n{\\displaystyle(C_{m},A_{m})}\n\ninSthatconvergestothesolutioninRas\n\n\n\n\nδ\n\nm\n\n\n→\n0\n\n\n{\\displaystyle\\delta_{m}\\rightarrow0}\n\n,andhencegivesthesolutiontoQ.\nSpectralpenalties-Dinnuzoetal[16]suggestedsettingFastheFrobeniusnorm\n\n\n\n\n\nt\nr\n(\n\nA\n\n⊤\n\n\nA\n)\n\n\n\n\n{\\displaystyle{\\sqrt{tr(A^{\\top}A)}}}\n\n.TheyoptimizedQdirectlyusingblockcoordinatedescent,notaccountingfordifficultiesattheboundaryof\n\n\n\n\n\nR\n\n\nn\n×\nT\n\n\n×\n\nS\n\n+\n\n\nT\n\n\n\n\n{\\displaystyle\\mathbb{R}^{n\\timesT}\\timesS_{+}^{T}}\n\n.\nClusteredtaskslearning-Jacobetal[17]suggestedtolearnAinthesettingwhereTtasksareorganizedinRdisjointclusters.Inthiscaselet\n\n\n\nE\n∈\n{\n0\n,\n1\n\n}\n\nT\n×\nR\n\n\n\n\n{\\displaystyleE\\in\\{0,1\\}^{T\\timesR}}\n\nbethematrixwith\n\n\n\n\nE\n\nt\n,\nr\n\n\n=\n\nI\n\n(\n\ntask \n\nt\n∈\n\ngroup \n\nr\n)\n\n\n{\\displaystyleE_{t,r}=\\mathbb{I}({\\text{task}}t\\in{\\text{group}}r)}\n\n.Setting\n\n\n\nM\n=\nI\n−\n\nE\n\n†\n\n\n\nE\n\nT\n\n\n\n\n{\\displaystyleM=I-E^{\\dagger}E^{T}}\n\n,and\n\n\n\nU\n=\n\n\n1\nT\n\n\n\n\n11\n\n\n⊤\n\n\n\n\n{\\displaystyleU={\\frac{1}{T}}\\mathbf{11}^{\\top}}\n\n,thetaskmatrix\n\n\n\n\nA\n\n†\n\n\n\n\n{\\displaystyleA^{\\dagger}}\n\ncanbeparameterizedasafunctionof\n\n\n\nM\n\n\n{\\displaystyleM}\n\n:\n\n\n\n\nA\n\n†\n\n\n(\nM\n)\n=\n\nϵ\n\nM\n\n\nU\n+\n\nϵ\n\nB\n\n\n(\nM\n−\nU\n)\n+\nϵ\n(\nI\n−\nM\n)\n\n\n{\\displaystyleA^{\\dagger}(M)=\\epsilon_{M}U+\\epsilon_{B}(M-U)+\\epsilon(I-M)}\n\n,withtermsthatpenalizetheaverage,betweenclustersvarianceandwithinclustersvariancerespectivelyofthetaskpredictions.Misnotconvex,butthereisaconvexrelaxation\n\n\n\n\n\n\nS\n\n\n\nc\n\n\n=\n{\nM\n∈\n\nS\n\n+\n\n\nT\n\n\n:\nI\n−\nM\n∈\n\nS\n\n+\n\n\nT\n\n\n∧\nt\nr\n(\nM\n)\n=\nr\n}\n\n\n{\\displaystyle{\\mathcal{S}}_{c}=\\{M\\inS_{+}^{T}:I-M\\inS_{+}^{T}\\landtr(M)=r\\}}\n\n.Inthisformulation,\n\n\n\nF\n(\nA\n)\n=\n\nI\n\n(\nA\n(\nM\n)\n∈\n{\nA\n:\nM\n∈\n\n\n\nS\n\n\n\nC\n\n\n}\n)\n\n\n{\\displaystyleF(A)=\\mathbb{I}(A(M)\\in\\{A:M\\in{\\mathcal{S}}_{C}\\})}\n\n.\nNon-convexpenalties-PenaltiescanbeconstructedsuchthatAisconstrainedtobeagraphLaplacian,orthatAhaslowrankfactorization.Howeverthesepenaltiesarenotconvex,andtheanalysisofthebarriermethodproposedbyCilibertoetal.doesnotgothroughinthesecases.\nNon-separablekernels-Separablekernelsarelimited,inparticulartheydonotaccountforstructuresintheinteractionspacebetweentheinputandoutputdomainsjointly.Futureworkisneededtodevelopmodelsforthesekernels.\nUsingtheprinciplesofMTL,techniquesforcollaborativespamfilteringthatfacilitatespersonalizationhavebeenproposed.Inlargescaleopenmembershipemailsystems,mostusersdonotlabelenoughmessagesforanindividuallocalclassifiertobeeffective,whilethedataistoonoisytobeusedforaglobalfilteracrossallusers.Ahybridglobal/individualclassifiercanbeeffectiveatabsorbingtheinfluenceofuserswholabelemailsverydiligentlyfromthegeneralpublic.Thiscanbeaccomplishedwhilestillprovidingsufficientqualitytouserswithfewlabeledinstances.[18]\nUsingboosteddecisiontrees,onecanenableimplicitdatasharingandregularization.Thislearningmethodcanbeusedonweb-searchrankingdatasets.Oneexampleistouserankingdatasetsfromseveralcountries.Here,multitasklearningisparticularlyhelpfulasdatasetsfromdifferentcountriesvarylargelyinsizebecauseofthecostofeditorialjudgments.Ithasbeendemonstratedthatlearningvarioustasksjointlycanleadtosignificantimprovementsinperformancewithsurprisingreliability.[19]\nInordertofacilitatetransferofknowledge,ITinfrastructureisbeingdeveloped.Onesuchproject,RoboEarth,aimstosetupanopensourceinternetdatabasethatcanbeaccessedandcontinuallyupdatedfromaroundtheworld.Thegoalistofacilitateacloud-basedinteractiveknowledgebase,accessibletotechnologycompaniesandacademicinstitutions,whichcanenhancethesensing,actingandlearningcapabilitiesofrobotsandotherartificialintelligenceagents.[20]\nTheMulti-TaskLearningviaStructurAlRegularization(MALSAR)Matlabpackage[21]implementsthefollowingmulti-tasklearningalgorithms:\n