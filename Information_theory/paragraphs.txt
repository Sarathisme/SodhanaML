Informationtheorystudiesthequantification,storage,andcommunicationofinformation.ItwasoriginallyproposedbyClaudeE.Shannonin1948tofindfundamentallimitsonsignalprocessingandcommunicationoperationssuchasdatacompression,inalandmarkpaperentitled"AMathematicalTheoryofCommunication".Applicationsoffundamentaltopicsofinformationtheoryincludelosslessdatacompression(e.g.ZIPfiles),lossydatacompression(e.g.MP3sandJPEGs),andchannelcoding(e.g.fordigitalsubscriberline(DSL)).ItsimpacthasbeencrucialtothesuccessoftheVoyagermissionstodeepspace,theinventionofthecompactdisc,thefeasibilityofmobilephones,thedevelopmentoftheInternet,thestudyoflinguisticsandofhumanperception,theunderstandingofblackholes,andnumerousotherfields.\nAkeymeasureininformationtheoryis"entropy".Entropyquantifiestheamountofuncertaintyinvolvedinthevalueofarandomvariableortheoutcomeofarandomprocess.Forexample,identifyingtheoutcomeofafaircoinflip(withtwoequallylikelyoutcomes)provideslessinformation(lowerentropy)thanspecifyingtheoutcomefromarollofadie(withsixequallylikelyoutcomes).Someotherimportantmeasuresininformationtheoryaremutualinformation,channelcapacity,errorexponents,andrelativeentropy.\nThefieldisattheintersectionofmathematics,statistics,computerscience,physics,neurobiology,informationengineering,andelectricalengineering.Thetheoryhasalsofoundapplicationsinotherareas,includingstatisticalinference,naturallanguageprocessing,cryptography,neurobiology,[1]humanvision,[2]theevolution[3]andfunction[4]ofmolecularcodes(bioinformatics),modelselectioninstatistics,[5]thermalphysics,[6]quantumcomputing,linguistics,plagiarismdetection,[7]patternrecognition,andanomalydetection.[8]Importantsub-fieldsofinformationtheoryincludesourcecoding,channelcoding,algorithmiccomplexitytheory,algorithmicinformationtheory,information-theoreticsecurity,andmeasuresofinformation.\nInformationtheorystudiesthetransmission,processing,extraction,andutilizationofinformation.Abstractly,informationcanbethoughtofastheresolutionofuncertainty.Inthecaseofcommunicationofinformationoveranoisychannel,thisabstractconceptwasmadeconcretein1948byClaudeShannoninhispaper"AMathematicalTheoryofCommunication",inwhich"information"isthoughtofasasetofpossiblemessages,wherethegoalistosendthesemessagesoveranoisychannel,andthentohavethereceiverreconstructthemessagewithlowprobabilityoferror,inspiteofthechannelnoise.Shannon\'smainresult,thenoisy-channelcodingtheoremshowedthat,inthelimitofmanychanneluses,therateofinformationthatisasymptoticallyachievableisequaltothechannelcapacity,aquantitydependentmerelyonthestatisticsofthechanneloverwhichthemessagesaresent.[1]\nInformationtheoryiscloselyassociatedwithacollectionofpureandapplieddisciplinesthathavebeeninvestigatedandreducedtoengineeringpracticeunderavarietyofrubricsthroughouttheworldoverthepasthalfcenturyormore:adaptivesystems,anticipatorysystems,artificialintelligence,complexsystems,complexityscience,cybernetics,informatics,machinelearning,alongwithsystemssciencesofmanydescriptions.Informationtheoryisabroadanddeepmathematicaltheory,withequallybroadanddeepapplications,amongstwhichisthevitalfieldofcodingtheory.\nCodingtheoryisconcernedwithfindingexplicitmethods,calledcodes,forincreasingtheefficiencyandreducingtheerrorrateofdatacommunicationovernoisychannelstonearthechannelcapacity.Thesecodescanberoughlysubdividedintodatacompression(sourcecoding)anderror-correction(channelcoding)techniques.Inthelattercase,ittookmanyyearstofindthemethodsShannon\'sworkprovedwerepossible.Athirdclassofinformationtheorycodesarecryptographicalgorithms(bothcodesandciphers).Concepts,methodsandresultsfromcodingtheoryandinformationtheoryarewidelyusedincryptographyandcryptanalysis.Seethearticleban(unit)forahistoricalapplication.\nInformationtheoryisalsousedininformationretrieval,intelligencegathering,gambling,statistics,andeveninmusicalcomposition.\nThelandmarkeventthatestablishedthedisciplineofinformationtheoryandbroughtittoimmediateworldwideattentionwasthepublicationofClaudeE.Shannon\'sclassicpaper"AMathematicalTheoryofCommunication"intheBellSystemTechnicalJournalinJulyandOctober1948.\nPriortothispaper,limitedinformation-theoreticideashadbeendevelopedatBellLabs,allimplicitlyassumingeventsofequalprobability.HarryNyquist\'s1924paper,CertainFactorsAffectingTelegraphSpeed,containsatheoreticalsectionquantifying"intelligence"andthe"linespeed"atwhichitcanbetransmittedbyacommunicationsystem,givingtherelationW=Klogm(recallingBoltzmann\'sconstant),whereWisthespeedoftransmissionofintelligence,misthenumberofdifferentvoltagelevelstochoosefromateachtimestep,andKisaconstant.RalphHartley\'s1928paper,TransmissionofInformation,usesthewordinformationasameasurablequantity,reflectingthereceiver\'sabilitytodistinguishonesequenceofsymbolsfromanyother,thusquantifyinginformationasH=logSn=nlogS,whereSwasthenumberofpossiblesymbols,andnthenumberofsymbolsinatransmission.Theunitofinformationwasthereforethedecimaldigit,whichhassincesometimesbeencalledthehartleyinhishonorasaunitorscaleormeasureofinformation.AlanTuringin1940usedsimilarideasaspartofthestatisticalanalysisofthebreakingoftheGermansecondworldwarEnigmaciphers.\nMuchofthemathematicsbehindinformationtheorywitheventsofdifferentprobabilitiesweredevelopedforthefieldofthermodynamicsbyLudwigBoltzmannandJ.WillardGibbs.Connectionsbetweeninformation-theoreticentropyandthermodynamicentropy,includingtheimportantcontributionsbyRolfLandauerinthe1960s,areexploredinEntropyinthermodynamicsandinformationtheory.\nInShannon\'srevolutionaryandgroundbreakingpaper,theworkforwhichhadbeensubstantiallycompletedatBellLabsbytheendof1944,Shannonforthefirsttimeintroducedthequalitativeandquantitativemodelofcommunicationasastatisticalprocessunderlyinginformationtheory,openingwiththeassertionthat\nWithitcametheideasof\nInformationtheoryisbasedonprobabilitytheoryandstatistics.Informationtheoryoftenconcernsitselfwithmeasuresofinformationofthedistributionsassociatedwithrandomvariables.Importantquantitiesofinformationareentropy,ameasureofinformationinasinglerandomvariable,andmutualinformation,ameasureofinformationincommonbetweentworandomvariables.Theformerquantityisapropertyoftheprobabilitydistributionofarandomvariableandgivesalimitontherateatwhichdatageneratedbyindependentsampleswiththegivendistributioncanbereliablycompressed.Thelatterisapropertyofthejointdistributionoftworandomvariables,andisthemaximumrateofreliablecommunicationacrossanoisychannelinthelimitoflongblocklengths,whenthechannelstatisticsaredeterminedbythejointdistribution.\nThechoiceoflogarithmicbaseinthefollowingformulaedeterminestheunitofinformationentropythatisused.Acommonunitofinformationisthebit,basedonthebinarylogarithm.Otherunitsincludethenat,whichisbasedonthenaturallogarithm,andthedecimaldigit,whichisbasedonthecommonlogarithm.\nInwhatfollows,anexpressionoftheformplogpisconsideredbyconventiontobeequaltozerowheneverp=0.Thisisjustifiedbecause\n\n\n\n\nlim\n\np\n→\n0\n+\n\n\np\nlog\n⁡\np\n=\n0\n\n\n{\\displaystyle\\lim_{p\\rightarrow0+}p\\logp=0}\n\nforanylogarithmicbase.\nBasedontheprobabilitymassfunctionofeachsourcesymboltobecommunicated,theShannonentropyH,inunitsofbits(persymbol),isgivenby\nwherepiistheprobabilityofoccurrenceofthei-thpossiblevalueofthesourcesymbol.Thisequationgivestheentropyintheunitsof"bits"(persymbol)becauseitusesalogarithmofbase2,andthisbase-2measureofentropyhassometimesbeencalledthe"shannon"inhishonor.Entropyisalsocommonlycomputedusingthenaturallogarithm(basee,whereeisEuler\'snumber),whichproducesameasurementofentropyin"nats"persymbolandsometimessimplifiestheanalysisbyavoidingtheneedtoincludeextraconstantsintheformulas.Otherbasesarealsopossible,butlesscommonlyused.Forexample,alogarithmofbase28=256willproduceameasurementinbytespersymbol,andalogarithmofbase10willproduceameasurementindecimaldigits(orhartleys)persymbol.\nIntuitively,theentropyHXofadiscreterandomvariableXisameasureoftheamountofuncertaintyassociatedwiththevalueofXwhenonlyitsdistributionisknown.\nTheentropyofasourcethatemitsasequenceofNsymbolsthatareindependentandidenticallydistributed(iid)isN\xe2\x8b\x85Hbits(permessageofNsymbols).Ifthesourcedatasymbolsareidenticallydistributedbutnotindependent,theentropyofamessageoflengthNwillbelessthanN\xe2\x8b\x85H.\nIfonetransmits1000bits(0sand1s),andthevalueofeachofthesebitsisknowntothereceiver(hasaspecificvaluewithcertainty)aheadoftransmission,itisclearthatnoinformationistransmitted.If,however,eachbitisindependentlyequallylikelytobe0or1,1000shannonsofinformation(moreoftencalledbits)havebeentransmitted.Betweenthesetwoextremes,informationcanbequantifiedasfollows.If\xf0\x9d\x95\x8fisthesetofallmessages{x1,...,xn}thatXcouldbe,andp(x)istheprobabilityofsome\n\n\n\nx\n∈\n\nX\n\n\n\n{\\displaystylex\\in\\mathbb{X}}\n\n,thentheentropy,H,ofXisdefined:[9]\n(Here,I(x)istheself-information,whichistheentropycontributionofanindividualmessage,and\xf0\x9d\x94\xbcXistheexpectedvalue.)Apropertyofentropyisthatitismaximizedwhenallthemessagesinthemessagespaceareequiprobablep(x)=1/n;i.e.,mostunpredictable,inwhichcaseH(X)=logn.\nThespecialcaseofinformationentropyforarandomvariablewithtwooutcomesisthebinaryentropyfunction,usuallytakentothelogarithmicbase2,thushavingtheshannon(Sh)asunit:\nThejointentropyoftwodiscreterandomvariablesXandYismerelytheentropyoftheirpairing:(X,Y).ThisimpliesthatifXandYareindependent,thentheirjointentropyisthesumoftheirindividualentropies.\nForexample,if(X,Y)representsthepositionofachesspiece\xe2\x80\x94XtherowandYthecolumn,thenthejointentropyoftherowofthepieceandthecolumnofthepiecewillbetheentropyofthepositionofthepiece.\nDespitesimilarnotation,jointentropyshouldnotbeconfusedwithcrossentropy.\nTheconditionalentropyorconditionaluncertaintyofXgivenrandomvariableY(alsocalledtheequivocationofXaboutY)istheaverageconditionalentropyoverY:[10]\nBecauseentropycanbeconditionedonarandomvariableoronthatrandomvariablebeingacertainvalue,careshouldbetakennottoconfusethesetwodefinitionsofconditionalentropy,theformerofwhichisinmorecommonuse.Abasicpropertyofthisformofconditionalentropyisthat:\nMutualinformationmeasurestheamountofinformationthatcanbeobtainedaboutonerandomvariablebyobservinganother.Itisimportantincommunicationwhereitcanbeusedtomaximizetheamountofinformationsharedbetweensentandreceivedsignals.ThemutualinformationofXrelativetoYisgivenby:\nwhereSI(SpecificmutualInformation)isthepointwisemutualinformation.\nAbasicpropertyofthemutualinformationisthat\nThatis,knowingY,wecansaveanaverageofI(X;Y)bitsinencodingXcomparedtonotknowingY.\nMutualinformationissymmetric:\nMutualinformationcanbeexpressedastheaverageKullback\xe2\x80\x93Leiblerdivergence(informationgain)betweentheposteriorprobabilitydistributionofXgiventhevalueofYandthepriordistributiononX:\nInotherwords,thisisameasureofhowmuch,ontheaverage,theprobabilitydistributiononXwillchangeifwearegiventhevalueofY.Thisisoftenrecalculatedasthedivergencefromtheproductofthemarginaldistributionstotheactualjointdistribution:\nMutualinformationiscloselyrelatedtothelog-likelihoodratiotestinthecontextofcontingencytablesandthemultinomialdistributionandtoPearson\'s\xcf\x872test:mutualinformationcanbeconsideredastatisticforassessingindependencebetweenapairofvariables,andhasawell-specifiedasymptoticdistribution.\nTheKullback\xe2\x80\x93Leiblerdivergence(orinformationdivergence,informationgain,orrelativeentropy)isawayofcomparingtwodistributions:a"true"probabilitydistributionp(X),andanarbitraryprobabilitydistributionq(X).Ifwecompressdatainamannerthatassumesq(X)isthedistributionunderlyingsomedata,when,inreality,p(X)isthecorrectdistribution,theKullback\xe2\x80\x93Leiblerdivergenceisthenumberofaverageadditionalbitsperdatumnecessaryforcompression.Itisthusdefined\nAlthoughitissometimesusedasa\'distancemetric\',KLdivergenceisnotatruemetricsinceitisnotsymmetricanddoesnotsatisfythetriangleinequality(makingitasemi-quasimetric).\nAnotherinterpretationoftheKLdivergenceisthe"unnecessarysurprise"introducedbyapriorfromthetruth:supposeanumberXisabouttobedrawnrandomlyfromadiscretesetwithprobabilitydistributionp(x).IfAliceknowsthetruedistributionp(x),whileBobbelieves(hasaprior)thatthedistributionisq(x),thenBobwillbemoresurprisedthanAlice,onaverage,uponseeingthevalueofX.TheKLdivergenceisthe(objective)expectedvalueofBob\'s(subjective)surprisalminusAlice\'ssurprisal,measuredinbitsifthelogisinbase2.Inthisway,theextenttowhichBob\'sprioris"wrong"canbequantifiedintermsofhow"unnecessarilysurprised"itisexpectedtomakehim.\nOtherimportantinformationtheoreticquantitiesincludeR\xc3\xa9nyientropy(ageneralizationofentropy),differentialentropy(ageneralizationofquantitiesofinformationtocontinuousdistributions),andtheconditionalmutualinformation.\nCodingtheoryisoneofthemostimportantanddirectapplicationsofinformationtheory.Itcanbesubdividedintosourcecodingtheoryandchannelcodingtheory.Usingastatisticaldescriptionfordata,informationtheoryquantifiesthenumberofbitsneededtodescribethedata,whichistheinformationentropyofthesource.\nThisdivisionofcodingtheoryintocompressionandtransmissionisjustifiedbytheinformationtransmissiontheorems,orsource\xe2\x80\x93channelseparationtheoremsthatjustifytheuseofbitsastheuniversalcurrencyforinformationinmanycontexts.However,thesetheoremsonlyholdinthesituationwhereonetransmittinguserwishestocommunicatetoonereceivinguser.Inscenarioswithmorethanonetransmitter(themultiple-accesschannel),morethanonereceiver(thebroadcastchannel)orintermediary"helpers"(therelaychannel),ormoregeneralnetworks,compressionfollowedbytransmissionmaynolongerbeoptimal.Networkinformationtheoryreferstothesemulti-agentcommunicationmodels.\nAnyprocessthatgeneratessuccessivemessagescanbeconsideredasourceofinformation.Amemorylesssourceisoneinwhicheachmessageisanindependentidenticallydistributedrandomvariable,whereasthepropertiesofergodicityandstationarityimposelessrestrictiveconstraints.Allsuchsourcesarestochastic.Thesetermsarewellstudiedintheirownrightoutsideinformationtheory.\nInformationrateistheaverageentropypersymbol.Formemorylesssources,thisismerelytheentropyofeachsymbol,while,inthecaseofastationarystochasticprocess,itis\nthatis,theconditionalentropyofasymbolgivenalltheprevioussymbolsgenerated.Forthemoregeneralcaseofaprocessthatisnotnecessarilystationary,theaveragerateis\nthatis,thelimitofthejointentropypersymbol.Forstationarysources,thesetwoexpressionsgivethesameresult.[11]\nItiscommonininformationtheorytospeakofthe"rate"or"entropy"ofalanguage.Thisisappropriate,forexample,whenthesourceofinformationisEnglishprose.Therateofasourceofinformationisrelatedtoitsredundancyandhowwellitcanbecompressed,thesubjectofsourcecoding.\nCommunicationsoverachannel\xe2\x80\x94suchasanethernetcable\xe2\x80\x94istheprimarymotivationofinformationtheory.Asanyonewho\'severusedatelephone(mobileorlandline)knows,however,suchchannelsoftenfailtoproduceexactreconstructionofasignal;noise,periodsofsilence,andotherformsofsignalcorruptionoftendegradequality.\nConsiderthecommunicationsprocessoveradiscretechannel.Asimplemodeloftheprocessisshownbelow:\n\n\nHereXrepresentsthespaceofmessagestransmitted,andYthespaceofmessagesreceivedduringaunittimeoverourchannel.Letp(y|x)betheconditionalprobabilitydistributionfunctionofYgivenX.Wewillconsiderp(y|x)tobeaninherentfixedpropertyofourcommunicationschannel(representingthenatureofthenoiseofourchannel).ThenthejointdistributionofXandYiscompletelydeterminedbyourchannelandbyourchoiceoff(x),themarginaldistributionofmessageswechoosetosendoverthechannel.Undertheseconstraints,wewouldliketomaximizetherateofinformation,orthesignal,wecancommunicateoverthechannel.Theappropriatemeasureforthisisthemutualinformation,andthismaximummutualinformationiscalledthechannelcapacityandisgivenby:\nThiscapacityhasthefollowingpropertyrelatedtocommunicatingatinformationrateR(whereRisusuallybitspersymbol).ForanyinformationrateR<Candcodingerror\xce\xb5>0,forlargeenoughN,thereexistsacodeoflengthNandrate\xe2\x89\xa5Randadecodingalgorithm,suchthatthemaximalprobabilityofblockerroris\xe2\x89\xa4\xce\xb5;thatis,itisalwayspossibletotransmitwitharbitrarilysmallblockerror.Inaddition,foranyrateR>C,itisimpossibletotransmitwitharbitrarilysmallblockerror.\nChannelcodingisconcernedwithfindingsuchnearlyoptimalcodesthatcanbeusedtotransmitdataoveranoisychannelwithasmallcodingerrorataratenearthechannelcapacity.\nInformationtheoreticconceptsapplytocryptographyandcryptanalysis.Turing\'sinformationunit,theban,wasusedintheUltraproject,breakingtheGermanEnigmamachinecodeandhasteningtheendofWorldWarIIinEurope.Shannonhimselfdefinedanimportantconceptnowcalledtheunicitydistance.Basedontheredundancyoftheplaintext,itattemptstogiveaminimumamountofciphertextnecessarytoensureuniquedecipherability.\nInformationtheoryleadsustobelieveitismuchmoredifficulttokeepsecretsthanitmightfirstappear.Abruteforceattackcanbreaksystemsbasedonasymmetrickeyalgorithmsoronmostcommonlyusedmethodsofsymmetrickeyalgorithms(sometimescalledsecretkeyalgorithms),suchasblockciphers.Thesecurityofallsuchmethodscurrentlycomesfromtheassumptionthatnoknownattackcanbreaktheminapracticalamountoftime.\nInformationtheoreticsecurityreferstomethodssuchastheone-timepadthatarenotvulnerabletosuchbruteforceattacks.Insuchcases,thepositiveconditionalmutualinformationbetweentheplaintextandciphertext(conditionedonthekey)canensurepropertransmission,whiletheunconditionalmutualinformationbetweentheplaintextandciphertextremainszero,resultinginabsolutelysecurecommunications.Inotherwords,aneavesdropperwouldnotbeabletoimprovehisorherguessoftheplaintextbygainingknowledgeoftheciphertextbutnotofthekey.However,asinanyothercryptographicsystem,caremustbeusedtocorrectlyapplyeveninformation-theoreticallysecuremethods;theVenonaprojectwasabletocracktheone-timepadsoftheSovietUnionduetotheirimproperreuseofkeymaterial.\nPseudorandomnumbergeneratorsarewidelyavailableincomputerlanguagelibrariesandapplicationprograms.Theyare,almostuniversally,unsuitedtocryptographicuseastheydonotevadethedeterministicnatureofmoderncomputerequipmentandsoftware.Aclassofimprovedrandomnumbergeneratorsistermedcryptographicallysecurepseudorandomnumbergenerators,buteventheyrequirerandomseedsexternaltothesoftwaretoworkasintended.Thesecanbeobtainedviaextractors,ifdonecarefully.Themeasureofsufficientrandomnessinextractorsismin-entropy,avaluerelatedtoShannonentropythroughR\xc3\xa9nyientropy;R\xc3\xa9nyientropyisalsousedinevaluatingrandomnessincryptographicsystems.Althoughrelated,thedistinctionsamongthesemeasuresmeanthatarandomvariablewithhighShannonentropyisnotnecessarilysatisfactoryforuseinanextractorandsoforcryptographyuses.\nOneearlycommercialapplicationofinformationtheorywasinthefieldofseismicoilexploration.Workinthisfieldmadeitpossibletostripoffandseparatetheunwantednoisefromthedesiredseismicsignal.Informationtheoryanddigitalsignalprocessingofferamajorimprovementofresolutionandimageclarityoverpreviousanalogmethods.[12]\nSemioticiansDoedeNautaandWinfriedN\xc3\xb6thbothconsideredCharlesSandersPierceashavingcreatedatheoryofinformationinhisworksonsemiotics.[13]:171[14]:137Nautadefinedsemioticinformationtheoryasthestudyof"theinternalprocessesofcoding,filtering,andinformationprocessing."[13]:91\nConceptsfrominformationtheorysuchasredundancyandcodecontrolhavebeenusedbysemioticianssuchasUmbertoEcoandFerruccioRossi-Landitoexplainideologyasaformofmessagetransmissionwherebyadominantsocialclassemitsitsmessagebyusingsignsthatexhibitahighdegreeofredundancysuchthatonlyonemessageisdecodedamongaselectionofcompetingones.[15]\nInformationtheoryalsohasapplicationsingamblingandinvesting,blackholes,andbioinformatics.\n