Parallelcomputingisatypeofcomputationinwhichmanycalculationsortheexecutionofprocessesarecarriedoutsimultaneously.[1]Largeproblemscanoftenbedividedintosmallerones,whichcanthenbesolvedatthesametime.Thereareseveraldifferentformsofparallelcomputing:bit-level,instruction-level,data,andtaskparallelism.Parallelismhaslongbeenemployedinhigh-performancecomputing,butit\'sgainingbroaderinterestduetothephysicalconstraintspreventingfrequencyscaling.[2]Aspowerconsumption(andconsequentlyheatgeneration)bycomputershasbecomeaconcerninrecentyears,[3]parallelcomputinghasbecomethedominantparadigmincomputerarchitecture,mainlyintheformofmulti-coreprocessors.[4]\nParallelcomputingiscloselyrelatedtoconcurrentcomputing\xe2\x80\x94theyarefrequentlyusedtogether,andoftenconflated,thoughthetwoaredistinct:itispossibletohaveparallelismwithoutconcurrency(suchasbit-levelparallelism),andconcurrencywithoutparallelism(suchasmultitaskingbytime-sharingonasingle-coreCPU).[5][6]Inparallelcomputing,acomputationaltaskistypicallybrokendownintoseveral,oftenmany,verysimilarsubtasksthatcanbeprocessedindependentlyandwhoseresultsarecombinedafterwards,uponcompletion.Incontrast,inconcurrentcomputing,thevariousprocessesoftendonotaddressrelatedtasks;whentheydo,asistypicalindistributedcomputing,theseparatetasksmayhaveavariednatureandoftenrequiresomeinter-processcommunicationduringexecution.\nParallelcomputerscanberoughlyclassifiedaccordingtothelevelatwhichthehardwaresupportsparallelism,withmulti-coreandmulti-processorcomputershavingmultipleprocessingelementswithinasinglemachine,whileclusters,MPPs,andgridsusemultiplecomputerstoworkonthesametask.Specializedparallelcomputerarchitecturesaresometimesusedalongsidetraditionalprocessors,foracceleratingspecifictasks.\nInsomecasesparallelismistransparenttotheprogrammer,suchasinbit-levelorinstruction-levelparallelism,butexplicitlyparallelalgorithms,particularlythosethatuseconcurrency,aremoredifficulttowritethansequentialones,[7]becauseconcurrencyintroducesseveralnewclassesofpotentialsoftwarebugs,ofwhichraceconditionsarethemostcommon.Communicationandsynchronizationbetweenthedifferentsubtasksaretypicallysomeofthegreatestobstaclestogettinggoodparallelprogramperformance.\nAtheoreticalupperboundonthespeed-upofasingleprogramasaresultofparallelizationisgivenbyAmdahl\'slaw.\nTraditionally,computersoftwarehasbeenwrittenforserialcomputation.Tosolveaproblem,analgorithmisconstructedandimplementedasaserialstreamofinstructions.Theseinstructionsareexecutedonacentralprocessingunitononecomputer.Onlyoneinstructionmayexecuteatatime\xe2\x80\x94afterthatinstructionisfinished,thenextoneisexecuted.[8]\nParallelcomputing,ontheotherhand,usesmultipleprocessingelementssimultaneouslytosolveaproblem.Thisisaccomplishedbybreakingtheproblemintoindependentpartssothateachprocessingelementcanexecuteitspartofthealgorithmsimultaneouslywiththeothers.Theprocessingelementscanbediverseandincluderesourcessuchasasinglecomputerwithmultipleprocessors,severalnetworkedcomputers,specializedhardware,oranycombinationoftheabove.[8]Historicallyparallelcomputingwasusedforscientificcomputingandthesimulationofscientificproblems,particularlyinthenaturalandengineeringsciences,suchasmeteorology.Thisledtothedesignofparallelhardwareandsoftware,aswellashighperformancecomputing.[9]\nFrequencyscalingwasthedominantreasonforimprovementsincomputerperformancefromthemid-1980suntil2004.Theruntimeofaprogramisequaltothenumberofinstructionsmultipliedbytheaveragetimeperinstruction.Maintainingeverythingelseconstant,increasingtheclockfrequencydecreasestheaveragetimeittakestoexecuteaninstruction.Anincreaseinfrequencythusdecreasesruntimeforallcompute-boundprograms.[10]However,powerconsumptionPbyachipisgivenbytheequationP=C\xc3\x97V2\xc3\x97F,whereCisthecapacitancebeingswitchedperclockcycle(proportionaltothenumberoftransistorswhoseinputschange),Visvoltage,andFistheprocessorfrequency(cyclespersecond).[11]Increasesinfrequencyincreasetheamountofpowerusedinaprocessor.IncreasingprocessorpowerconsumptionledultimatelytoIntel\'sMay8,2004cancellationofitsTejasandJayhawkprocessors,whichisgenerallycitedastheendoffrequencyscalingasthedominantcomputerarchitectureparadigm.[12]\nTodealwiththeproblemofpowerconsumptionandoverheatingthemajorcentralprocessingunit(CPUorprocessor)manufacturersstartedtoproducepowerefficientprocessorswithmultiplecores.Thecoreisthecomputingunitoftheprocessorandinmulti-coreprocessorseachcoreisindependentandcanaccessthesamememoryconcurrently.Multi-coreprocessorshavebroughtparallelcomputingtodesktopcomputers.Thusparallelisationofserialprogrammeshasbecomeamainstreamprogrammingtask.In2012quad-coreprocessorsbecamestandardfordesktopcomputers,whileservershave10and12coreprocessors.FromMoore\'slawitcanbepredictedthatthenumberofcoresperprocessorwilldoubleevery18\xe2\x80\x9324months.Thiscouldmeanthatafter2020atypicalprocessorwillhavedozensorhundredsofcores.[13]\nAnoperatingsystemcanensurethatdifferenttasksanduserprogrammesareruninparallelontheavailablecores.However,foraserialsoftwareprogrammetotakefulladvantageofthemulti-corearchitecturetheprogrammerneedstorestructureandparallelisethecode.Aspeed-upofapplicationsoftwareruntimewillnolongerbeachievedthroughfrequencyscaling,insteadprogrammerswillneedtoparallelisetheirsoftwarecodetotakeadvantageoftheincreasingcomputingpowerofmulticorearchitectures.[14]\nOptimally,thespeedupfromparallelizationwouldbelinear\xe2\x80\x94doublingthenumberofprocessingelementsshouldhalvetheruntime,anddoublingitasecondtimeshouldagainhalvetheruntime.However,veryfewparallelalgorithmsachieveoptimalspeedup.Mostofthemhaveanear-linearspeedupforsmallnumbersofprocessingelements,whichflattensoutintoaconstantvalueforlargenumbersofprocessingelements.\nThepotentialspeedupofanalgorithmonaparallelcomputingplatformisgivenbyAmdahl\'slaw[15]\nwhere\nSinceSlatency<1/(1-p),itshowsthatasmallpartoftheprogramwhichcannotbeparallelizedwilllimittheoverallspeedupavailablefromparallelization.Aprogramsolvingalargemathematicalorengineeringproblemwilltypicallyconsistofseveralparallelizablepartsandseveralnon-parallelizable(serial)parts.Ifthenon-parallelizablepartofaprogramaccountsfor10%oftheruntime(p=0.9),wecangetnomorethana10timesspeedup,regardlessofhowmanyprocessorsareadded.Thisputsanupperlimitontheusefulnessofaddingmoreparallelexecutionunits."Whenataskcannotbepartitionedbecauseofsequentialconstraints,theapplicationofmoreefforthasnoeffectontheschedule.Thebearingofachildtakesninemonths,nomatterhowmanywomenareassigned."[16]\nAmdahl\'slawonlyappliestocaseswheretheproblemsizeisfixed.Inpractice,asmorecomputingresourcesbecomeavailable,theytendtogetusedonlargerproblems(largerdatasets),andthetimespentintheparallelizablepartoftengrowsmuchfasterthantheinherentlyserialwork.[17]Inthiscase,Gustafson\'slawgivesalesspessimisticandmorerealisticassessmentofparallelperformance:[18]\nBothAmdahl\'slawandGustafson\'slawassumethattherunningtimeoftheserialpartoftheprogramisindependentofthenumberofprocessors.Amdahl\'slawassumesthattheentireproblemisoffixedsizesothatthetotalamountofworktobedoneinparallelisalsoindependentofthenumberofprocessors,whereasGustafson\'slawassumesthatthetotalamountofworktobedoneinparallelvarieslinearlywiththenumberofprocessors.\nUnderstandingdatadependenciesisfundamentalinimplementingparallelalgorithms.Noprogramcanrunmorequicklythanthelongestchainofdependentcalculations(knownasthecriticalpath),sincecalculationsthatdependuponpriorcalculationsinthechainmustbeexecutedinorder.However,mostalgorithmsdonotconsistofjustalongchainofdependentcalculations;thereareusuallyopportunitiestoexecuteindependentcalculationsinparallel.\nLetPiandPjbetwoprogramsegments.Bernstein\'sconditions[19]describewhenthetwoareindependentandcanbeexecutedinparallel.ForPi,letIibealloftheinputvariablesandOitheoutputvariables,andlikewiseforPj.PiandPjareindependentiftheysatisfy\nViolationofthefirstconditionintroducesaflowdependency,correspondingtothefirstsegmentproducingaresultusedbythesecondsegment.Thesecondconditionrepresentsananti-dependency,whenthesecondsegmentproducesavariableneededbythefirstsegment.Thethirdandfinalconditionrepresentsanoutputdependency:whentwosegmentswritetothesamelocation,theresultcomesfromthelogicallylastexecutedsegment.[20]\nConsiderthefollowingfunctions,whichdemonstrateseveralkindsofdependencies:\nInthisexample,instruction3cannotbeexecutedbefore(oreveninparallelwith)instruction2,becauseinstruction3usesaresultfrominstruction2.Itviolatescondition1,andthusintroducesaflowdependency.\nInthisexample,therearenodependenciesbetweentheinstructions,sotheycanallberuninparallel.\nBernstein\'sconditionsdonotallowmemorytobesharedbetweendifferentprocesses.Forthat,somemeansofenforcinganorderingbetweenaccessesisnecessary,suchassemaphores,barriersorsomeothersynchronizationmethod.\nSubtasksinaparallelprogramareoftencalledthreads.Someparallelcomputerarchitecturesusesmaller,lightweightversionsofthreadsknownasfibers,whileothersusebiggerversionsknownasprocesses.However,"threads"isgenerallyacceptedasagenerictermforsubtasks.[21]Threadswilloftenneedsynchronizedaccesstoanobjectorotherresource,forexamplewhentheymustupdateavariablethatissharedbetweenthem.Withoutsynchronization,theinstructionsbetweenthetwothreadsmaybeinterleavedinanyorder.Forexample,considerthefollowingprogram:\nIfinstruction1Bisexecutedbetween1Aand3A,orifinstruction1Aisexecutedbetween1Band3B,theprogramwillproduceincorrectdata.Thisisknownasaracecondition.Theprogrammermustusealocktoprovidemutualexclusion.Alockisaprogramminglanguageconstructthatallowsonethreadtotakecontrolofavariableandpreventotherthreadsfromreadingorwritingit,untilthatvariableisunlocked.Thethreadholdingthelockisfreetoexecuteitscriticalsection(thesectionofaprogramthatrequiresexclusiveaccesstosomevariable),andtounlockthedatawhenitisfinished.Therefore,toguaranteecorrectprogramexecution,theaboveprogramcanberewrittentouselocks:\nOnethreadwillsuccessfullylockvariableV,whiletheotherthreadwillbelockedout\xe2\x80\x94unabletoproceeduntilVisunlockedagain.Thisguaranteescorrectexecutionoftheprogram.Locksmaybenecessarytoensurecorrectprogramexecutionwhenthreadsmustserializeaccesstoresources,buttheirusecangreatlyslowaprogramandmayaffectitsreliability.[22]\nLockingmultiplevariablesusingnon-atomiclocksintroducesthepossibilityofprogramdeadlock.Anatomiclocklocksmultiplevariablesallatonce.Ifitcannotlockallofthem,itdoesnotlockanyofthem.Iftwothreadseachneedtolockthesametwovariablesusingnon-atomiclocks,itispossiblethatonethreadwilllockoneofthemandthesecondthreadwilllockthesecondvariable.Insuchacase,neitherthreadcancomplete,anddeadlockresults.[23]\nManyparallelprogramsrequirethattheirsubtasksactinsynchrony.Thisrequirestheuseofabarrier.Barriersaretypicallyimplementedusingalockorasemaphore.[24]Oneclassofalgorithms,knownaslock-freeandwait-freealgorithms,altogetheravoidstheuseoflocksandbarriers.However,thisapproachisgenerallydifficulttoimplementandrequirescorrectlydesigneddatastructures.[25]\nNotallparallelizationresultsinspeed-up.Generally,asataskissplitupintomoreandmorethreads,thosethreadsspendanever-increasingportionoftheirtimecommunicatingwitheachotherorwaitingoneachotherforaccesstoresources.[26][27]Oncetheoverheadfromresourcecontentionorcommunicationdominatesthetimespentonothercomputation,furtherparallelization(thatis,splittingtheworkloadoverevenmorethreads)increasesratherthandecreasestheamountoftimerequiredtofinish.Thisproblem,knownasparallelslowdown,[28]canbeimprovedinsomecasesbysoftwareanalysisandredesign.[29]\nApplicationsareoftenclassifiedaccordingtohowoftentheirsubtasksneedtosynchronizeorcommunicatewitheachother.Anapplicationexhibitsfine-grainedparallelismifitssubtasksmustcommunicatemanytimespersecond;itexhibitscoarse-grainedparallelismiftheydonotcommunicatemanytimespersecond,anditexhibitsembarrassingparallelismiftheyrarelyorneverhavetocommunicate.Embarrassinglyparallelapplicationsareconsideredtheeasiesttoparallelize.\nParallelprogramminglanguagesandparallelcomputersmusthaveaconsistencymodel(alsoknownasamemorymodel).Theconsistencymodeldefinesrulesforhowoperationsoncomputermemoryoccurandhowresultsareproduced.\nOneofthefirstconsistencymodelswasLeslieLamport\'ssequentialconsistencymodel.Sequentialconsistencyisthepropertyofaparallelprogramthatitsparallelexecutionproducesthesameresultsasasequentialprogram.Specifically,aprogramissequentiallyconsistentif"theresultsofanyexecutionisthesameasiftheoperationsofalltheprocessorswereexecutedinsomesequentialorder,andtheoperationsofeachindividualprocessorappearinthissequenceintheorderspecifiedbyitsprogram".[30]\nSoftwaretransactionalmemoryisacommontypeofconsistencymodel.Softwaretransactionalmemoryborrowsfromdatabasetheorytheconceptofatomictransactionsandappliesthemtomemoryaccesses.\nMathematically,thesemodelscanberepresentedinseveralways.Introducedin1962,Petrinetswereanearlyattempttocodifytherulesofconsistencymodels.Dataflowtheorylaterbuiltuponthese,andDataflowarchitectureswerecreatedtophysicallyimplementtheideasofdataflowtheory.Beginninginthelate1970s,processcalculisuchasCalculusofCommunicatingSystemsandCommunicatingSequentialProcessesweredevelopedtopermitalgebraicreasoningaboutsystemscomposedofinteractingcomponents.Morerecentadditionstotheprocesscalculusfamily,suchasthe\xcf\x80-calculus,haveaddedthecapabilityforreasoningaboutdynamictopologies.LogicssuchasLamport\'sTLA+,andmathematicalmodelssuchastracesandActoreventdiagrams,havealsobeendevelopedtodescribethebehaviorofconcurrentsystems.\nMichaelJ.Flynncreatedoneoftheearliestclassificationsystemsforparallel(andsequential)computersandprograms,nowknownasFlynn\'staxonomy.Flynnclassifiedprogramsandcomputersbywhethertheywereoperatingusingasinglesetormultiplesetsofinstructions,andwhetherornotthoseinstructionswereusingasinglesetormultiplesetsofdata.\nThesingle-instruction-single-data(SISD)classificationisequivalenttoanentirelysequentialprogram.Thesingle-instruction-multiple-data(SIMD)classificationisanalogoustodoingthesameoperationrepeatedlyoveralargedataset.Thisiscommonlydoneinsignalprocessingapplications.Multiple-instruction-single-data(MISD)isararelyusedclassification.Whilecomputerarchitecturestodealwiththisweredevised(suchassystolicarrays),fewapplicationsthatfitthisclassmaterialized.Multiple-instruction-multiple-data(MIMD)programsarebyfarthemostcommontypeofparallelprograms.\nAccordingtoDavidA.PattersonandJohnL.Hennessy,"Somemachinesarehybridsofthesecategories,ofcourse,butthisclassicmodelhassurvivedbecauseitissimple,easytounderstand,andgivesagoodfirstapproximation.Itisalso\xe2\x80\x94perhapsbecauseofitsunderstandability\xe2\x80\x94themostwidelyusedscheme."[31]\nFromtheadventofvery-large-scaleintegration(VLSI)computer-chipfabricationtechnologyinthe1970suntilabout1986,speed-upincomputerarchitecturewasdrivenbydoublingcomputerwordsize\xe2\x80\x94theamountofinformationtheprocessorcanmanipulatepercycle.[32]Increasingthewordsizereducesthenumberofinstructionstheprocessormustexecutetoperformanoperationonvariableswhosesizesaregreaterthanthelengthoftheword.Forexample,wherean8-bitprocessormustaddtwo16-bitintegers,theprocessormustfirstaddthe8 lower-orderbitsfromeachintegerusingthestandardadditioninstruction,thenaddthe8 higher-orderbitsusinganadd-with-carryinstructionandthecarrybitfromthelowerorderaddition;thus,an8-bitprocessorrequirestwoinstructionstocompleteasingleoperation,wherea16-bitprocessorwouldbeabletocompletetheoperationwithasingleinstruction.\nHistorically,4-bitmicroprocessorswerereplacedwith8-bit,then16-bit,then32-bitmicroprocessors.Thistrendgenerallycametoanendwiththeintroductionof32-bitprocessors,whichhasbeenastandardingeneral-purposecomputingfortwodecades.Notuntiltheearly2000s,withtheadventofx86-64architectures,did64-bitprocessorsbecomecommonplace.\nAcomputerprogramis,inessence,astreamofinstructionsexecutedbyaprocessor.Withoutinstruction-levelparallelism,aprocessorcanonlyissuelessthanoneinstructionperclockcycle(IPC<1).Theseprocessorsareknownassubscalarprocessors.Theseinstructionscanbere-orderedandcombinedintogroupswhicharethenexecutedinparallelwithoutchangingtheresultoftheprogram.Thisisknownasinstruction-levelparallelism.Advancesininstruction-levelparallelismdominatedcomputerarchitecturefromthemid-1980suntilthemid-1990s.[33]\nAllmodernprocessorshavemulti-stageinstructionpipelines.Eachstageinthepipelinecorrespondstoadifferentactiontheprocessorperformsonthatinstructioninthatstage;aprocessorwithanN-stagepipelinecanhaveuptoNdifferentinstructionsatdifferentstagesofcompletionandthuscanissueoneinstructionperclockcycle(IPC=1).Theseprocessorsareknownasscalarprocessors.ThecanonicalexampleofapipelinedprocessorisaRISCprocessor,withfivestages:instructionfetch(IF),instructiondecode(ID),execute(EX),memoryaccess(MEM),andregisterwriteback(WB).ThePentium4processorhada35-stagepipeline.[34]\nMostmodernprocessorsalsohavemultipleexecutionunits.Theyusuallycombinethisfeaturewithpipeliningandthuscanissuemorethanoneinstructionperclockcycle(IPC>1).Theseprocessorsareknownassuperscalarprocessors.Instructionscanbegroupedtogetheronlyifthereisnodatadependencybetweenthem.ScoreboardingandtheTomasuloalgorithm(whichissimilartoscoreboardingbutmakesuseofregisterrenaming)aretwoofthemostcommontechniquesforimplementingout-of-orderexecutionandinstruction-levelparallelism.\nTaskparallelismsisthecharacteristicofaparallelprogramthat"entirelydifferentcalculationscanbeperformedoneitherthesameordifferentsetsofdata".[35]Thiscontrastswithdataparallelism,wherethesamecalculationisperformedonthesameordifferentsetsofdata.Taskparallelisminvolvesthedecompositionofataskintosub-tasksandthenallocatingeachsub-tasktoaprocessorforexecution.Theprocessorswouldthenexecutethesesub-tasksconcurrentlyandoftencooperatively.Taskparallelismdoesnotusuallyscalewiththesizeofaproblem.[36]\nMainmemoryinaparallelcomputeriseithersharedmemory(sharedbetweenallprocessingelementsinasingleaddressspace),ordistributedmemory(inwhicheachprocessingelementhasitsownlocaladdressspace).[37]Distributedmemoryreferstothefactthatthememoryislogicallydistributed,butoftenimpliesthatitisphysicallydistributedaswell.Distributedsharedmemoryandmemoryvirtualizationcombinethetwoapproaches,wheretheprocessingelementhasitsownlocalmemoryandaccesstothememoryonnon-localprocessors.Accessestolocalmemoryaretypicallyfasterthanaccessestonon-localmemory.\nComputerarchitecturesinwhicheachelementofmainmemorycanbeaccessedwithequallatencyandbandwidthareknownasuniformmemoryaccess(UMA)systems.Typically,thatcanbeachievedonlybyasharedmemorysystem,inwhichthememoryisnotphysicallydistributed.Asystemthatdoesnothavethispropertyisknownasanon-uniformmemoryaccess(NUMA)architecture.Distributedmemorysystemshavenon-uniformmemoryaccess.\nComputersystemsmakeuseofcaches\xe2\x80\x94smallandfastmemorieslocatedclosetotheprocessorwhichstoretemporarycopiesofmemoryvalues(nearbyinboththephysicalandlogicalsense).Parallelcomputersystemshavedifficultieswithcachesthatmaystorethesamevalueinmorethanonelocation,withthepossibilityofincorrectprogramexecution.Thesecomputersrequireacachecoherencysystem,whichkeepstrackofcachedvaluesandstrategicallypurgesthem,thusensuringcorrectprogramexecution.Bussnoopingisoneofthemostcommonmethodsforkeepingtrackofwhichvaluesarebeingaccessed(andthusshouldbepurged).Designinglarge,high-performancecachecoherencesystemsisaverydifficultproblemincomputerarchitecture.Asaresult,sharedmemorycomputerarchitecturesdonotscaleaswellasdistributedmemorysystemsdo.[37]\nProcessor\xe2\x80\x93processorandprocessor\xe2\x80\x93memorycommunicationcanbeimplementedinhardwareinseveralways,includingviashared(eithermultiportedormultiplexed)memory,acrossbarswitch,asharedbusoraninterconnectnetworkofamyriadoftopologiesincludingstar,ring,tree,hypercube,fathypercube(ahypercubewithmorethanoneprocessoratanode),orn-dimensionalmesh.\nParallelcomputersbasedoninterconnectednetworksneedtohavesomekindofroutingtoenablethepassingofmessagesbetweennodesthatarenotdirectlyconnected.Themediumusedforcommunicationbetweentheprocessorsislikelytobehierarchicalinlargemultiprocessormachines.\nParallelcomputerscanberoughlyclassifiedaccordingtothelevelatwhichthehardwaresupportsparallelism.Thisclassificationisbroadlyanalogoustothedistancebetweenbasiccomputingnodes.Thesearenotmutuallyexclusive;forexample,clustersofsymmetricmultiprocessorsarerelativelycommon.\nAmulti-coreprocessorisaprocessorthatincludesmultipleprocessingunits(called"cores")onthesamechip.Thisprocessordiffersfromasuperscalarprocessor,whichincludesmultipleexecutionunitsandcanissuemultipleinstructionsperclockcyclefromoneinstructionstream(thread);incontrast,amulti-coreprocessorcanissuemultipleinstructionsperclockcyclefrommultipleinstructionstreams.IBM\'sCellmicroprocessor,designedforuseintheSonyPlayStation3,isaprominentmulti-coreprocessor.Eachcoreinamulti-coreprocessorcanpotentiallybesuperscalaraswell\xe2\x80\x94thatis,oneveryclockcycle,eachcorecanissuemultipleinstructionsfromonethread.\nInstructionpipelining(ofwhichIntel\'sHyper-Threadingisthebestknown)wasanearlyformofpseudo-multi-coreism.Aprocessorcapableofconcurrentmultithreadingincludesmultipleexecutionunitsinthesameprocessingunit\xe2\x80\x94thatisithasasuperscalararchitecture\xe2\x80\x94andcanissuemultipleinstructionsperclockcyclefrommultiplethreads.Temporalmultithreadingontheotherhandincludesasingleexecutionunitinthesameprocessingunitandcanissueoneinstructionatatimefrommultiplethreads.\nAsymmetricmultiprocessor(SMP)isacomputersystemwithmultipleidenticalprocessorsthatsharememoryandconnectviaabus.[38]Buscontentionpreventsbusarchitecturesfromscaling.Asaresult,SMPsgenerallydonotcomprisemorethan32 processors.[39]Becauseofthesmallsizeoftheprocessorsandthesignificantreductionintherequirementsforbusbandwidthachievedbylargecaches,suchsymmetricmultiprocessorsareextremelycost-effective,providedthatasufficientamountofmemorybandwidthexists.[38]\nAdistributedcomputer(alsoknownasadistributedmemorymultiprocessor)isadistributedmemorycomputersysteminwhichtheprocessingelementsareconnectedbyanetwork.Distributedcomputersarehighlyscalable.Theterms"concurrentcomputing","parallelcomputing",and"distributedcomputing"havealotofoverlap,andnocleardistinctionexistsbetweenthem.[40]Thesamesystemmaybecharacterizedbothas"parallel"and"distributed";theprocessorsinatypicaldistributedsystemrunconcurrentlyinparallel.[41]\nAclusterisagroupoflooselycoupledcomputersthatworktogetherclosely,sothatinsomerespectstheycanberegardedasasinglecomputer.[42]Clustersarecomposedofmultiplestandalonemachinesconnectedbyanetwork.Whilemachinesinaclusterdonothavetobesymmetric,loadbalancingismoredifficultiftheyarenot.ThemostcommontypeofclusteristheBeowulfcluster,whichisaclusterimplementedonmultipleidenticalcommercialoff-the-shelfcomputersconnectedwithaTCP/IPEthernetlocalareanetwork.[43]BeowulftechnologywasoriginallydevelopedbyThomasSterlingandDonaldBecker.87%ofallTop500supercomputersareclusters.[44]TheremainingareMassivelyParallelProcessors,explainedbelow.\nBecausegridcomputingsystems(describedbelow)caneasilyhandleembarrassinglyparallelproblems,modernclustersaretypicallydesignedtohandlemoredifficultproblems\xe2\x80\x94problemsthatrequirenodestoshareintermediateresultswitheachothermoreoften.Thisrequiresahighbandwidthand,moreimportantly,alow-latencyinterconnectionnetwork.Manyhistoricandcurrentsupercomputersusecustomizedhigh-performancenetworkhardwarespecificallydesignedforclustercomputing,suchastheCrayGemininetwork.[45]Asof2014,mostcurrentsupercomputersusesomeoff-the-shelfstandardnetworkhardware,oftenMyrinet,InfiniBand,orGigabitEthernet.\nAmassivelyparallelprocessor(MPP)isasinglecomputerwithmanynetworkedprocessors.MPPshavemanyofthesamecharacteristicsasclusters,butMPPshavespecializedinterconnectnetworks(whereasclustersusecommodityhardwarefornetworking).MPPsalsotendtobelargerthanclusters,typicallyhaving"farmore"than100 processors.[46]InanMPP,"eachCPUcontainsitsownmemoryandcopyoftheoperatingsystemandapplication.Eachsubsystemcommunicateswiththeothersviaahigh-speedinterconnect."[47]\nIBM\'sBlueGene/L,thefifthfastestsupercomputerintheworldaccordingtotheJune2009TOP500ranking,isanMPP.\nGridcomputingisthemostdistributedformofparallelcomputing.ItmakesuseofcomputerscommunicatingovertheInternettoworkonagivenproblem.BecauseofthelowbandwidthandextremelyhighlatencyavailableontheInternet,distributedcomputingtypicallydealsonlywithembarrassinglyparallelproblems.Manydistributedcomputingapplicationshavebeencreated,ofwhichSETI@homeandFolding@homearethebest-knownexamples.[48]\nMostgridcomputingapplicationsusemiddleware(softwarethatsitsbetweentheoperatingsystemandtheapplicationtomanagenetworkresourcesandstandardizethesoftwareinterface).ThemostcommondistributedcomputingmiddlewareistheBerkeleyOpenInfrastructureforNetworkComputing(BOINC).Often,distributedcomputingsoftwaremakesuseof"sparecycles",performingcomputationsattimeswhenacomputerisidling.\nWithinparallelcomputing,therearespecializedparalleldevicesthatremainnicheareasofinterest.Whilenotdomain-specific,theytendtobeapplicabletoonlyafewclassesofparallelproblems.\nReconfigurablecomputingistheuseofafield-programmablegatearray(FPGA)asaco-processortoageneral-purposecomputer.AnFPGAis,inessence,acomputerchipthatcanrewireitselfforagiventask.\nFPGAscanbeprogrammedwithhardwaredescriptionlanguagessuchasVHDLorVerilog.However,programmingintheselanguagescanbetedious.SeveralvendorshavecreatedCtoHDLlanguagesthatattempttoemulatethesyntaxandsemanticsoftheCprogramminglanguage,withwhichmostprogrammersarefamiliar.ThebestknownCtoHDLlanguagesareMitrion-C,ImpulseC,DIME-C,andHandel-C.SpecificsubsetsofSystemCbasedonC++canalsobeusedforthispurpose.\nAMD\'sdecisiontoopenitsHyperTransporttechnologytothird-partyvendorshasbecometheenablingtechnologyforhigh-performancereconfigurablecomputing.[49]AccordingtoMichaelR.D\'Amour,ChiefOperatingOfficerofDRCComputerCorporation,"whenwefirstwalkedintoAMD,theycalledus\'thesocketstealers.\'Nowtheycallustheirpartners."[49]\nGeneral-purposecomputingongraphicsprocessingunits(GPGPU)isafairlyrecenttrendincomputerengineeringresearch.GPUsareco-processorsthathavebeenheavilyoptimizedforcomputergraphicsprocessing.[50]Computergraphicsprocessingisafielddominatedbydataparalleloperations\xe2\x80\x94particularlylinearalgebramatrixoperations.\nIntheearlydays,GPGPUprogramsusedthenormalgraphicsAPIsforexecutingprograms.However,severalnewprogramminglanguagesandplatformshavebeenbuilttodogeneralpurposecomputationonGPUswithbothNvidiaandAMDreleasingprogrammingenvironmentswithCUDAandStreamSDKrespectively.OtherGPUprogramminglanguagesincludeBrookGPU,PeakStream,andRapidMind.NvidiahasalsoreleasedspecificproductsforcomputationintheirTeslaseries.ThetechnologyconsortiumKhronosGrouphasreleasedtheOpenCLspecification,whichisaframeworkforwritingprogramsthatexecuteacrossplatformsconsistingofCPUsandGPUs.AMD,Apple,Intel,NvidiaandothersaresupportingOpenCL.\nSeveralapplication-specificintegratedcircuit(ASIC)approacheshavebeendevisedfordealingwithparallelapplications.[51][52][53]\nBecauseanASICis(bydefinition)specifictoagivenapplication,itcanbefullyoptimizedforthatapplication.Asaresult,foragivenapplication,anASICtendstooutperformageneral-purposecomputer.However,ASICsarecreatedbyUVphotolithography.Thisprocessrequiresamaskset,whichcanbeextremelyexpensive.AmasksetcancostoveramillionUSdollars.[54](Thesmallerthetransistorsrequiredforthechip,themoreexpensivethemaskwillbe.)Meanwhile,performanceincreasesingeneral-purposecomputingovertime(asdescribedbyMoore\'slaw)tendtowipeoutthesegainsinonlyoneortwochipgenerations.[49]Highinitialcost,andthetendencytobeovertakenbyMoore\'s-law-drivengeneral-purposecomputing,hasrenderedASICsunfeasibleformostparallelcomputingapplications.However,somehavebeenbuilt.OneexampleisthePFLOPSRIKENMDGRAPE-3machinewhichusescustomASICsformoleculardynamicssimulation.\nAvectorprocessorisaCPUorcomputersystemthatcanexecutethesameinstructiononlargesetsofdata.Vectorprocessorshavehigh-leveloperationsthatworkonlineararraysofnumbersorvectors.AnexamplevectoroperationisA=B\xc3\x97C,whereA,B,andCareeach64-elementvectorsof64-bitfloating-pointnumbers.[55]TheyarecloselyrelatedtoFlynn\'sSIMDclassification.[55]\nCraycomputersbecamefamousfortheirvector-processingcomputersinthe1970sand1980s.However,vectorprocessors\xe2\x80\x94bothasCPUsandasfullcomputersystems\xe2\x80\x94havegenerallydisappeared.Modernprocessorinstructionsetsdoincludesomevectorprocessinginstructions,suchaswithFreescaleSemiconductor\'sAltiVecandIntel\'sStreamingSIMDExtensions(SSE).\nConcurrentprogramminglanguages,libraries,APIs,andparallelprogrammingmodels(suchasalgorithmicskeletons)havebeencreatedforprogrammingparallelcomputers.Thesecangenerallybedividedintoclassesbasedontheassumptionstheymakeabouttheunderlyingmemoryarchitecture\xe2\x80\x94sharedmemory,distributedmemory,orshareddistributedmemory.Sharedmemoryprogramminglanguagescommunicatebymanipulatingsharedmemoryvariables.Distributedmemoryusesmessagepassing.POSIXThreadsandOpenMParetwoofthemostwidelyusedsharedmemoryAPIs,whereasMessagePassingInterface(MPI)isthemostwidelyusedmessage-passingsystemAPI.[56]Oneconceptusedinprogrammingparallelprogramsisthefutureconcept,whereonepartofaprogrampromisestodeliverarequireddatumtoanotherpartofaprogramatsomefuturetime.\nCAPSentrepriseandPathscalearealsocoordinatingtheirefforttomakehybridmulti-coreparallelprogramming(HMPP)directivesanopenstandardcalledOpenHMPP.TheOpenHMPPdirective-basedprogrammingmodeloffersasyntaxtoefficientlyoffloadcomputationsonhardwareacceleratorsandtooptimizedatamovementto/fromthehardwarememory.OpenHMPPdirectivesdescriberemoteprocedurecall(RPC)onanacceleratordevice(e.g.GPU)ormoregenerallyasetofcores.ThedirectivesannotateCorFortrancodestodescribetwosetsoffunctionalities:theoffloadingofprocedures(denotedcodelets)ontoaremotedeviceandtheoptimizationofdatatransfersbetweentheCPUmainmemoryandtheacceleratormemory.\nTheriseofconsumerGPUshasledtosupportforcomputekernels,eitheringraphicsAPIs(referredtoascomputeshaders),indedicatedAPIs(suchasOpenCL),orinotherlanguageextensions.\nAutomaticparallelizationofasequentialprogrambyacompileristhe"holygrail"ofparallelcomputing,especiallywiththeaforementionedlimitofprocessorfrequency.Despitedecadesofworkbycompilerresearchers,automaticparallelizationhashadonlylimitedsuccess.[57]\nMainstreamparallelprogramminglanguagesremaineitherexplicitlyparallelor(atbest)partiallyimplicit,inwhichaprogrammergivesthecompilerdirectivesforparallelization.Afewfullyimplicitparallelprogramminglanguagesexist\xe2\x80\x94SISAL,ParallelHaskell,SequenceL,SystemC(forFPGAs),Mitrion-C,VHDL,andVerilog.\nAsacomputersystemgrowsincomplexity,themeantimebetweenfailuresusuallydecreases.Applicationcheckpointingisatechniquewherebythecomputersystemtakesa"snapshot"oftheapplication\xe2\x80\x94arecordofallcurrentresourceallocationsandvariablestates,akintoacoredump\xe2\x80\x94;thisinformationcanbeusedtorestoretheprogramifthecomputershouldfail.Applicationcheckpointingmeansthattheprogramhastorestartfromonlyitslastcheckpointratherthanthebeginning.Whilecheckpointingprovidesbenefitsinavarietyofsituations,itisespeciallyusefulinhighlyparallelsystemswithalargenumberofprocessorsusedinhighperformancecomputing.[58]\nAsparallelcomputersbecomelargerandfaster,wearenowabletosolveproblemsthathadpreviouslytakentoolongtorun.Fieldsasvariedasbioinformatics(forproteinfoldingandsequenceanalysis)andeconomics(formathematicalfinance)havetakenadvantageofparallelcomputing.Commontypesofproblemsinparallelcomputingapplicationsinclude:[59]\nParallelcomputingcanalsobeappliedtothedesignoffault-tolerantcomputersystems,particularlyvialockstepsystemsperformingthesameoperationinparallel.Thisprovidesredundancyincaseonecomponentfails,andalsoallowsautomaticerrordetectionanderrorcorrectioniftheresultsdiffer.Thesemethodscanbeusedtohelppreventsingle-eventupsetscausedbytransienterrors.[60]Althoughadditionalmeasuresmayberequiredinembeddedorspecializedsystems,thismethodcanprovideacosteffectiveapproachtoachieven-modularredundancyincommercialoff-the-shelfsystems.\nTheoriginsoftrue(MIMD)parallelismgobacktoLuigiFedericoMenabreaandhisSketchoftheAnalyticEngineInventedbyCharlesBabbage.[62][63][64]\nInApril1958,S.Gill(Ferranti)discussedparallelprogrammingandtheneedforbranchingandwaiting.[65]Alsoin1958,IBMresearchersJohnCockeandDanielSlotnickdiscussedtheuseofparallelisminnumericalcalculationsforthefirsttime.[66]BurroughsCorporationintroducedtheD825in1962,afour-processorcomputerthataccessedupto16memorymodulesthroughacrossbarswitch.[67]In1967,AmdahlandSlotnickpublishedadebateaboutthefeasibilityofparallelprocessingatAmericanFederationofInformationProcessingSocietiesConference.[66]ItwasduringthisdebatethatAmdahl\'slawwascoinedtodefinethelimitofspeed-upduetoparallelism.\nIn1969,HoneywellintroduceditsfirstMulticssystem,asymmetricmultiprocessorsystemcapableofrunninguptoeightprocessorsinparallel.[66]C.mmp,amulti-processorprojectatCarnegieMellonUniversityinthe1970s,wasamongthefirstmultiprocessorswithmorethanafewprocessors.Thefirstbus-connectedmultiprocessorwithsnoopingcacheswastheSynapseN+1in1984.[63]\nSIMDparallelcomputerscanbetracedbacktothe1970s.ThemotivationbehindearlySIMDcomputerswastoamortizethegatedelayoftheprocessor\'scontrolunitovermultipleinstructions.[68]In1964,SlotnickhadproposedbuildingamassivelyparallelcomputerfortheLawrenceLivermoreNationalLaboratory.[66]HisdesignwasfundedbytheUSAirForce,whichwastheearliestSIMDparallel-computingeffort,ILLIACIV.[66]Thekeytoitsdesignwasafairlyhighparallelism,withupto256 processors,whichallowedthemachinetoworkonlargedatasetsinwhatwouldlaterbeknownasvectorprocessing.However,ILLIACIVwascalled"themostinfamousofsupercomputers",becausetheprojectwasonlyone-fourthcompleted,buttook11 yearsandcostalmostfourtimestheoriginalestimate.[61]Whenitwasfinallyreadytorunitsfirstrealapplicationin1976,itwasoutperformedbyexistingcommercialsupercomputerssuchastheCray-1.\nIntheearly1970s,attheMITComputerScienceandArtificialIntelligenceLaboratory,MarvinMinskyandSeymourPapertstarteddevelopingtheSocietyofMindtheory,whichviewsthebiologicalbrainasmassivelyparallelcomputer.In1986,MinskypublishedTheSocietyofMind,whichclaimsthat\xe2\x80\x9cmindisformedfrommanylittleagents,eachmindlessbyitself\xe2\x80\x9d.[69]Thetheoryattemptstoexplainhowwhatwecallintelligencecouldbeaproductoftheinteractionofnon-intelligentparts.Minskysaysthatthebiggestsourceofideasaboutthetheorycamefromhisworkintryingtocreateamachinethatusesaroboticarm,avideocamera,andacomputertobuildwithchildren\'sblocks.[70]\nSimilarmodels(whichalsoviewbiologicalbrainasmassivelyparallelcomputer,i.e.,thebrainismadeupofaconstellationofindependentorsemi-independentagents)werealsodescribedby:\n\n\n