Supervisedlearningisthemachinelearningtaskoflearningafunctionthatmapsaninputtoanoutputbasedonexampleinput-outputpairs.[1]Itinfersafunctionfromlabeledtrainingdataconsistingofasetoftrainingexamples.[2]Insupervisedlearning,eachexampleisapairconsistingofaninputobject(typicallyavector)andadesiredoutputvalue(alsocalledthesupervisorysignal).Asupervisedlearningalgorithmanalyzesthetrainingdataandproducesaninferredfunction,whichcanbeusedformappingnewexamples.Anoptimalscenariowillallowforthealgorithmtocorrectlydeterminetheclasslabelsforunseeninstances.Thisrequiresthelearningalgorithmtogeneralizefromthetrainingdatatounseensituationsina"reasonable"way(seeinductivebias).\nTheparalleltaskinhumanandanimalpsychologyisoftenreferredtoasconceptlearning.\nInordertosolveagivenproblemofsupervisedlearning,onehastoperformthefollowingsteps:\nAwiderangeofsupervisedlearningalgorithmsareavailable,eachwithitsstrengthsandweaknesses.Thereisnosinglelearningalgorithmthatworksbestonallsupervisedlearningproblems(seetheNofreelunchtheorem).\nTherearefourmajorissuestoconsiderinsupervisedlearning:\nAfirstissueisthetradeoffbetweenbiasandvariance.[3]Imaginethatwehaveavailableseveraldifferent,butequallygood,trainingdatasets.Alearningalgorithmisbiasedforaparticularinput\n\n\n\nx\n\n\n{\\displaystylex}\n\nif,whentrainedoneachofthesedatasets,itissystematicallyincorrectwhenpredictingthecorrectoutputfor\n\n\n\nx\n\n\n{\\displaystylex}\n\n.Alearningalgorithmhashighvarianceforaparticularinput\n\n\n\nx\n\n\n{\\displaystylex}\n\nifitpredictsdifferentoutputvalueswhentrainedondifferenttrainingsets.Thepredictionerrorofalearnedclassifierisrelatedtothesumofthebiasandthevarianceofthelearningalgorithm.[4]Generally,thereisatradeoffbetweenbiasandvariance.Alearningalgorithmwithlowbiasmustbe"flexible"sothatitcanfitthedatawell.Butifthelearningalgorithmistooflexible,itwillfiteachtrainingdatasetdifferently,andhencehavehighvariance.Akeyaspectofmanysupervisedlearningmethodsisthattheyareabletoadjustthistradeoffbetweenbiasandvariance(eitherautomaticallyorbyprovidingabias/varianceparameterthattheusercanadjust).\nThesecondissueistheamountoftrainingdataavailablerelativetothecomplexityofthe"true"function(classifierorregressionfunction).Ifthetruefunctionissimple,thenan"inflexible"learningalgorithmwithhighbiasandlowvariancewillbeabletolearnitfromasmallamountofdata.Butifthetruefunctionishighlycomplex(e.g.,becauseitinvolvescomplexinteractionsamongmanydifferentinputfeaturesandbehavesdifferentlyindifferentpartsoftheinputspace),thenthefunctionwillonlybelearnablefromaverylargeamountoftrainingdataandusinga"flexible"learningalgorithmwithlowbiasandhighvariance.\nAthirdissueisthedimensionalityoftheinputspace.Iftheinputfeaturevectorshaveveryhighdimension,thelearningproblemcanbedifficultevenifthetruefunctiononlydependsonasmallnumberofthosefeatures.Thisisbecausethemany"extra"dimensionscanconfusethelearningalgorithmandcauseittohavehighvariance.Hence,highinputdimensionalitytypicallyrequirestuningtheclassifiertohavelowvarianceandhighbias.Inpractice,iftheengineercanmanuallyremoveirrelevantfeaturesfromtheinputdata,thisislikelytoimprovetheaccuracyofthelearnedfunction.Inaddition,therearemanyalgorithmsforfeatureselectionthatseektoidentifytherelevantfeaturesanddiscardtheirrelevantones.Thisisaninstanceofthemoregeneralstrategyofdimensionalityreduction,whichseekstomaptheinputdataintoalower-dimensionalspacepriortorunningthesupervisedlearningalgorithm.\nAfourthissueisthedegreeofnoiseinthedesiredoutputvalues(thesupervisorytargetvariables).Ifthedesiredoutputvaluesareoftenincorrect(becauseofhumanerrororsensorerrors),thenthelearningalgorithmshouldnotattempttofindafunctionthatexactlymatchesthetrainingexamples.Attemptingtofitthedatatoocarefullyleadstooverfitting.Youcanoverfitevenwhentherearenomeasurementerrors(stochasticnoise)ifthefunctionyouaretryingtolearnistoocomplexforyourlearningmodel.Insuchasituation,thepartofthetargetfunctionthatcannotbemodeled"corrupts"yourtrainingdata-thisphenomenonhasbeencalleddeterministicnoise.Wheneithertypeofnoiseispresent,itisbettertogowithahigherbias,lowervarianceestimator.\nInpractice,thereareseveralapproachestoalleviatenoiseintheoutputvaluessuchasearlystoppingtopreventoverfittingaswellasdetectingandremovingthenoisytrainingexamplespriortotrainingthesupervisedlearningalgorithm.Thereareseveralalgorithmsthatidentifynoisytrainingexamplesandremovingthesuspectednoisytrainingexamplespriortotraininghasdecreasedgeneralizationerrorwithstatisticalsignificance.[5][6]\nOtherfactorstoconsiderwhenchoosingandapplyingalearningalgorithmincludethefollowing:\nWhenconsideringanewapplication,theengineercancomparemultiplelearningalgorithmsandexperimentallydeterminewhichoneworksbestontheproblemathand(seecrossvalidation).Tuningtheperformanceofalearningalgorithmcanbeverytime-consuming.Givenfixedresources,itisoftenbettertospendmoretimecollectingadditionaltrainingdataandmoreinformativefeaturesthanitistospendextratimetuningthelearningalgorithms.\nThemostwidelyusedlearningalgorithmsare:\nGivenasetof\n\n\n\nN\n\n\n{\\displaystyleN}\n\ntrainingexamplesoftheform\n\n\n\n{\n(\n\nx\n\n1\n\n\n,\n\ny\n\n1\n\n\n)\n,\n.\n.\n.\n,\n(\n\nx\n\nN\n\n\n,\n\n\ny\n\nN\n\n\n)\n}\n\n\n{\\displaystyle\\{(x_{1},y_{1}),...,(x_{N},\\;y_{N})\\}}\n\nsuchthat\n\n\n\n\nx\n\ni\n\n\n\n\n{\\displaystylex_{i}}\n\nisthefeaturevectorofthei-thexampleand\n\n\n\n\ny\n\ni\n\n\n\n\n{\\displaystyley_{i}}\n\nisitslabel(i.e.,class),alearningalgorithmseeksafunction\n\n\n\ng\n:\nX\n→\nY\n\n\n{\\displaystyleg:X\\toY}\n\n,where\n\n\n\nX\n\n\n{\\displaystyleX}\n\nistheinputspaceand\n\n\n\n\nY\n\n\n{\\displaystyleY}\n\nistheoutputspace.Thefunction\n\n\n\ng\n\n\n{\\displaystyleg}\n\nisanelementofsomespaceofpossiblefunctions\n\n\n\nG\n\n\n{\\displaystyleG}\n\n,usuallycalledthehypothesisspace.Itissometimesconvenientto\nrepresent\n\n\n\ng\n\n\n{\\displaystyleg}\n\nusingascoringfunction\n\n\n\nf\n:\nX\n×\nY\n→\n\n\nR\n\n\n\n\n{\\displaystylef:X\\timesY\\to{\\mathbb{R}}}\n\nsuchthat\n\n\n\ng\n\n\n{\\displaystyleg}\n\nisdefinedasreturningthe\n\n\n\ny\n\n\n{\\displaystyley}\n\nvaluethatgivesthehighestscore:\n\n\n\ng\n(\nx\n)\n=\n\n\n\narg\n⁡\nmax\n\ny\n\n\n\nf\n(\nx\n,\ny\n)\n\n\n{\\displaystyleg(x)={\\underset{y}{\\arg\\max}}\\;f(x,y)}\n\n.Let\n\n\n\nF\n\n\n{\\displaystyleF}\n\ndenotethespaceofscoringfunctions.\nAlthough\n\n\n\nG\n\n\n{\\displaystyleG}\n\nand\n\n\n\nF\n\n\n{\\displaystyleF}\n\ncanbeanyspaceoffunctions,manylearningalgorithmsareprobabilisticmodelswhere\n\n\n\ng\n\n\n{\\displaystyleg}\n\ntakestheformofaconditionalprobabilitymodel\n\n\n\ng\n(\nx\n)\n=\nP\n(\ny\n\n|\n\nx\n)\n\n\n{\\displaystyleg(x)=P(y|x)}\n\n,or\n\n\n\nf\n\n\n{\\displaystylef}\n\ntakestheformofajointprobabilitymodel\n\n\n\nf\n(\nx\n,\ny\n)\n=\nP\n(\nx\n,\ny\n)\n\n\n{\\displaystylef(x,y)=P(x,y)}\n\n.Forexample,naiveBayesandlineardiscriminantanalysisarejointprobabilitymodels,whereaslogisticregressionisaconditionalprobabilitymodel.\nTherearetwobasicapproachestochoosing\n\n\n\nf\n\n\n{\\displaystylef}\n\nor\n\n\n\ng\n\n\n{\\displaystyleg}\n\n:empiricalriskminimizationandstructuralriskminimization.[7]Empiricalriskminimizationseeksthefunctionthatbestfitsthetrainingdata.Structuralriskminimizationincludesapenaltyfunctionthatcontrolsthebias/variancetradeoff.\nInbothcases,itisassumedthatthetrainingsetconsistsofasampleofindependentandidenticallydistributedpairs,\n\n\n\n(\n\nx\n\ni\n\n\n,\n\n\ny\n\ni\n\n\n)\n\n\n{\\displaystyle(x_{i},\\;y_{i})}\n\n.Inordertomeasurehowwellafunctionfitsthetrainingdata,alossfunction\n\n\n\nL\n:\nY\n×\nY\n→\n\n\n\nR\n\n\n\n≥\n0\n\n\n\n\n{\\displaystyleL:Y\\timesY\\to{\\mathbb{R}}^{\\geq0}}\n\nisdefined.Fortrainingexample\n\n\n\n(\n\nx\n\ni\n\n\n,\n\n\ny\n\ni\n\n\n)\n\n\n{\\displaystyle(x_{i},\\;y_{i})}\n\n,thelossofpredictingthevalue\n\n\n\n\n\n\ny\n^\n\n\n\n\n\n{\\displaystyle{\\hat{y}}}\n\nis\n\n\n\nL\n(\n\ny\n\ni\n\n\n,\n\n\n\ny\n^\n\n\n\n)\n\n\n{\\displaystyleL(y_{i},{\\hat{y}})}\n\n.\nTherisk\n\n\n\nR\n(\ng\n)\n\n\n{\\displaystyleR(g)}\n\noffunction\n\n\n\ng\n\n\n{\\displaystyleg}\n\nisdefinedastheexpectedlossof\n\n\n\ng\n\n\n{\\displaystyleg}\n\n.Thiscanbeestimatedfromthetrainingdataas\nInempiricalriskminimization,thesupervisedlearningalgorithmseeksthefunction\n\n\n\ng\n\n\n{\\displaystyleg}\n\nthatminimizes\n\n\n\nR\n(\ng\n)\n\n\n{\\displaystyleR(g)}\n\n.Hence,asupervisedlearningalgorithmcanbeconstructedbyapplyinganoptimizationalgorithmtofind\n\n\n\ng\n\n\n{\\displaystyleg}\n\n.\nWhen\n\n\n\ng\n\n\n{\\displaystyleg}\n\nisaconditionalprobabilitydistribution\n\n\n\nP\n(\ny\n\n|\n\nx\n)\n\n\n{\\displaystyleP(y|x)}\n\nandthelossfunctionisthenegativeloglikelihood:\n\n\n\nL\n(\ny\n,\n\n\n\ny\n^\n\n\n\n)\n=\n−\nlog\n⁡\nP\n(\ny\n\n|\n\nx\n)\n\n\n{\\displaystyleL(y,{\\hat{y}})=-\\logP(y|x)}\n\n,thenempiricalriskminimizationisequivalenttomaximumlikelihoodestimation.\nWhen\n\n\n\nG\n\n\n{\\displaystyleG}\n\ncontainsmanycandidatefunctionsorthetrainingsetisnotsufficientlylarge,empiricalriskminimizationleadstohighvarianceandpoorgeneralization.Thelearningalgorithmisable\ntomemorizethetrainingexampleswithoutgeneralizingwell.Thisiscalledoverfitting.\nStructuralriskminimizationseekstopreventoverfittingbyincorporatingaregularizationpenaltyintotheoptimization.TheregularizationpenaltycanbeviewedasimplementingaformofOccam\'srazorthatpreferssimplerfunctionsovermorecomplexones.\nAwidevarietyofpenaltieshavebeenemployedthatcorrespondtodifferentdefinitionsofcomplexity.Forexample,considerthecasewherethefunction\n\n\n\ng\n\n\n{\\displaystyleg}\n\nisalinearfunctionoftheform\nApopularregularizationpenaltyis\n\n\n\n\n∑\n\nj\n\n\n\nβ\n\nj\n\n\n2\n\n\n\n\n{\\displaystyle\\sum_{j}\\beta_{j}^{2}}\n\n,whichisthesquaredEuclideannormoftheweights,alsoknownasthe\n\n\n\n\nL\n\n2\n\n\n\n\n{\\displaystyleL_{2}}\n\nnorm.Othernormsincludethe\n\n\n\n\nL\n\n1\n\n\n\n\n{\\displaystyleL_{1}}\n\nnorm,\n\n\n\n\n∑\n\nj\n\n\n\n|\n\n\nβ\n\nj\n\n\n\n|\n\n\n\n{\\displaystyle\\sum_{j}|\\beta_{j}|}\n\n,andthe\n\n\n\n\nL\n\n0\n\n\n\n\n{\\displaystyleL_{0}}\n\nnorm,whichisthenumberofnon-zero\n\n\n\n\nβ\n\nj\n\n\n\n\n{\\displaystyle\\beta_{j}}\n\ns.Thepenaltywillbedenotedby\n\n\n\nC\n(\ng\n)\n\n\n{\\displaystyleC(g)}\n\n.\nThesupervisedlearningoptimizationproblemistofindthefunction\n\n\n\ng\n\n\n{\\displaystyleg}\n\nthatminimizes\nTheparameter\n\n\n\nλ\n\n\n{\\displaystyle\\lambda}\n\ncontrolsthebias-variancetradeoff.When\n\n\n\nλ\n=\n0\n\n\n{\\displaystyle\\lambda=0}\n\n,thisgivesempiricalriskminimizationwithlowbiasandhighvariance.When\n\n\n\nλ\n\n\n{\\displaystyle\\lambda}\n\nislarge,thelearningalgorithmwillhavehighbiasandlowvariance.Thevalueof\n\n\n\nλ\n\n\n{\\displaystyle\\lambda}\n\ncanbechosenempiricallyviacrossvalidation.\nThecomplexitypenaltyhasaBayesianinterpretationasthenegativelogpriorprobabilityof\n\n\n\ng\n\n\n{\\displaystyleg}\n\n,\n\n\n\n−\nlog\n⁡\nP\n(\ng\n)\n\n\n{\\displaystyle-\\logP(g)}\n\n,inwhichcase\n\n\n\nJ\n(\ng\n)\n\n\n{\\displaystyleJ(g)}\n\nistheposteriorprobababilityof\n\n\n\ng\n\n\n{\\displaystyleg}\n\n.\nThetrainingmethodsdescribedabovearediscriminativetrainingmethods,becausetheyseektofindafunction\n\n\n\ng\n\n\n{\\displaystyleg}\n\nthatdiscriminateswellbetweenthedifferentoutputvalues(seediscriminativemodel).Forthespecialcasewhere\n\n\n\nf\n(\nx\n,\ny\n)\n=\nP\n(\nx\n,\ny\n)\n\n\n{\\displaystylef(x,y)=P(x,y)}\n\nisajointprobabilitydistributionandthelossfunctionisthenegativeloglikelihood\n\n\n\n−\n\n∑\n\ni\n\n\nlog\n⁡\nP\n(\n\nx\n\ni\n\n\n,\n\ny\n\ni\n\n\n)\n,\n\n\n{\\displaystyle-\\sum_{i}\\logP(x_{i},y_{i}),}\n\nariskminimizationalgorithmissaidtoperformgenerativetraining,because\n\n\n\nf\n\n\n{\\displaystylef}\n\ncanberegardedasagenerativemodelthatexplainshowthedataweregenerated.Generativetrainingalgorithmsareoftensimplerandmorecomputationallyefficientthandiscriminativetrainingalgorithms.Insomecases,thesolutioncanbecomputedinclosedformasinnaiveBayesandlineardiscriminantanalysis.\nThereareseveralwaysinwhichthestandardsupervisedlearningproblemcanbegeneralized:\n