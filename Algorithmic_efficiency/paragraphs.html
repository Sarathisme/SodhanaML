[<p class="mw-empty-elt">\n</p>, <p>In <a href="/wiki/Computer_science" title="Computer science">computer science</a>, <b>algorithmic efficiency</b> is a property of an <a href="/wiki/Algorithm" title="Algorithm">algorithm</a> which relates to the number of <a href="/wiki/Computational_resource" title="Computational resource">computational resources</a> used by the algorithm. An algorithm must be <a href="/wiki/Analysis_of_algorithms" title="Analysis of algorithms">analyzed</a> to determine its resource usage, and the efficiency of an algorithm can be measured based on usage of different resources. Algorithmic efficiency can be thought of as analogous to engineering <a href="/wiki/Productivity" title="Productivity">productivity</a> for a repeating or continuous process.\n</p>, <p>For maximum efficiency we wish to minimize resource usage. However, different resources such as <a href="/wiki/Time_complexity" title="Time complexity">time</a> and <a class="mw-redirect" href="/wiki/Space_complexity" title="Space complexity">space</a> complexity cannot be compared directly, so which of two algorithms is considered to be more efficient often depends on which measure of efficiency is considered most important. \n</p>, <p>For example, <a href="/wiki/Bubble_sort" title="Bubble sort">bubble sort</a> and <a href="/wiki/Timsort" title="Timsort">timsort</a> are both <a href="/wiki/Sorting_algorithm" title="Sorting algorithm">algorithms to sort a list</a> of items from smallest to largest. Bubble sort sorts the list in time proportional to the number of elements squared (<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\scriptstyle {{\\mathcal {O}}\\left(n^{2}\\right)}}" xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mstyle displaystyle="false" scriptlevel="1">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mrow class="MJX-TeXAtom-ORD">\n              <mrow class="MJX-TeXAtom-ORD">\n                <mi class="MJX-tex-caligraphic" mathvariant="script">O</mi>\n              </mrow>\n            </mrow>\n            <mrow>\n              <mo>(</mo>\n              <msup>\n                <mi>n</mi>\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mn>2</mn>\n                </mrow>\n              </msup>\n              <mo>)</mo>\n            </mrow>\n          </mrow>\n        </mstyle>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\scriptstyle {{\\mathcal {O}}\\left(n^{2}\\right)}}</annotation>\n  </semantics>\n</math></span><img alt="{\\displaystyle \\scriptstyle {{\\mathcal {O}}\\left(n^{2}\\right)}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/26009da5f498623abf8228105aa559c7dbc2d8a3" style="vertical-align: -0.838ex; width:4.632ex; height:2.509ex;"/></span>, see <a href="/wiki/Big_O_notation" title="Big O notation">Big O notation</a>), but only requires a small amount of extra <a href="/wiki/Computer_memory" title="Computer memory">memory</a> which is constant with respect to the length of the list (<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\textstyle \\scriptstyle {{\\mathcal {O}}\\left(1\\right)}}" xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="false" scriptlevel="0">\n        <mstyle displaystyle="false" scriptlevel="1">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mrow class="MJX-TeXAtom-ORD">\n              <mrow class="MJX-TeXAtom-ORD">\n                <mi class="MJX-tex-caligraphic" mathvariant="script">O</mi>\n              </mrow>\n            </mrow>\n            <mrow>\n              <mo>(</mo>\n              <mn>1</mn>\n              <mo>)</mo>\n            </mrow>\n          </mrow>\n        </mstyle>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\textstyle \\scriptstyle {{\\mathcal {O}}\\left(1\\right)}}</annotation>\n  </semantics>\n</math></span><img alt="{\\textstyle \\scriptstyle {{\\mathcal {O}}\\left(1\\right)}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a181db9b409d9502947b86cb681fb2552980afab" style="vertical-align: -0.671ex; width:3.409ex; height:2.176ex;"/></span>). Timsort sorts the list in time <a class="mw-redirect" href="/wiki/Linearithmic" title="Linearithmic">linearithmic</a> (proportional to a quantity times its logarithm) in the list\'s length (<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\textstyle \\scriptstyle {\\mathcal {O\\left(n\\log n\\right)}}}" xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="false" scriptlevel="0">\n        <mstyle displaystyle="false" scriptlevel="1">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mrow class="MJX-TeXAtom-ORD">\n              <mi class="MJX-tex-caligraphic" mathvariant="script">O</mi>\n              <mrow>\n                <mo>(</mo>\n                <mrow>\n                  <mi class="MJX-tex-caligraphic" mathvariant="script">n</mi>\n                  <mi>log</mi>\n                  <mo>‚Å°<!-- \xe2\x81\xa1 --></mo>\n                  <mi class="MJX-tex-caligraphic" mathvariant="script">n</mi>\n                </mrow>\n                <mo>)</mo>\n              </mrow>\n            </mrow>\n          </mrow>\n        </mstyle>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\textstyle \\scriptstyle {\\mathcal {O\\left(n\\log n\\right)}}}</annotation>\n  </semantics>\n</math></span><img alt="{\\textstyle \\scriptstyle {\\mathcal {O\\left(n\\log n\\right)}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cfaa9b9f8166c22db8225b2f4df36a4a7323b6ca" style="vertical-align: -0.671ex; width:7.435ex; height:2.176ex;"/></span>), but has a space requirement <a href="/wiki/Proportionality_(mathematics)" title="Proportionality (mathematics)">linear</a> in the length of the list (<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\textstyle \\scriptstyle {\\mathcal {O\\left(n\\right)}}}" xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="false" scriptlevel="0">\n        <mstyle displaystyle="false" scriptlevel="1">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mrow class="MJX-TeXAtom-ORD">\n              <mi class="MJX-tex-caligraphic" mathvariant="script">O</mi>\n              <mrow>\n                <mo>(</mo>\n                <mi class="MJX-tex-caligraphic" mathvariant="script">n</mi>\n                <mo>)</mo>\n              </mrow>\n            </mrow>\n          </mrow>\n        </mstyle>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\textstyle \\scriptstyle {\\mathcal {O\\left(n\\right)}}}</annotation>\n  </semantics>\n</math></span><img alt="{\\textstyle \\scriptstyle {\\mathcal {O\\left(n\\right)}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4fb377d3898d582a8931bc26663d7908db627948" style="vertical-align: -0.671ex; width:3.574ex; height:2.176ex;"/></span>). If large lists must be sorted at high speed for a given application, timsort is a better choice; however, if minimizing the memory footprint of the sorting is more important, bubble sort is a better choice.\n</p>, <p>The importance of efficiency with respect to time was emphasised by <a href="/wiki/Ada_Lovelace" title="Ada Lovelace">Ada Lovelace</a> in 1843 as applying to <a href="/wiki/Charles_Babbage" title="Charles Babbage">Charles Babbage</a>\'s mechanical analytical engine:\n</p>, <p>"In almost every computation a great variety of arrangements for the succession of the processes is possible, and various considerations must influence the selections amongst them for the purposes of a calculating engine. One essential object is to choose that arrangement which shall tend to reduce to a minimum the time necessary for completing the calculation"<sup class="reference" id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup></p>, <p>Early <a class="mw-redirect" href="/wiki/Electronic_computer" title="Electronic computer">electronic computers</a> were severely limited both by the speed of operations and the amount of memory available. In some cases it was realized that there was a <a class="mw-redirect" href="/wiki/Space%E2%80%93time_trade-off" title="Space\xe2\x80\x93time trade-off">space\xe2\x80\x93time trade-off</a>, whereby a <a href="/wiki/Task_(computing)" title="Task (computing)">task</a> could be handled either by using a fast algorithm which used quite a lot of working memory, or by using a slower algorithm which used very little working memory. The engineering trade-off was then to use the fastest algorithm which would fit in the available memory.\n</p>, <p>Modern computers are significantly faster than the early computers, and have a much larger amount of memory available (<a class="mw-redirect" href="/wiki/Orders_of_magnitude_(computing)" title="Orders of magnitude (computing)">Gigabytes instead of Kilobytes</a>). Nevertheless, <a href="/wiki/Donald_Knuth" title="Donald Knuth">Donald Knuth</a> emphasised that efficiency is still an important consideration:\n</p>, <p> "In established engineering disciplines a 12% improvement, easily obtained, is never considered marginal and I believe the same viewpoint should prevail in software engineering"<sup class="reference" id="cite_ref-Knuth1974_2-0"><a href="#cite_note-Knuth1974-2">[2]</a></sup></p>, <p>An algorithm is considered efficient if its resource consumption, also known as computational cost, is at or below some acceptable level. Roughly speaking, \'acceptable\' means:  it will run in a reasonable amount of time or space on an available computer, typically as a <a href="/wiki/Function_(mathematics)" title="Function (mathematics)">function</a> of the size of the input. Since the 1950s computers have seen dramatic increases in both the available computational power and in the available amount of memory, so current acceptable levels would have been unacceptable even 10 years ago. In fact, thanks to the <a href="/wiki/Moore%27s_law" title="Moore's law">approximate doubling of computer power ever 2 years</a>, tasks that are acceptably efficient on modern <a href="/wiki/Smartphone" title="Smartphone">smartphones</a> and <a href="/wiki/Embedded_system" title="Embedded system">embedded systems</a> may have been unacceptably inefficient for industrial <a href="/wiki/Server_(computing)" title="Server (computing)">servers</a> 10 years ago.\n</p>, <p>Computer manufacturers frequently bring out new models, often with higher <a href="/wiki/Computer_performance" title="Computer performance">performance</a>. Software costs can be quite high, so in some cases the simplest and cheapest way of getting higher performance might be to just buy a faster computer, provided it is <a href="/wiki/Backward_compatibility" title="Backward compatibility">compatible</a> with an existing computer.\n</p>, <p>There are many ways in which the resources used by an algorithm can be measured: the two most common measures are speed and memory usage; other measures could include transmission speed, temporary disk usage, long-term disk usage, power consumption, <a href="/wiki/Total_cost_of_ownership" title="Total cost of ownership">total cost of ownership</a>, <a href="/wiki/Response_time_(technology)" title="Response time (technology)">response time</a> to external stimuli, etc. Many of these measures depend on the size of the input to the algorithm, i.e. the amount of data to be processed. They might also depend on the way in which the data is arranged; for example, some <a href="/wiki/Sorting_algorithm" title="Sorting algorithm">sorting algorithms</a> perform poorly on data which is already sorted, or which is sorted in reverse order.\n</p>, <p>In practice, there are other factors which can affect the efficiency of an algorithm, such as requirements for accuracy and/or reliability. As detailed below, the way in which an algorithm is implemented can also have a significant effect on actual efficiency, though many aspects of this relate to <a class="mw-redirect" href="/wiki/Optimization_(computer_science)" title="Optimization (computer science)">optimization</a> issues.\n</p>, <p>In the theoretical <a href="/wiki/Analysis_of_algorithms" title="Analysis of algorithms">analysis of algorithms</a>, the normal practice is to estimate their complexity in the asymptotic sense. The most commonly used notation to describe resource consumption or "complexity" is <a href="/wiki/Donald_Knuth" title="Donald Knuth">Donald Knuth</a>\'s <a href="/wiki/Big_O_notation" title="Big O notation">Big O notation</a>, representing the complexity of an algorithm as a function of the size of the input <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\textstyle \\scriptstyle n}" xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="false" scriptlevel="0">\n        <mstyle displaystyle="false" scriptlevel="1">\n          <mi>n</mi>\n        </mstyle>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\textstyle \\scriptstyle n}</annotation>\n  </semantics>\n</math></span><img alt="{\\textstyle \\scriptstyle n}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ea3a4fd21e1c8ebe98295b82e618f6b08ba0aa27" style="vertical-align: -0.338ex; width:0.986ex; height:1.343ex;"/></span>. Big O notation is an <a href="/wiki/Asymptote" title="Asymptote">asymptotic</a> measure of function complexity, where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\textstyle \\scriptstyle {f\\left(n\\right)\\,=\\,{\\mathcal {O}}\\left(g\\left(n\\right)\\right)}}" xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="false" scriptlevel="0">\n        <mstyle displaystyle="false" scriptlevel="1">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>f</mi>\n            <mrow>\n              <mo>(</mo>\n              <mi>n</mi>\n              <mo>)</mo>\n            </mrow>\n            <mspace width="thinmathspace"></mspace>\n            <mo>=</mo>\n            <mspace width="thinmathspace"></mspace>\n            <mrow class="MJX-TeXAtom-ORD">\n              <mrow class="MJX-TeXAtom-ORD">\n                <mi class="MJX-tex-caligraphic" mathvariant="script">O</mi>\n              </mrow>\n            </mrow>\n            <mrow>\n              <mo>(</mo>\n              <mrow>\n                <mi>g</mi>\n                <mrow>\n                  <mo>(</mo>\n                  <mi>n</mi>\n                  <mo>)</mo>\n                </mrow>\n              </mrow>\n              <mo>)</mo>\n            </mrow>\n          </mrow>\n        </mstyle>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\textstyle \\scriptstyle {f\\left(n\\right)\\,=\\,{\\mathcal {O}}\\left(g\\left(n\\right)\\right)}}</annotation>\n  </semantics>\n</math></span><img alt="{\\textstyle \\scriptstyle {f\\left(n\\right)\\,=\\,{\\mathcal {O}}\\left(g\\left(n\\right)\\right)}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/dbd779222176a2723e19d2eaf77fd62708ecca42" style="vertical-align: -0.671ex; width:10.865ex; height:2.176ex;"/></span>roughly means the time requirement for an algorithm is proportional to <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\scriptstyle {g\\left(n\\right)}}" xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mstyle displaystyle="false" scriptlevel="1">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>g</mi>\n            <mrow>\n              <mo>(</mo>\n              <mi>n</mi>\n              <mo>)</mo>\n            </mrow>\n          </mrow>\n        </mstyle>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\scriptstyle {g\\left(n\\right)}}</annotation>\n  </semantics>\n</math></span><img alt="{\\displaystyle \\scriptstyle {g\\left(n\\right)}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e8739942a0f094b6700b09d89179b51560dd877a" style="vertical-align: -0.671ex; width:3.055ex; height:2.176ex;"/></span>, omitting <a class="mw-redirect" href="/wiki/Lower-order_terms" title="Lower-order terms">lower-order terms</a> that contribute less than <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\scriptstyle {g\\left(n\\right)}}" xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mstyle displaystyle="false" scriptlevel="1">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>g</mi>\n            <mrow>\n              <mo>(</mo>\n              <mi>n</mi>\n              <mo>)</mo>\n            </mrow>\n          </mrow>\n        </mstyle>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\scriptstyle {g\\left(n\\right)}}</annotation>\n  </semantics>\n</math></span><img alt="{\\displaystyle \\scriptstyle {g\\left(n\\right)}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e8739942a0f094b6700b09d89179b51560dd877a" style="vertical-align: -0.671ex; width:3.055ex; height:2.176ex;"/></span>to the growth of the function <a href="/wiki/Limit_(mathematics)" title="Limit (mathematics)">as <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\scriptstyle n}" xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mstyle displaystyle="false" scriptlevel="1">\n          <mi>n</mi>\n        </mstyle>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\scriptstyle n}</annotation>\n  </semantics>\n</math></span><img alt="\\scriptstyle n" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/76760f2e577f85ef1b9818b3a1f7676f3378c0e7" style="vertical-align: -0.338ex; width:0.986ex; height:1.343ex;"/></span>grows arbitrarily large</a>. This estimate may be misleading when <i><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\textstyle \\scriptstyle n}" xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="false" scriptlevel="0">\n        <mstyle displaystyle="false" scriptlevel="1">\n          <mi>n</mi>\n        </mstyle>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\textstyle \\scriptstyle n}</annotation>\n  </semantics>\n</math></span><img alt="{\\textstyle \\scriptstyle n}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ea3a4fd21e1c8ebe98295b82e618f6b08ba0aa27" style="vertical-align: -0.338ex; width:0.986ex; height:1.343ex;"/></span></i>is small, but is generally sufficiently accurate when <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\textstyle \\scriptstyle n}" xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="false" scriptlevel="0">\n        <mstyle displaystyle="false" scriptlevel="1">\n          <mi>n</mi>\n        </mstyle>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\textstyle \\scriptstyle n}</annotation>\n  </semantics>\n</math></span><img alt="{\\textstyle \\scriptstyle n}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ea3a4fd21e1c8ebe98295b82e618f6b08ba0aa27" style="vertical-align: -0.338ex; width:0.986ex; height:1.343ex;"/></span>is large as the notation is asymptotic. For example, bubble sort may be faster than <a href="/wiki/Merge_sort" title="Merge sort">merge sort</a> when only a few items are to be sorted; however either implementation is likely to meet performance requirements for a small list. Typically, programmers are interested in algorithms that <a href="/wiki/Scalability" title="Scalability">scale</a> efficiently to large input sizes, and merge sort is preferred over bubble sort for lists of length encountered in most data-intensive programs.\n</p>, <p>Some examples of Big O notation applied to algorithms\' asymptotic time complexity include:\n</p>, <p>For new versions of software or to provide comparisons with competitive systems, <a href="/wiki/Benchmark_(computing)" title="Benchmark (computing)">benchmarks</a> are sometimes used, which assist with gauging an algorithms relative performance. If a new <a href="/wiki/Sorting_algorithm" title="Sorting algorithm">sort algorithm</a> is produced, for example, it can be compared with its predecessors to ensure that at least it is efficient as before with known data, taking into consideration any functional improvements. Benchmarks can be used by customers when comparing various products from alternative suppliers to estimate which product will best suit their specific requirements in terms of functionality and performance. For example, in the <a href="/wiki/Mainframe_computer" title="Mainframe computer">mainframe</a> world certain proprietary <a href="/wiki/Mainframe_sort_merge" title="Mainframe sort merge">sort</a> products from independent software companies such as <a href="/wiki/Syncsort" title="Syncsort">Syncsort</a> compete with products from the major suppliers such as <a href="/wiki/IBM" title="IBM">IBM</a> for speed.\n</p>, <p>Some benchmarks provide opportunities for producing an analysis comparing the relative speed of various compiled and interpreted languages for example<sup class="reference" id="cite_ref-fourmilab.ch_3-0"><a href="#cite_note-fourmilab.ch-3">[3]</a></sup><sup class="reference" id="cite_ref-4"><a href="#cite_note-4">[4]</a></sup>\nand <a href="/wiki/The_Computer_Language_Benchmarks_Game" title="The Computer Language Benchmarks Game">The Computer Language Benchmarks Game</a> compares the performance of implementations of typical programming problems in several programming languages.\n</p>, <p>Even creating "<a href="/wiki/Do_it_yourself" title="Do it yourself">do it yourself</a>" benchmarks can demonstrate the relative performance of different programming languages, using a variety of user specified criteria. This is quite simple, as a "Nine language performance roundup" by Christopher W. Cowell-Shah demonstrates by example.<sup class="reference" id="cite_ref-5"><a href="#cite_note-5">[5]</a></sup>\n</p>, <p>Implementation issues can also have an effect on efficiency, such as the choice of programming language, or the way in which the algorithm is actually coded,<sup class="reference" id="cite_ref-KriegelSchubert2016_6-0"><a href="#cite_note-KriegelSchubert2016-6">[6]</a></sup> or the choice of a <a href="/wiki/Compiler" title="Compiler">compiler</a> for a particular language, or the <a class="mw-redirect" href="/wiki/Compiler_optimization" title="Compiler optimization">compilation options</a> used, or even the <a href="/wiki/Operating_system" title="Operating system">operating system</a> being used. In many cases a language implemented by an <a href="/wiki/Interpreter_(computing)" title="Interpreter (computing)">interpreter</a> may be much slower than a language implemented by a compiler.<sup class="reference" id="cite_ref-fourmilab.ch_3-1"><a href="#cite_note-fourmilab.ch-3">[3]</a></sup> See the articles on <a href="/wiki/Just-in-time_compilation" title="Just-in-time compilation">just-in-time compilation</a> and <a href="/wiki/Interpreted_language" title="Interpreted language">interpreted languages</a>.\n</p>, <p>There are other factors which may affect time or space issues, but which may be outside of a programmer\'s control; these include <a class="mw-redirect" href="/wiki/Data_alignment" title="Data alignment">data alignment</a>, <a href="/wiki/Granularity#Data_granularity" title="Granularity">data granularity</a>, <a href="/wiki/Locality_of_reference" title="Locality of reference">cache locality</a>, <a href="/wiki/Cache_coherence" title="Cache coherence">cache coherency</a>, <a href="/wiki/Garbage_collection_(computer_science)" title="Garbage collection (computer science)">garbage collection</a>, <a href="/wiki/Instruction-level_parallelism" title="Instruction-level parallelism">instruction-level parallelism</a>, <a class="mw-redirect mw-disambig" href="/wiki/Multithreading_(disambiguation)" title="Multithreading (disambiguation)">multi-threading</a> (at either a hardware or software level), <a href="/wiki/Simultaneous_multithreading" title="Simultaneous multithreading">simultaneous multitasking</a>, and <a href="/wiki/Subroutine" title="Subroutine">subroutine</a> calls.<sup class="reference" id="cite_ref-steele1997_7-0"><a href="#cite_note-steele1997-7">[7]</a></sup>\n</p>, <p>Some processors have capabilities for <a href="/wiki/Vector_processor" title="Vector processor">vector processing</a>, which allow a <a href="/wiki/SIMD" title="SIMD">single instruction to operate on multiple operands</a>; it may or may not be easy for a programmer or compiler to use these capabilities. Algorithms designed for sequential processing may need to be completely redesigned to make use of <a href="/wiki/Parallel_computing" title="Parallel computing">parallel processing</a>, or they could be easily reconfigured. As <a href="/wiki/Parallel_computing" title="Parallel computing">parallel</a> and <a href="/wiki/Distributed_computing" title="Distributed computing">distributed computing</a> grow in importance in the late <a href="/wiki/2010s" title="2010s">2010\'s</a>, more investments are being made into efficient <a href="/wiki/High-level_programming_language" title="High-level programming language">high-level</a> <a href="/wiki/Application_programming_interface" title="Application programming interface">Application programming interfaces</a> for parallel and distributed computing systems such as <a href="/wiki/CUDA" title="CUDA">CUDA</a>, <a href="/wiki/TensorFlow" title="TensorFlow">TensorFlow</a>, <a href="/wiki/Apache_Hadoop" title="Apache Hadoop">Hadoop</a>, <a href="/wiki/OpenMP" title="OpenMP">OpenMP</a> and <a href="/wiki/Message_Passing_Interface" title="Message Passing Interface">MPI</a>.\n</p>, <p>Another problem which can arise in programming is that processors compatible with the same <a href="/wiki/Instruction_set_architecture" title="Instruction set architecture">instruction set</a> (such as <a href="/wiki/X86-64" title="X86-64">x86-64</a> or <a href="/wiki/ARM_architecture" title="ARM architecture">ARM</a>) may implement an instruction in different ways, so that instructions which are relatively fast on some models may be relatively slow on other models. This often presents challenges to <a href="/wiki/Optimizing_compiler" title="Optimizing compiler">optimizing compilers</a>, which must have a great amount of knowledge of the specific <a href="/wiki/Central_processing_unit" title="Central processing unit">CPU</a> and other hardware available on the compilation target to best optimize a program for performance. In the extreme case, a compiler may be forced to <a class="mw-redirect" href="/wiki/Software_emulation" title="Software emulation">emulate</a> instructions not supported on a compilation target platform, forcing it to <a href="/wiki/Code_generation_(compiler)" title="Code generation (compiler)">generate code</a> or <a class="mw-redirect" href="/wiki/Linking_(computing)" title="Linking (computing)">link</a> an external <a href="/wiki/Library_(computing)" title="Library (computing)">library call</a> to produce a result that is otherwise incomputable on that platform, even if it is natively supported and more efficient in hardware on other platforms. This is often the case in <a href="/wiki/Embedded_system" title="Embedded system">embedded systems</a> with respect to <a href="/wiki/Floating-point_arithmetic" title="Floating-point arithmetic">floating-point arithmetic</a>, where small and <a class="mw-redirect" href="/wiki/Low-power_computing" title="Low-power computing">low-power</a> <a href="/wiki/Microcontroller" title="Microcontroller">microcontrollers</a> often lack hardware support for floating-point arithmetic and thus require computationally expensive software routines to produce floating point calculations.\n</p>, <p>Measures are normally expressed as a function of the size of the input <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle \\scriptstyle {n}}" xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mstyle displaystyle="false" scriptlevel="1">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>n</mi>\n          </mrow>\n        </mstyle>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\scriptstyle {n}}</annotation>\n  </semantics>\n</math></span><img alt="{\\displaystyle \\scriptstyle {n}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4108828bc4534897ce3e5e92aba9d8c5de06392a" style="vertical-align: -0.338ex; width:0.986ex; height:1.343ex;"/></span>.\n</p>, <p>The two most common measures are:\n</p>, <p>For computers whose power is supplied by a battery (e.g. <a href="/wiki/Laptop" title="Laptop">laptops</a> and <a href="/wiki/Smartphone" title="Smartphone">smartphones</a>), or for very long/large calculations (e.g. <a href="/wiki/Supercomputer" title="Supercomputer">supercomputers</a>), other measures of interest are:\n</p>, <p>As of  2018<sup class="plainlinks noexcerpt noprint asof-tag update" style="display:none;"><a class="external text" href="//en.wikipedia.org/w/index.php?title=Algorithmic_efficiency&amp;action=edit">[update]</a></sup>, power consumption is growing as an important metric for computational tasks of all types and at all scales ranging from <a href="/wiki/Embedded_system" title="Embedded system">embedded</a> <a href="/wiki/Internet_of_things" title="Internet of things">Internet of things</a> devices to <a class="mw-redirect" href="/wiki/System-on-chip" title="System-on-chip">system-on-chip</a> devices to <a href="/wiki/Server_farm" title="Server farm">server farms</a>. This trend is often referred to as <a href="/wiki/Green_computing" title="Green computing">green computing</a>.\n</p>, <p>Less common measures of computational efficiency may also be relevant in some cases:\n</p>, <p><a href="/wiki/Analysis_of_algorithms" title="Analysis of algorithms">Analyze</a> the algorithm, typically using <a href="/wiki/Time_complexity" title="Time complexity">time complexity</a> analysis to get an estimate of the running time as a function of the size of the input data. The result is normally expressed using <a href="/wiki/Big_O_notation" title="Big O notation">Big O notation</a>. This is useful for comparing algorithms, especially when a large amount of data is to be processed. More detailed estimates are needed to compare algorithm performance when the amount of data is small, although this is likely to be of less importance. <a href="/wiki/Parallel_algorithm" title="Parallel algorithm">Algorithms which include parallel processing</a> may be <a href="/wiki/Analysis_of_parallel_algorithms" title="Analysis of parallel algorithms">more difficult to analyze</a>.\n</p>, <p>Use a <a href="/wiki/Benchmark_(computing)" title="Benchmark (computing)">benchmark</a> to time the use of an algorithm. Many programming languages have an available function which provides <a href="/wiki/CPU_time" title="CPU time">CPU time usage</a>. For long-running algorithms the elapsed time could also be of interest. Results should generally be averaged over several tests.\n</p>, <p>Run-based profiling can be very sensitive to hardware configuration and the possibility of other programs or tasks running at the same time in a <a class="mw-redirect" href="/wiki/Multi-processing" title="Multi-processing">multi-processing</a> and <a class="mw-redirect" href="/wiki/Multi-programming" title="Multi-programming">multi-programming</a> environment.\n</p>, <p>This sort of test also depends heavily on the selection of a particular programming language, compiler, and compiler options, so algorithms being compared must all be implemented under the same conditions.\n</p>, <p>This section is concerned with the use of memory resources (<a href="/wiki/Processor_register" title="Processor register">registers</a>, <a href="/wiki/Cache_(computing)" title="Cache (computing)">cache</a>, <a href="/wiki/Random-access_memory" title="Random-access memory">RAM</a>, <a href="/wiki/Virtual_memory" title="Virtual memory">virtual memory</a>, <a href="/wiki/Auxiliary_memory" title="Auxiliary memory">secondary memory</a>) while the algorithm is being executed. As for time analysis above, <a href="/wiki/Analysis_of_algorithms" title="Analysis of algorithms">analyze</a> the algorithm, typically using <a class="mw-redirect" href="/wiki/Space_complexity" title="Space complexity">space complexity</a> analysis to get an estimate of the run-time memory needed as a function as the size of the input data. The result is normally expressed using <a href="/wiki/Big_O_notation" title="Big O notation">Big O notation</a>.\n</p>, <p>There are up to four aspects of memory usage to consider:\n</p>, <p>Early electronic computers, and early home computers, had relatively small amounts of working memory. For example, the 1949 <a class="mw-redirect" href="/wiki/Electronic_Delay_Storage_Automatic_Calculator" title="Electronic Delay Storage Automatic Calculator">Electronic Delay Storage Automatic Calculator</a> (EDSAC) had a maximum working memory of 1024 17-bit words, while the 1980 Sinclair <a href="/wiki/ZX80" title="ZX80">ZX80</a> came initially with 1024 8-bit bytes of working memory. In the late <a href="/wiki/2010s" title="2010s">2010s</a>, it is typical for <a href="/wiki/Personal_computer" title="Personal computer">personal computers</a> to have between 4 and 32 <a href="/wiki/Gigabyte" title="Gigabyte">GB</a> of RAM, an increase of over 300 million times as much memory.\n</p>, <p>Current computers can have relatively large amounts of memory (possibly Gigabytes), so having to squeeze an algorithm into a confined amount of memory is much less of a problem than it used to be. But the presence of four different categories of memory can be significant:\n</p>, <p>An algorithm whose memory needs will fit in cache memory will be much faster than an algorithm which fits in main memory, which in turn will be very much faster than an algorithm which has to resort to virtual memory. Because of this, <a href="/wiki/Cache_replacement_policies" title="Cache replacement policies">cache replacement policies</a> are extremely important to high-performance computing, as are <a class="mw-redirect" href="/wiki/Cache-aware_model" title="Cache-aware model">cache-aware programming</a> and <a href="/wiki/Data_structure_alignment" title="Data structure alignment">data alignment</a>. To further complicate the issue, some systems have up to three levels of cache memory, with varying effective speeds. Different systems will have different amounts of these various types of memory, so the effect of algorithm memory needs can vary greatly from one system to another.\n</p>, <p>In the early days of electronic computing, if an algorithm and its data wouldn\'t fit in main memory then the algorithm couldn\'t be used. Nowadays the use of virtual memory appears to provide lots of memory, but at the cost of performance. If an algorithm and its data will fit in cache memory, then very high speed can be obtained; in this case minimizing space will also help minimize time. This is called the <a href="/wiki/Principle_of_locality" title="Principle of locality">principle of locality</a>, and can be subdivided into <a href="/wiki/Locality_of_reference" title="Locality of reference">locality of reference</a>, <a class="mw-redirect" href="/wiki/Spatial_locality" title="Spatial locality">spatial locality</a> and <a class="mw-redirect" href="/wiki/Temporal_locality" title="Temporal locality">temporal locality</a>. An algorithm which will not fit completely in cache memory but which exhibits locality of reference may perform reasonably well.\n</p>, <p>Software efficiency halves every 18 months, compensating Moore\'s Law\n</p>, <p>In ubiquitous systems, halving the instructions executed can double the battery life and big data sets bring big opportunities for better software and algorithms: Reducing the number of operations from N x N to N x log(N) has a dramatic effect when N is large ... for N = 30 billion, this change is as good as 50 years of technology improvements.\n</p>, <p>The following competitions invite entries for the best algorithms based on some arbitrary criteria decided by the judges:\n</p>]