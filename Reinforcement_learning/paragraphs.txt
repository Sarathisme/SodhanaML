Reinforcementlearning(RL)isanareaofmachinelearningconcernedwithhowsoftwareagentsoughttotakeactionsinanenvironmentsoastomaximizesomenotionofcumulativereward.Theproblem,duetoitsgenerality,isstudiedinmanyotherdisciplines,suchasgametheory,controltheory,operationsresearch,informationtheory,simulation-basedoptimization,multi-agentsystems,swarmintelligence,statisticsandgeneticalgorithms.Intheoperationsresearchandcontrolliterature,reinforcementlearningiscalledapproximatedynamicprogramming,orneuro-dynamicprogramming.[1][2]\nTheproblemsofinterestinreinforcementlearninghavealsobeenstudiedinthetheoryofoptimalcontrol,whichisconcernedmostlywiththeexistenceandcharacterizationofoptimalsolutions,andalgorithmsfortheirexactcomputation,andlesswithlearningorapproximation,particularlyintheabsenceofamathematicalmodeloftheenvironment.Ineconomicsandgametheory,reinforcementlearningmaybeusedtoexplainhowequilibriummayariseunderboundedrationality.\nInmachinelearning,theenvironmentistypicallyformulatedasaMarkovDecisionProcess(MDP),asmanyreinforcementlearningalgorithmsforthiscontextutilizedynamicprogrammingtechniques.[2][1][3]ThemaindifferencebetweentheclassicaldynamicprogrammingmethodsandreinforcementlearningalgorithmsisthatthelatterdonotassumeknowledgeofanexactmathematicalmodeloftheMDPandtheytargetlargeMDPswhereexactmethodsbecomeinfeasible.[2][1]\nReinforcementlearningisconsideredasoneofthreemachinelearningparadigms,alongsidesupervisedlearningandunsupervisedlearning.Itdiffersfromsupervisedlearninginthatcorrectinput/outputpairs[clarificationneeded]neednotbepresented,andsub-optimalactionsneednotbeexplicitlycorrected.Insteadthefocusisonperformance[clarificationneeded],whichinvolvesfindingabalancebetweenexploration(ofunchartedterritory)andexploitation(ofcurrentknowledge).[4]Theexplorationvs.exploitationtrade-offhasbeenmostthoroughlystudiedthroughthemulti-armedbanditproblemandinfiniteMDPs.[citationneeded]\nBasicreinforcementismodeledasaMarkovdecisionprocess:\nRulesareoftenstochastic.Theobservationtypicallyinvolvesthescalar,immediaterewardassociatedwiththelasttransition.Inmanyworks,theagentisassumedtoobservethecurrentenvironmentalstate(fullobservability).Ifnot,theagenthaspartialobservability.Sometimesthesetofactionsavailabletotheagentisrestricted(azerobalancecannotbereduced).\nAreinforcementlearningagentinteractswithitsenvironmentindiscretetimesteps.Ateachtimet,theagentreceivesanobservation\n\n\n\n\no\n\nt\n\n\n\n\n{\\displaystyleo_{t}}\n\n,whichtypicallyincludesthereward\n\n\n\n\nr\n\nt\n\n\n\n\n{\\displaystyler_{t}}\n\n.Itthenchoosesanaction\n\n\n\n\na\n\nt\n\n\n\n\n{\\displaystylea_{t}}\n\nfromthesetofavailableactions,whichissubsequentlysenttotheenvironment.Theenvironmentmovestoanewstate\n\n\n\n\ns\n\nt\n+\n1\n\n\n\n\n{\\displaystyles_{t+1}}\n\nandthereward\n\n\n\n\nr\n\nt\n+\n1\n\n\n\n\n{\\displaystyler_{t+1}}\n\nassociatedwiththetransition\n\n\n\n(\n\ns\n\nt\n\n\n,\n\na\n\nt\n\n\n,\n\ns\n\nt\n+\n1\n\n\n)\n\n\n{\\displaystyle(s_{t},a_{t},s_{t+1})}\n\nisdetermined.Thegoalofareinforcementlearningagentistocollectasmuchrewardaspossible.Theagentcan(possiblyrandomly)chooseanyactionasafunctionofthehistory.\nWhentheagent\'sperformanceiscomparedtothatofanagentthatactsoptimally,thedifferenceinperformancegivesrisetothenotionofregret.Inordertoactnearoptimally,theagentmustreasonaboutthelongtermconsequencesofitsactions(i.e.,maximizefutureincome),althoughtheimmediaterewardassociatedwiththismightbenegative.\nThus,reinforcementlearningisparticularlywell-suitedtoproblemsthatincludealong-termversusshort-termrewardtrade-off.Ithasbeenappliedsuccessfullytovariousproblems,includingrobotcontrol,elevatorscheduling,telecommunications,backgammon,checkers[5]andgo(AlphaGo).\nTwoelementsmakereinforcementlearningpowerful:theuseofsamplestooptimizeperformanceandtheuseoffunctionapproximationtodealwithlargeenvironments.Thankstothesetwokeycomponents,reinforcementlearningcanbeusedinlargeenvironmentsinthefollowingsituations:\nThefirsttwooftheseproblemscouldbeconsideredplanningproblems(sincesomeformofmodelisavailable),whilethelastonecouldbeconsideredtobeagenuinelearningproblem.However,reinforcementlearningconvertsbothplanningproblemstomachinelearningproblems.\nReinforcementlearningrequirescleverexplorationmechanisms.Randomlyselectingactions,withoutreferencetoanestimatedprobabilitydistribution,showspoorperformance.Thecaseof(small)finiteMarkovdecisionprocessesisrelativelywellunderstood.However,duetothelackofalgorithmsthatprovablyscalewellwiththenumberofstates(orscaletoproblemswithinfinitestatespaces),simpleexplorationmethodsarethemostpractical.\nOnesuchmethodis\n\n\n\nϵ\n\n\n{\\displaystyle\\epsilon}\n\n-greedy,whentheagentchoosestheactionthatitbelieveshasthebestlong-termeffectwithprobability\n\n\n\n1\n−\nϵ\n\n\n{\\displaystyle1-\\epsilon}\n\n.Ifnoactionwhichsatisfiesthisconditionisfound,theagentchoosesanactionuniformlyatrandom.Here,\n\n\n\n0\n<\nϵ\n<\n1\n\n\n{\\displaystyle0<\\epsilon<1}\n\nisatuningparameter,whichissometimeschanged,eitheraccordingtoafixedschedule(makingtheagentexploreprogressivelyless),oradaptivelybasedonheuristics.[7]\nEveniftheissueofexplorationisdisregardedandevenifthestatewasobservable(assumedhereafter),theproblemremainstousepastexperiencetofindoutwhichactionsaregood.\nTheagent\'sactionselectionismodeledasamapcalledpolicy:\nThepolicymapgivestheprobabilityoftakingaction\n\n\n\na\n\n\n{\\displaystylea}\n\nwheninstate\n\n\n\ns\n\n\n{\\displaystyles}\n\n.[8]:61Therearealsonon-probabilisticpolicies.\nValuefunction\n\n\n\n\nV\n\nπ\n\n\n(\ns\n)\n\n\n{\\displaystyleV_{\\pi}(s)}\n\nisdefinedastheexpectedreturnstartingwithstate\n\n\n\ns\n\n\n{\\displaystyles}\n\n,i.e.\n\n\n\n\ns\n\n0\n\n\n=\ns\n\n\n{\\displaystyles_{0}=s}\n\n,andsuccessivelyfollowingpolicy\n\n\n\nπ\n\n\n{\\displaystyle\\pi}\n\n.Hence,roughlyspeaking,thevaluefunctionestimates"howgood"itistobeinagivenstate.[8]:60\nwheretherandomvariable\n\n\n\nR\n\n\n{\\displaystyleR}\n\ndenotesthereturn,andisdefinedasthesumoffuturediscountedrewards\nwhere\n\n\n\n\nr\n\nt\n\n\n\n\n{\\displaystyler_{t}}\n\nistherewardatstep\n\n\n\nt\n\n\n{\\displaystylet}\n\n,\n\n\n\nγ\n∈\n[\n0\n,\n1\n]\n\n\n{\\displaystyle\\gamma\\in[0,1]}\n\nisthediscount-rate.\nThealgorithmmustfindapolicywithmaximumexpectedreturn.FromthetheoryofMDPsitisknownthat,withoutlossofgenerality,thesearchcanberestrictedtothesetofso-calledstationarypolicies.Apolicyisstationaryiftheaction-distributionreturnedbyitdependsonlyonthelaststatevisited(fromtheobservationagent\'shistory).Thesearchcanbefurtherrestrictedtodeterministicstationarypolicies.Adeterministicstationarypolicydeterministicallyselectsactionsbasedonthecurrentstate.Sinceanysuchpolicycanbeidentifiedwithamappingfromthesetofstatestothesetofactions,thesepoliciescanbeidentifiedwithsuchmappingswithnolossofgenerality.\nThebruteforceapproachentailstwosteps:\nOneproblemwiththisisthatthenumberofpoliciescanbelarge,oreveninfinite.Anotheristhatvarianceofthereturnsmaybelarge,whichrequiresmanysamplestoaccuratelyestimatethereturnofeachpolicy.\nTheseproblemscanbeamelioratedifweassumesomestructureandallowsamplesgeneratedfromonepolicytoinfluencetheestimatesmadeforothers.Thetwomainapproachesforachievingthisarevaluefunctionestimationanddirectpolicysearch.\nValuefunctionapproachesattempttofindapolicythatmaximizesthereturnbymaintainingasetofestimatesofexpectedreturnsforsomepolicy(usuallyeitherthe"current"[on-policy]ortheoptimal[off-policy]one).\nThesemethodsrelyonthetheoryofMDPs,whereoptimalityisdefinedinasensethatisstrongerthantheaboveone:Apolicyiscalledoptimalifitachievesthebestexpectedreturnfromanyinitialstate(i.e.,initialdistributionsplaynoroleinthisdefinition).Again,anoptimalpolicycanalwaysbefoundamongststationarypolicies.\nTodefineoptimalityinaformalmanner,definethevalueofapolicy\n\n\n\nπ\n\n\n{\\displaystyle\\pi}\n\nby\nwhere\n\n\n\nR\n\n\n{\\displaystyleR}\n\nstandsforthereturnassociatedwithfollowing\n\n\n\nπ\n\n\n{\\displaystyle\\pi}\n\nfromtheinitialstate\n\n\n\ns\n\n\n{\\displaystyles}\n\n.Defining\n\n\n\n\nV\n\n∗\n\n\n(\ns\n)\n\n\n{\\displaystyleV^{*}(s)}\n\nasthemaximumpossiblevalueof\n\n\n\n\nV\n\nπ\n\n\n(\ns\n)\n\n\n{\\displaystyleV^{\\pi}(s)}\n\n,where\n\n\n\nπ\n\n\n{\\displaystyle\\pi}\n\nisallowedtochange,\nApolicythatachievestheseoptimalvaluesineachstateiscalledoptimal.Clearly,apolicythatisoptimalinthisstrongsenseisalsooptimalinthesensethatitmaximizestheexpectedreturn\n\n\n\n\nρ\n\nπ\n\n\n\n\n{\\displaystyle\\rho^{\\pi}}\n\n,since\n\n\n\n\nρ\n\nπ\n\n\n=\nE\n[\n\nV\n\nπ\n\n\n(\nS\n)\n]\n\n\n{\\displaystyle\\rho^{\\pi}=E[V^{\\pi}(S)]}\n\n,where\n\n\n\nS\n\n\n{\\displaystyleS}\n\nisastaterandomlysampledfromthedistribution\n\n\n\nμ\n\n\n{\\displaystyle\\mu}\n\n[clarificationneeded].\nAlthoughstate-valuessufficetodefineoptimality,itisusefultodefineaction-values.Givenastate\n\n\n\ns\n\n\n{\\displaystyles}\n\n,anaction\n\n\n\na\n\n\n{\\displaystylea}\n\nandapolicy\n\n\n\nπ\n\n\n{\\displaystyle\\pi}\n\n,theaction-valueofthepair\n\n\n\n(\ns\n,\na\n)\n\n\n{\\displaystyle(s,a)}\n\nunder\n\n\n\nπ\n\n\n{\\displaystyle\\pi}\n\nisdefinedby\nwhere\n\n\n\nR\n\n\n{\\displaystyleR}\n\nnowstandsfortherandomreturnassociatedwithfirsttakingaction\n\n\n\na\n\n\n{\\displaystylea}\n\ninstate\n\n\n\ns\n\n\n{\\displaystyles}\n\nandfollowing\n\n\n\nπ\n\n\n{\\displaystyle\\pi}\n\n,thereafter.\nThetheoryofMDPsstatesthatif\n\n\n\n\nπ\n\n∗\n\n\n\n\n{\\displaystyle\\pi^{*}}\n\nisanoptimalpolicy,weactoptimally(taketheoptimalaction)bychoosingtheactionfrom\n\n\n\n\nQ\n\n\nπ\n\n∗\n\n\n\n\n(\ns\n,\n⋅\n)\n\n\n{\\displaystyleQ^{\\pi^{*}}(s,\\cdot)}\n\nwiththehighestvalueateachstate,\n\n\n\ns\n\n\n{\\displaystyles}\n\n.Theaction-valuefunctionofsuchanoptimalpolicy(\n\n\n\n\nQ\n\n\nπ\n\n∗\n\n\n\n\n\n\n{\\displaystyleQ^{\\pi^{*}}}\n\n)iscalledtheoptimalaction-valuefunctionandiscommonlydenotedby\n\n\n\n\nQ\n\n∗\n\n\n\n\n{\\displaystyleQ^{*}}\n\n.Insummary,theknowledgeoftheoptimalaction-valuefunctionalonesufficestoknowhowtoactoptimally.\nAssumingfullknowledgeoftheMDP,thetwobasicapproachestocomputetheoptimalaction-valuefunctionarevalueiterationandpolicyiteration.Bothalgorithmscomputeasequenceoffunctions\n\n\n\n\nQ\n\nk\n\n\n\n\n{\\displaystyleQ_{k}}\n\n(\n\n\n\nk\n=\n0\n,\n1\n,\n2\n,\n…\n\n\n{\\displaystylek=0,1,2,\\ldots}\n\n)thatconvergeto\n\n\n\n\nQ\n\n∗\n\n\n\n\n{\\displaystyleQ^{*}}\n\n.Computingthesefunctionsinvolvescomputingexpectationsoverthewholestate-space,whichisimpracticalforallbutthesmallest(finite)MDPs.Inreinforcementlearningmethods,expectationsareapproximatedbyaveragingoversamplesandusingfunctionapproximationtechniquestocopewiththeneedtorepresentvaluefunctionsoverlargestate-actionspaces.\nMonteCarlomethodscanbeusedinanalgorithmthatmimicspolicyiteration.Policyiterationconsistsoftwosteps:policyevaluationandpolicyimprovement.\nMonteCarloisusedinthepolicyevaluationstep.Inthisstep,givenastationary,deterministicpolicy\n\n\n\nπ\n\n\n{\\displaystyle\\pi}\n\n,thegoalistocomputethefunctionvalues\n\n\n\n\nQ\n\nπ\n\n\n(\ns\n,\na\n)\n\n\n{\\displaystyleQ^{\\pi}(s,a)}\n\n(oragoodapproximationtothem)forallstate-actionpairs\n\n\n\n(\ns\n,\na\n)\n\n\n{\\displaystyle(s,a)}\n\n.Assuming(forsimplicity)thattheMDPisfinite,thatsufficientmemoryisavailabletoaccommodatetheaction-valuesandthattheproblemisepisodicandaftereachepisodeanewonestartsfromsomerandominitialstate.Then,theestimateofthevalueofagivenstate-actionpair\n\n\n\n(\ns\n,\na\n)\n\n\n{\\displaystyle(s,a)}\n\ncanbecomputedbyaveragingthesampledreturnsthatoriginatedfrom\n\n\n\n(\ns\n,\na\n)\n\n\n{\\displaystyle(s,a)}\n\novertime.Givensufficienttime,thisprocedurecanthusconstructapreciseestimate\n\n\n\nQ\n\n\n{\\displaystyleQ}\n\noftheaction-valuefunction\n\n\n\n\nQ\n\nπ\n\n\n\n\n{\\displaystyleQ^{\\pi}}\n\n.Thisfinishesthedescriptionofthepolicyevaluationstep.\nInthepolicyimprovementstep,thenextpolicyisobtainedbycomputingagreedypolicywithrespectto\n\n\n\nQ\n\n\n{\\displaystyleQ}\n\n:Givenastate\n\n\n\ns\n\n\n{\\displaystyles}\n\n,thisnewpolicyreturnsanactionthatmaximizes\n\n\n\nQ\n(\ns\n,\n⋅\n)\n\n\n{\\displaystyleQ(s,\\cdot)}\n\n.Inpracticelazyevaluationcandeferthecomputationofthemaximizingactionstowhentheyareneeded.\nProblemswiththisprocedureinclude:\nThefirstproblemiscorrectedbyallowingtheproceduretochangethepolicy(atsomeorallstates)beforethevaluessettle.Thistoomaybeproblematicasitmightpreventconvergence.Mostcurrentalgorithmsdothis,givingrisetotheclassofgeneralizedpolicyiterationalgorithms.Manyactorcriticmethodsbelongtothiscategory.\nThesecondissuecanbecorrectedbyallowingtrajectoriestocontributetoanystate-actionpairinthem.Thismayalsohelptosomeextentwiththethirdproblem,althoughabettersolutionwhenreturnshavehighvarianceisSutton\'s[9][10]temporaldifference(TD)methodsthatarebasedontherecursiveBellmanequation.NotethatthecomputationinTDmethodscanbeincremental(whenaftereachtransitionthememoryischangedandthetransitionisthrownaway),orbatch(whenthetransitionsarebatchedandtheestimatesarecomputedoncebasedonthebatch).Batchmethods,suchastheleast-squarestemporaldifferencemethod,[11]mayusetheinformationinthesamplesbetter,whileincrementalmethodsaretheonlychoicewhenbatchmethodsareinfeasibleduetotheirhighcomputationalormemorycomplexity.Somemethodstrytocombinethetwoapproaches.Methodsbasedontemporaldifferencesalsoovercomethefourthissue.\nInordertoaddressthefifthissue,functionapproximationmethodsareused.Linearfunctionapproximationstartswithamapping\n\n\n\nϕ\n\n\n{\\displaystyle\\phi}\n\nthatassignsafinite-dimensionalvectortoeachstate-actionpair.Then,theactionvaluesofastate-actionpair\n\n\n\n(\ns\n,\na\n)\n\n\n{\\displaystyle(s,a)}\n\nareobtainedbylinearlycombiningthecomponentsof\n\n\n\nϕ\n(\ns\n,\na\n)\n\n\n{\\displaystyle\\phi(s,a)}\n\nwithsomeweights\n\n\n\nθ\n\n\n{\\displaystyle\\theta}\n\n:\nThealgorithmsthenadjusttheweights,insteadofadjustingthevaluesassociatedwiththeindividualstate-actionpairs.Methodsbasedonideasfromnonparametricstatistics(whichcanbeseentoconstructtheirownfeatures)havebeenexplored.\nValueiterationcanalsobeusedasastartingpoint,givingrisetotheQ-Learningalgorithmanditsmanyvariants.[12]\nTheproblemwithusingaction-valuesisthattheymayneedhighlypreciseestimatesofthecompetingactionvaluesthatcanbehardtoobtainwhenthereturnsarenoisy.Thoughthisproblemismitigatedtosomeextentbytemporaldifferencemethods.Usingtheso-calledcompatiblefunctionapproximationmethodcompromisesgeneralityandefficiency.AnotherproblemspecifictoTDcomesfromtheirrelianceontherecursiveBellmanequation.MostTDmethodshaveaso-called\n\n\n\nλ\n\n\n{\\displaystyle\\lambda}\n\nparameter\n\n\n\n(\n0\n≤\nλ\n≤\n1\n)\n\n\n{\\displaystyle(0\\leq\\lambda\\leq1)}\n\nthatcancontinuouslyinterpolatebetweenMonteCarlomethodsthatdonotrelyontheBellmanequationsandthebasicTDmethodsthatrelyentirelyontheBellmanequations.Thiscanbeeffectiveinpalliatingthisissue.\nAnalternativemethodistosearchdirectlyin(somesubsetof)thepolicyspace,inwhichcasetheproblembecomesacaseofstochasticoptimization.Thetwoapproachesavailablearegradient-basedandgradient-freemethods.\nGradient-basedmethods(policygradientmethods)startwithamappingfromafinite-dimensional(parameter)spacetothespaceofpolicies:giventheparametervector\n\n\n\nθ\n\n\n{\\displaystyle\\theta}\n\n,let\n\n\n\n\nπ\n\nθ\n\n\n\n\n{\\displaystyle\\pi_{\\theta}}\n\ndenotethepolicyassociatedto\n\n\n\nθ\n\n\n{\\displaystyle\\theta}\n\n.Definingtheperformancefunctionby\nundermildconditionsthisfunctionwillbedifferentiableasafunctionoftheparametervector\n\n\n\nθ\n\n\n{\\displaystyle\\theta}\n\n.Ifthegradientof\n\n\n\nρ\n\n\n{\\displaystyle\\rho}\n\nwasknown,onecouldusegradientascent.Sinceananalyticexpressionforthegradientisnotavailable,onlyanoisyestimateisavailable.Suchanestimatecanbeconstructedinmanyways,givingrisetoalgorithmssuchasWilliams\'REINFORCE[13]method(whichisknownasthelikelihoodratiomethodinthesimulation-basedoptimizationliterature).[14]Policysearchmethodshavebeenusedintheroboticscontext.[15]Manypolicysearchmethodsmaygetstuckinlocaloptima(astheyarebasedonlocalsearch).\nAlargeclassofmethodsavoidsrelyingongradientinformation.Theseincludesimulatedannealing,cross-entropysearchormethodsofevolutionarycomputation.Manygradient-freemethodscanachieve(intheoryandinthelimit)aglobaloptimum.\nPolicysearchmethodsmayconvergeslowlygivennoisydata.Forexample,thishappensinepisodicproblemswhenthetrajectoriesarelongandthevarianceofthereturnsislarge.Value-functionbasedmethodsthatrelyontemporaldifferencesmighthelpinthiscase.Inrecentyears,actor\xe2\x80\x93criticmethodshavebeenproposedandperformedwellonvariousproblems.[16]\nBoththeasymptoticandfinite-samplebehaviorofmostalgorithmsiswellunderstood.Algorithmswithprovablygoodonlineperformance(addressingtheexplorationissue)areknown.\nEfficientexplorationoflargeMDPsislargelyunexplored(exceptforthecaseofbanditproblems).[clarificationneeded]Althoughfinite-timeperformanceboundsappearedformanyalgorithms,theseboundsareexpectedtoberatherlooseandthusmoreworkisneededtobetterunderstandtherelativeadvantagesandlimitations.\nForincrementalalgorithms,asymptoticconvergenceissueshavebeensettled.Temporal-difference-basedalgorithmsconvergeunderawidersetofconditionsthanwaspreviouslypossible(forexample,whenusedwitharbitrary,smoothfunctionapproximation).\nResearchtopicsinclude\nMultiagentordistributedreinforcementlearningisatopicofinterest.Applicationsareexpanding.[18]\nReinforcementlearningalgorithmssuchasTDlearningareunderinvestigationasamodelfordopamine-basedlearninginthebrain.Inthismodel,thedopaminergicprojectionsfromthesubstantianigratothebasalgangliafunctionasthepredictionerror.Reinforcementlearninghasbeenusedasapartofthemodelforhumanskilllearning,especiallyinrelationtotheinteractionbetweenimplicitandexplicitlearninginskillacquisition(thefirstpublicationonthisapplicationwasin1995-1996).[19]\nTheworkonlearningATARITVgamesbyGoogleDeepMind[20]increasedattentiontoend-to-endreinforcementlearningordeepreinforcementlearning.Thisapproachextendsreinforcementlearningtotheentireprocessfromobservationtoaction(sensorstomotorsorendtoend)byformingitusingadeepnetworkandwithoutexplicitlydesigningstatespaceoractionspace.\nIninversereinforcementlearning(IRL),norewardfunctionisgiven.Instead,therewardfunctionisinferredgivenanobservedbehaviorfromanexpert.Theideaistomimicobservedbehavior,whichisoftenoptimalorclosetooptimal.[21]\nInapprenticeshiplearning,anexpertdemonstratesthetargetbehavior.Thesystemtriestorecoverthepolicyviaobservation.\nMostreinforcementlearningpapersarepublishedatthemajormachinelearningandAIconferences(ICML,NIPS,AAAI,IJCAI,UAI,AIandStatistics)andjournals(JAIR,JMLR,Machinelearningjournal,IEEET-CIAIG).SometheorypapersarepublishedatCOLTandALT.However,manypapersappearinroboticsconferences(IROS,ICRA)andthe"agent"conferenceAAMAS.OperationsresearcherspublishtheirpapersattheINFORMSconferenceand,forexample,intheOperationResearch,andtheMathematicsofOperationsResearchjournals.ControlresearcherspublishtheirpapersattheCDCandACCconferences,or,e.g.,inthejournalsIEEETransactionsonAutomaticControl,orAutomatica,althoughappliedworkstendtobepublishedinmorespecializedjournals.TheWinterSimulationConferencealsopublishesmanyrelevantpapers.Otherthanthis,papersalsopublishedinthemajorconferencesoftheneuralnetworks,fuzzy,andevolutionarycomputationcommunities.TheannualIEEEsymposiumtitledApproximateDynamicProgrammingandReinforcementLearning(ADPRL)andthebiannualEuropeanWorkshoponReinforcementLearning(EWRL)aretworegularlyheldmeetingswhereRLresearchersmeet.\n