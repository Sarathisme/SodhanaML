[<p>\n<b>Computational complexity theory</b> focuses on classifying computational problems according to their inherent difficulty, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.\n</p>, <p>A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical <a class="mw-redirect" href="/wiki/Models_of_computation" title="Models of computation">models of computation</a> to study these problems and quantifying their <a href="/wiki/Computational_complexity" title="Computational complexity">computational complexity</a>, i.e., the amount of resources needed to solve them, such as time and storage. Other measures of complexity are also used, such as the amount of communication (used in <a href="/wiki/Communication_complexity" title="Communication complexity">communication complexity</a>), the number of <a href="/wiki/Logic_gate" title="Logic gate">gates</a> in a circuit (used in <a href="/wiki/Circuit_complexity" title="Circuit complexity">circuit complexity</a>) and the number of processors (used in <a href="/wiki/Parallel_computing" title="Parallel computing">parallel computing</a>). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do. The <a href="/wiki/P_versus_NP_problem" title="P versus NP problem">P versus NP problem</a>, one of the seven <a class="mw-redirect" href="/wiki/Millenium_Prize_Problems" title="Millenium Prize Problems">Millenium Prize Problems</a>, is dedicated to the field of computational complexity.<sup class="reference" id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup>\n</p>, <p>Closely related fields in theoretical computer science are <a href="/wiki/Analysis_of_algorithms" title="Analysis of algorithms">analysis of algorithms</a> and <a href="/wiki/Computability_theory" title="Computability theory">computability theory</a>. A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. More precisely, computational complexity theory tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kind of problems can, in principle, be solved algorithmically.\n</p>, <p>A <a href="/wiki/Computational_problem" title="Computational problem">computational problem</a> can be viewed as an infinite collection of <i>instances</i> together with a <i>solution</i> for every instance. The input string for a computational problem is referred to as a problem instance, and should not be confused with the problem itself. In computational complexity theory, a problem refers to the abstract question to be solved. In contrast, an instance of this problem is a rather concrete utterance, which can serve as the input for a decision problem. For example, consider the problem of <a class="mw-redirect" href="/wiki/Primality_testing" title="Primality testing">primality testing</a>. The instance is a number (e.g., 15) and the solution is "yes" if the number is prime and "no" otherwise (in this case, 15 is not prime and the answer is "no"). Stated another way, the <i>instance</i> is a particular input to the problem, and the <i>solution</i> is the output corresponding to the given input.\n</p>, <p>To further highlight the difference between a problem and an instance, consider the following instance of the decision version of the <a class="mw-redirect" href="/wiki/Traveling_salesman_problem" title="Traveling salesman problem">traveling salesman problem</a>: Is there a route of at most 2000 kilometres passing through all of Germany\'s 15 largest cities? The quantitative answer to this particular problem instance is of little use for solving other instances of the problem, such as asking for a round trip through all sites in <a href="/wiki/Milan" title="Milan">Milan</a> whose total length is at most 10 km. For this reason, complexity theory addresses computational problems and not particular problem instances.\n</p>, <p>When considering computational problems, a problem instance is a <a href="/wiki/String_(computer_science)" title="String (computer science)">string</a> over an <a class="mw-redirect" href="/wiki/Alphabet_(computer_science)" title="Alphabet (computer science)">alphabet</a>. Usually, the alphabet is taken to be the binary alphabet (i.e., the set {0,1}), and thus the strings are <a class="mw-redirect" href="/wiki/Bitstring" title="Bitstring">bitstrings</a>. As in a real-world <a href="/wiki/Computer" title="Computer">computer</a>, mathematical objects other than bitstrings must be suitably encoded. For example, <a href="/wiki/Integer" title="Integer">integers</a> can be represented in <a class="mw-redirect" href="/wiki/Binary_notation" title="Binary notation">binary notation</a>, and <a href="/wiki/Graph_(discrete_mathematics)" title="Graph (discrete mathematics)">graphs</a> can be encoded directly via their <a href="/wiki/Adjacency_matrix" title="Adjacency matrix">adjacency matrices</a>, or by encoding their <a href="/wiki/Adjacency_list" title="Adjacency list">adjacency lists</a> in binary.\n</p>, <p>Even though some proofs of complexity-theoretic theorems regularly assume some concrete choice of input encoding, one tries to keep the discussion abstract enough to be independent of the choice of encoding. This can be achieved by ensuring that different representations can be transformed into each other efficiently.\n</p>, <p><a href="/wiki/Decision_problem" title="Decision problem">Decision problems</a> are one of the central objects of study in computational complexity theory. A decision problem is a special type of computational problem whose answer is either <i>yes</i> or <i>no</i>, or alternately either 1 or 0. A decision problem can be viewed as a <a href="/wiki/Formal_language" title="Formal language">formal language</a>, where the members of the language are instances whose output is yes, and the non-members are those instances whose output is no. The objective is to decide, with the aid of an <a href="/wiki/Algorithm" title="Algorithm">algorithm</a>, whether a given input string is a member of the formal language under consideration. If the algorithm deciding this problem returns the answer <i>yes</i>, the algorithm is said to accept the input string, otherwise it is said to reject the input.\n</p>, <p>An example of a decision problem is the following. The input is an arbitrary <a href="/wiki/Graph_(discrete_mathematics)" title="Graph (discrete mathematics)">graph</a>. The problem consists in deciding whether the given graph is <a href="/wiki/Connectivity_(graph_theory)" title="Connectivity (graph theory)">connected</a>, or not. The formal language associated with this decision problem is then the set of all connected graphs \xe2\x80\x94 to obtain a precise definition of this language, one has to decide how graphs are encoded as binary strings.\n</p>, <p>A <a href="/wiki/Function_problem" title="Function problem">function problem</a> is a computational problem where a single output (of a <a class="mw-redirect" href="/wiki/Total_function" title="Total function">total function</a>) is expected for every input, but the output is more complex than that of a <a href="/wiki/Decision_problem" title="Decision problem">decision problem</a>\xe2\x80\x94that is, the output isn\'t just yes or no. Notable examples include the <a class="mw-redirect" href="/wiki/Traveling_salesman_problem" title="Traveling salesman problem">traveling salesman problem</a> and the <a class="mw-redirect" href="/wiki/Integer_factorization_problem" title="Integer factorization problem">integer factorization problem</a>.\n</p>, <p>It is tempting to think that the notion of function problems is much richer than the notion of decision problems. However, this is not really the case, since function problems can be recast as decision problems. For example, the multiplication of two integers can be expressed as the set of triples (<i>a</i>, <i>b</i>, <i>c</i>) such that the relation <i>a</i> \xc3\x97 <i>b</i> = <i>c</i> holds. Deciding whether a given triple is a member of this set corresponds to solving the problem of multiplying two numbers.\n</p>, <p>To measure the difficulty of solving a computational problem, one may wish to see how much time the best algorithm requires to solve the problem. However, the running time may, in general, depend on the instance. In particular, larger instances will require more time to solve. Thus the time required to solve a problem (or the space required, or any measure of complexity) is calculated as a function of the size of the instance. This is usually taken to be the size of the input in bits. Complexity theory is interested in how algorithms scale with an increase in the input size. For instance, in the problem of finding whether a graph is connected, how much more time does it take to solve a problem for a graph with 2<i>n</i> vertices compared to the time taken for a graph with <i>n</i> vertices?\n</p>, <p>If the input size is <i>n</i>, the time taken can be expressed as a function of <i>n</i>. Since the time taken on different inputs of the same size can be different, the worst-case time complexity T(<i>n</i>) is defined to be the maximum time taken over all inputs of size <i>n</i>. If T(<i>n</i>) is a polynomial in <i>n</i>, then the algorithm is said to be a <a class="mw-redirect" href="/wiki/Polynomial_time" title="Polynomial time">polynomial time</a> algorithm. <a href="/wiki/Cobham%27s_thesis" title="Cobham's thesis">Cobham\'s thesis</a> argues that a problem can be solved with a feasible amount of resources if it admits a polynomial time algorithm.\n</p>, <p>A Turing machine is a mathematical model of a general computing machine. It is a theoretical device that manipulates symbols contained on a strip of tape. Turing machines are not intended as a practical computing technology, but rather as a general model of a computing machine\xe2\x80\x94anything from an advanced supercomputer to a mathematician with a pencil and paper. It is believed that if a problem can be solved by an algorithm, there exists a Turing machine that solves the problem. Indeed, this is the statement of the <a href="/wiki/Church%E2%80%93Turing_thesis" title="Church\xe2\x80\x93Turing thesis">Church\xe2\x80\x93Turing thesis</a>. Furthermore, it is known that everything that can be computed on other models of computation known to us today, such as a <a class="mw-redirect" href="/wiki/RAM_machine" title="RAM machine">RAM machine</a>, <a href="/wiki/Conway%27s_Game_of_Life" title="Conway's Game of Life">Conway\'s Game of Life</a>, <a class="mw-redirect" href="/wiki/Cellular_automata" title="Cellular automata">cellular automata</a> or any programming language can be computed on a Turing machine. Since Turing machines are easy to analyze mathematically, and are believed to be as powerful as any other model of computation, the Turing machine is the most commonly used model in complexity theory.\n</p>, <p>Many types of Turing machines are used to define complexity classes, such as <a class="mw-redirect" href="/wiki/Deterministic_Turing_machine" title="Deterministic Turing machine">deterministic Turing machines</a>, <a href="/wiki/Probabilistic_Turing_machine" title="Probabilistic Turing machine">probabilistic Turing machines</a>, <a href="/wiki/Non-deterministic_Turing_machine" title="Non-deterministic Turing machine">non-deterministic Turing machines</a>, <a href="/wiki/Quantum_Turing_machine" title="Quantum Turing machine">quantum Turing machines</a>, <a href="/wiki/Symmetric_Turing_machine" title="Symmetric Turing machine">symmetric Turing machines</a> and <a href="/wiki/Alternating_Turing_machine" title="Alternating Turing machine">alternating Turing machines</a>. They are all equally powerful in principle, but when resources (such as time or space) are bounded, some of these may be more powerful than others.\n</p>, <p>A deterministic Turing machine is the most basic Turing machine, which uses a fixed set of rules to determine its future actions. A probabilistic Turing machine is a deterministic Turing machine with an extra supply of random bits. The ability to make probabilistic decisions often helps algorithms solve problems more efficiently. Algorithms that use random bits are called <a href="/wiki/Randomized_algorithm" title="Randomized algorithm">randomized algorithms</a>. A non-deterministic Turing machine is a deterministic Turing machine with an added feature of non-determinism, which allows a Turing machine to have multiple possible future actions from a given state. One way to view non-determinism is that the Turing machine branches into many possible computational paths at each step, and if it solves the problem in any of these branches, it is said to have solved the problem. Clearly, this model is not meant to be a physically realizable model, it is just a theoretically interesting abstract machine that gives rise to particularly interesting complexity classes. For examples, see <a class="mw-redirect" href="/wiki/Non-deterministic_algorithm" title="Non-deterministic algorithm">non-deterministic algorithm</a>.\n</p>, <p>Many machine models different from the standard <a href="/wiki/Turing_machine_equivalents#Multi-tape_Turing_machines" title="Turing machine equivalents">multi-tape Turing machines</a> have been proposed in the literature, for example <a class="mw-redirect" href="/wiki/Random_access_machine" title="Random access machine">random access machines</a>. Perhaps surprisingly, each of these models can be converted to another without providing any extra computational power. The time and memory consumption of these alternate models may vary.<sup class="reference" id="cite_ref-2"><a href="#cite_note-2">[2]</a></sup> What all these models have in common is that the machines operate <a href="/wiki/Deterministic_algorithm" title="Deterministic algorithm">deterministically</a>.\n</p>, <p>However, some computational problems are easier to analyze in terms of more unusual resources. For example, a non-deterministic Turing machine is a computational model that is allowed to branch out to check many different possibilities at once. The non-deterministic Turing machine has very little to do with how we physically want to compute algorithms, but its branching exactly captures many of the mathematical models we want to analyze, so that <a class="mw-redirect" href="/wiki/Non-deterministic_time" title="Non-deterministic time">non-deterministic time</a> is a very important resource in analyzing computational problems.\n</p>, <p>For a precise definition of what it means to solve a problem using a given amount of time and space, a computational model such as the <a class="mw-redirect" href="/wiki/Deterministic_Turing_machine" title="Deterministic Turing machine">deterministic Turing machine</a> is used. The <i>time required</i> by a deterministic Turing machine <i>M</i> on input <i>x</i> is the total number of state transitions, or steps, the machine makes before it halts and outputs the answer ("yes" or "no"). A Turing machine <i>M</i> is said to operate within time <i>f</i>(<i>n</i>), if the time required by <i>M</i> on each input of length <i>n</i> is at most <i>f</i>(<i>n</i>). A decision problem <i>A</i> can be solved in time <i>f</i>(<i>n</i>) if there exists a Turing machine operating in time <i>f</i>(<i>n</i>) that solves the problem. Since complexity theory is interested in classifying problems based on their difficulty, one defines sets of problems based on some criteria. For instance, the set of problems solvable within time <i>f</i>(<i>n</i>) on a deterministic Turing machine is then denoted by <a href="/wiki/DTIME" title="DTIME">DTIME</a>(<i>f</i>(<i>n</i>)).\n</p>, <p>Analogous definitions can be made for space requirements. Although time and space are the most well-known complexity resources, any <a href="/wiki/Complexity" title="Complexity">complexity measure</a> can be viewed as a computational resource. Complexity measures are very generally defined by the <a class="mw-redirect" href="/wiki/Blum_complexity_axioms" title="Blum complexity axioms">Blum complexity axioms</a>. Other complexity measures used in complexity theory include <a href="/wiki/Communication_complexity" title="Communication complexity">communication complexity</a>, <a href="/wiki/Circuit_complexity" title="Circuit complexity">circuit complexity</a>, and <a class="mw-redirect" href="/wiki/Decision_tree_complexity" title="Decision tree complexity">decision tree complexity</a>.\n</p>, <p>The complexity of an algorithm is often expressed using <a href="/wiki/Big_O_notation" title="Big O notation">big O notation</a>.\n</p>, <p>The <a href="/wiki/Best,_worst_and_average_case" title="Best, worst and average case">best, worst and average case</a> complexity refer to three different ways of measuring the time complexity (or any other complexity measure) of different inputs of the same size. Since some inputs of size <i>n</i> may be faster to solve than others, we define the following complexities:\n</p>, <p>The order from cheap to costly is: Best, average (of <a href="/wiki/Discrete_uniform_distribution" title="Discrete uniform distribution">discrete uniform distribution</a>), amortized, worst.\n</p>, <p>For example, consider the deterministic sorting algorithm <a href="/wiki/Quicksort" title="Quicksort">quicksort</a>. This solves the problem of sorting a list of integers that is given as the input. The worst-case is when the input is sorted or sorted in reverse order, and the algorithm takes time <a href="/wiki/Big_O_notation" title="Big O notation">O</a>(<i>n</i><sup>2</sup>) for this case. If we assume that all possible permutations of the input list are equally likely, the average time taken for sorting is O(<i>n</i> log <i>n</i>). The best case occurs when each pivoting divides the list in half, also needing O(<i>n</i> log <i>n</i>) time.\n</p>, <p>To classify the computation time (or similar resources, such as space consumption), one is interested in proving upper and lower bounds on the maximum amount of time required by the most efficient algorithm solving a given problem. The complexity of an algorithm is usually taken to be its worst-case complexity, unless specified otherwise. Analyzing a particular algorithm falls under the field of <a href="/wiki/Analysis_of_algorithms" title="Analysis of algorithms">analysis of algorithms</a>. To show an upper bound <i>T</i>(<i>n</i>) on the time complexity of a problem, one needs to show only that there is a particular algorithm with running time at most <i>T</i>(<i>n</i>). However, proving lower bounds is much more difficult, since lower bounds make a statement about all possible algorithms that solve a given problem. The phrase "all possible algorithms" includes not just the algorithms known today, but any algorithm that might be discovered in the future. To show a lower bound of <i>T</i>(<i>n</i>) for a problem requires showing that no algorithm can have time complexity lower than <i>T</i>(<i>n</i>).\n</p>, <p>Upper and lower bounds are usually stated using the <a href="/wiki/Big_O_notation" title="Big O notation">big O notation</a>, which hides constant factors and smaller terms. This makes the bounds independent of the specific details of the computational model used. For instance, if <i>T</i>(<i>n</i>) = 7<i>n</i><sup>2</sup> + 15<i>n</i> + 40, in big O notation one would write <i>T</i>(<i>n</i>) = O(<i>n</i><sup>2</sup>).\n</p>, <p>A <b>complexity class</b> is a set of problems of related complexity. Simpler complexity classes are defined by the following factors:\n</p>, <p>Some complexity classes have complicated definitions that do not fit into this framework. Thus, a typical complexity class has a definition like the following:\n</p>, <p>But bounding the computation time above by some concrete function <i>f</i>(<i>n</i>) often yields complexity classes that depend on the chosen machine model. For instance, the language {<i>xx</i> | <i>x</i> is any binary string} can be solved in <a class="mw-redirect" href="/wiki/Linear_time" title="Linear time">linear time</a> on a multi-tape Turing machine, but necessarily requires quadratic time in the model of single-tape Turing machines. If we allow polynomial variations in running time, <a href="/wiki/Cobham%27s_thesis" title="Cobham's thesis">Cobham-Edmonds thesis</a> states that "the time complexities in any two reasonable and general models of computation are polynomially related" (<a href="#CITEREFGoldreich2008">Goldreich 2008</a>, Chapter 1.2). This forms the basis for the complexity class <a href="/wiki/P_(complexity)" title="P (complexity)">P</a>, which is the set of decision problems solvable by a deterministic Turing machine within polynomial time. The corresponding set of function problems is <a href="/wiki/FP_(complexity)" title="FP (complexity)">FP</a>.\n</p>, <p>Many important complexity classes can be defined by bounding the time or space used by the algorithm. Some important complexity classes of decision problems defined in this manner are the following:\n</p>, <p>The logarithmic-space classes (necessarily) do not take into account the space needed to represent the problem.\n</p>, <p>It turns out that PSPACE = NPSPACE and EXPSPACE = NEXPSPACE by <a href="/wiki/Savitch%27s_theorem" title="Savitch's theorem">Savitch\'s theorem</a>.\n</p>, <p>Other important complexity classes include <a href="/wiki/BPP_(complexity)" title="BPP (complexity)">BPP</a>, <a href="/wiki/ZPP_(complexity)" title="ZPP (complexity)">ZPP</a> and <a href="/wiki/RP_(complexity)" title="RP (complexity)">RP</a>, which are defined using <a href="/wiki/Probabilistic_Turing_machine" title="Probabilistic Turing machine">probabilistic Turing machines</a>; <a href="/wiki/AC_(complexity)" title="AC (complexity)">AC</a> and <a href="/wiki/NC_(complexity)" title="NC (complexity)">NC</a>, which are defined using Boolean circuits; and <a href="/wiki/BQP" title="BQP">BQP</a> and <a href="/wiki/QMA" title="QMA">QMA</a>, which are defined using quantum Turing machines. <a class="mw-redirect" href="/wiki/Sharp-P" title="Sharp-P">#P</a> is an important complexity class of counting problems (not decision problems). Classes like <a href="/wiki/IP_(complexity)" title="IP (complexity)">IP</a> and <a class="mw-redirect" href="/wiki/AM_(complexity)" title="AM (complexity)">AM</a> are defined using <a href="/wiki/Interactive_proof_system" title="Interactive proof system">Interactive proof systems</a>. <a href="/wiki/ALL_(complexity)" title="ALL (complexity)">ALL</a> is the class of all decision problems.\n</p>, <p>For the complexity classes defined in this way, it is desirable to prove that relaxing the requirements on (say) computation time indeed defines a bigger set of problems. In particular, although DTIME(<i>n</i>) is contained in DTIME(<i>n</i><sup>2</sup>), it would be interesting to know if the inclusion is strict. For time and space requirements, the answer to such questions is given by the time and space hierarchy theorems respectively. They are called hierarchy theorems because they induce a proper hierarchy on the classes defined by constraining the respective resources. Thus there are pairs of complexity classes such that one is properly included in the other. Having deduced such proper set inclusions, we can proceed to make quantitative statements about how much more additional time or space is needed in order to increase the number of problems that can be solved.\n</p>, <p>More precisely, the <a href="/wiki/Time_hierarchy_theorem" title="Time hierarchy theorem">time hierarchy theorem</a> states that\n</p>, <p>The <a href="/wiki/Space_hierarchy_theorem" title="Space hierarchy theorem">space hierarchy theorem</a> states that\n</p>, <p>The time and space hierarchy theorems form the basis for most separation results of complexity classes. For instance, the time hierarchy theorem tells us that P is strictly contained in EXPTIME, and the space hierarchy theorem tells us that L is strictly contained in PSPACE.\n</p>, <p>Many complexity classes are defined using the concept of a reduction. A reduction is a transformation of one problem into another problem. It captures the informal notion of a problem being at most as difficult as another problem. For instance, if a problem <i>X</i> can be solved using an algorithm for <i>Y</i>, <i>X</i> is no more difficult than <i>Y</i>, and we say that <i>X</i> <i>reduces</i> to <i>Y</i>. There are many different types of reductions, based on the method of reduction, such as Cook reductions, Karp reductions and Levin reductions, and the bound on the complexity of reductions, such as <a href="/wiki/Polynomial-time_reduction" title="Polynomial-time reduction">polynomial-time reductions</a> or <a href="/wiki/Log-space_reduction" title="Log-space reduction">log-space reductions</a>.\n</p>, <p>The most commonly used reduction is a polynomial-time reduction. This means that the reduction process takes polynomial time. For example, the problem of squaring an integer can be reduced to the problem of multiplying two integers. This means an algorithm for multiplying two integers can be used to square an integer. Indeed, this can be done by giving the same input to both inputs of the multiplication algorithm. Thus we see that squaring is not more difficult than multiplication, since squaring can be reduced to multiplication.\n</p>, <p>This motivates the concept of a problem being hard for a complexity class. A problem <i>X</i> is <i>hard</i> for a class of problems <i>C</i> if every problem in <i>C</i> can be reduced to <i>X</i>. Thus no problem in <i>C</i> is harder than <i>X</i>, since an algorithm for <i>X</i> allows us to solve any problem in <i>C</i>. The notion of hard problems depends on the type of reduction being used. For complexity classes larger than P, polynomial-time reductions are commonly used. In particular, the set of problems that are hard for NP is the set of <a class="mw-redirect" href="/wiki/NP-hard" title="NP-hard">NP-hard</a> problems.\n</p>, <p>If a problem <i>X</i> is in <i>C</i> and hard for <i>C</i>, then <i>X</i> is said to be <i><a href="/wiki/Complete_(complexity)" title="Complete (complexity)">complete</a></i> for <i>C</i>. This means that <i>X</i> is the hardest problem in <i>C</i>. (Since many problems could be equally hard, one might say that <i>X</i> is one of the hardest problems in <i>C</i>.) Thus the class of <a class="mw-redirect" href="/wiki/NP-complete" title="NP-complete">NP-complete</a> problems contains the most difficult problems in NP, in the sense that they are the ones most likely not to be in P. Because the problem P = NP is not solved, being able to reduce a known NP-complete problem, \xce\xa0<sub>2</sub>, to another problem, \xce\xa0<sub>1</sub>, would indicate that there is no known polynomial-time solution for \xce\xa0<sub>1</sub>. This is because a polynomial-time solution to \xce\xa0<sub>1</sub> would yield a polynomial-time solution to \xce\xa0<sub>2</sub>. Similarly, because all NP problems can be reduced to the set, finding an <a class="mw-redirect" href="/wiki/NP-complete" title="NP-complete">NP-complete</a> problem that can be solved in polynomial time would mean that P = NP.<sup class="reference" id="cite_ref-Sipser2006_3-0"><a href="#cite_note-Sipser2006-3">[3]</a></sup>\n</p>, <p>The complexity class P is often seen as a mathematical abstraction modeling those computational tasks that admit an efficient algorithm. This hypothesis is called the <a class="mw-redirect" href="/wiki/Cobham%E2%80%93Edmonds_thesis" title="Cobham\xe2\x80\x93Edmonds thesis">Cobham\xe2\x80\x93Edmonds thesis</a>. The complexity class <a href="/wiki/NP_(complexity)" title="NP (complexity)">NP</a>, on the other hand, contains many problems that people would like to solve efficiently, but for which no efficient algorithm is known, such as the <a href="/wiki/Boolean_satisfiability_problem" title="Boolean satisfiability problem">Boolean satisfiability problem</a>, the <a href="/wiki/Hamiltonian_path_problem" title="Hamiltonian path problem">Hamiltonian path problem</a> and the <a class="mw-redirect" href="/wiki/Vertex_cover_problem" title="Vertex cover problem">vertex cover problem</a>. Since deterministic Turing machines are special non-deterministic Turing machines, it is easily observed that each problem in P is also member of the class NP.\n</p>, <p>The question of whether P equals NP is one of the most important open questions in theoretical computer science because of the wide implications of a solution.<sup class="reference" id="cite_ref-Sipser2006_3-1"><a href="#cite_note-Sipser2006-3">[3]</a></sup> If the answer is yes, many important problems can be shown to have more efficient solutions. These include various types of <a href="/wiki/Integer_programming" title="Integer programming">integer programming</a> problems in <a href="/wiki/Operations_research" title="Operations research">operations research</a>, many problems in <a href="/wiki/Logistics" title="Logistics">logistics</a>, <a href="/wiki/Protein_structure_prediction" title="Protein structure prediction">protein structure prediction</a> in <a href="/wiki/Biology" title="Biology">biology</a>,<sup class="reference" id="cite_ref-5"><a href="#cite_note-5">[5]</a></sup> and the ability to find formal proofs of <a href="/wiki/Pure_mathematics" title="Pure mathematics">pure mathematics</a> theorems.<sup class="reference" id="cite_ref-6"><a href="#cite_note-6">[6]</a></sup> The P versus NP problem is one of the <a href="/wiki/Millennium_Prize_Problems" title="Millennium Prize Problems">Millennium Prize Problems</a> proposed by the <a href="/wiki/Clay_Mathematics_Institute" title="Clay Mathematics Institute">Clay Mathematics Institute</a>. There is a US$1,000,000 prize for resolving the problem.<sup class="reference" id="cite_ref-7"><a href="#cite_note-7">[7]</a></sup>\n</p>, <p>It was shown by Ladner that if <b>P</b> \xe2\x89\xa0 <b>NP</b> then there exist problems in <b>NP</b> that are neither in <b>P</b> nor <b>NP-complete</b>.<sup class="reference" id="cite_ref-Ladner75_4-1"><a href="#cite_note-Ladner75-4">[4]</a></sup> Such problems are called <a href="/wiki/NP-intermediate" title="NP-intermediate">NP-intermediate</a> problems. The <a href="/wiki/Graph_isomorphism_problem" title="Graph isomorphism problem">graph isomorphism problem</a>, the <a class="mw-redirect" href="/wiki/Discrete_logarithm_problem" title="Discrete logarithm problem">discrete logarithm problem</a> and the <a class="mw-redirect" href="/wiki/Integer_factorization_problem" title="Integer factorization problem">integer factorization problem</a> are examples of problems believed to be NP-intermediate. They are some of the very few NP problems not known to be in <b>P</b> or to be <b>NP-complete</b>.\n</p>, <p>The <a href="/wiki/Graph_isomorphism_problem" title="Graph isomorphism problem">graph isomorphism problem</a> is the computational problem of determining whether two finite <a href="/wiki/Graph_(discrete_mathematics)" title="Graph (discrete mathematics)">graphs</a> are <a href="/wiki/Graph_isomorphism" title="Graph isomorphism">isomorphic</a>. An important unsolved problem in complexity theory is whether the graph isomorphism problem is in <b>P</b>, <b>NP-complete</b>, or NP-intermediate. The answer is not known, but it is believed that the problem is at least not NP-complete.<sup class="reference" id="cite_ref-AK06_8-0"><a href="#cite_note-AK06-8">[8]</a></sup> If graph isomorphism is NP-complete, the <a class="mw-redirect" href="/wiki/Polynomial_time_hierarchy" title="Polynomial time hierarchy">polynomial time hierarchy</a> collapses to its second level.<sup class="reference" id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup> Since it is widely believed that the polynomial hierarchy does not collapse to any finite level, it is believed that graph isomorphism is not NP-complete. The best algorithm for this problem, due to <a href="/wiki/L%C3%A1szl%C3%B3_Babai" title="L\xc3\xa1szl\xc3\xb3 Babai">L\xc3\xa1szl\xc3\xb3 Babai</a> and <a class="mw-redirect" href="/wiki/Eugene_Luks" title="Eugene Luks">Eugene Luks</a> has run time <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle O(2^{\\sqrt {n\\log n}})}" xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>O</mi>\n        <mo stretchy="false">(</mo>\n        <msup>\n          <mn>2</mn>\n          <mrow class="MJX-TeXAtom-ORD">\n            <msqrt>\n              <mi>n</mi>\n              <mi>log</mi>\n              <mo>⁡<!-- \xe2\x81\xa1 --></mo>\n              <mi>n</mi>\n            </msqrt>\n          </mrow>\n        </msup>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle O(2^{\\sqrt {n\\log n}})}</annotation>\n  </semantics>\n</math></span><img alt="{\\displaystyle O(2^{\\sqrt {n\\log n}})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d4186036dc1b46116f1fbd2d2ca92d9c4bf3d040" style="vertical-align: -0.838ex; width:11.468ex; height:3.509ex;"/></span> for graphs with <i>n</i> vertices, although some recent work by Babai offers some potentially new perspectives on this.<sup class="reference" id="cite_ref-10"><a href="#cite_note-10">[10]</a></sup>\n</p>, <p>The <a class="mw-redirect" href="/wiki/Integer_factorization_problem" title="Integer factorization problem">integer factorization problem</a> is the computational problem of determining the <a class="mw-redirect" href="/wiki/Prime_factorization" title="Prime factorization">prime factorization</a> of a given integer. Phrased as a decision problem, it is the problem of deciding whether the input has a prime factor less than <i>k</i>. No efficient integer factorization algorithm is known, and this fact forms the basis of several modern cryptographic systems, such as the <a class="mw-redirect" href="/wiki/RSA_(algorithm)" title="RSA (algorithm)">RSA</a> algorithm. The integer factorization problem is in <b>NP</b> and in <b>co-NP</b> (and even in UP and co-UP<sup class="reference" id="cite_ref-11"><a href="#cite_note-11">[11]</a></sup>). If the problem is <b>NP-complete</b>, the polynomial time hierarchy will collapse to its first level (i.e., <b>NP</b> will equal <b>co-NP</b>). The best known algorithm for integer factorization is the <a href="/wiki/General_number_field_sieve" title="General number field sieve">general number field sieve</a>, which takes time <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle O(e^{\\left({\\frac {64}{9}}\\right)^{1/3}(\\log n)^{1/3}(\\log \\log n)^{2/3}})}" xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>O</mi>\n        <mo stretchy="false">(</mo>\n        <msup>\n          <mi>e</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <msup>\n              <mrow>\n                <mo>(</mo>\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mfrac>\n                    <mn>64</mn>\n                    <mn>9</mn>\n                  </mfrac>\n                </mrow>\n                <mo>)</mo>\n              </mrow>\n              <mrow class="MJX-TeXAtom-ORD">\n                <mn>1</mn>\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mo>/</mo>\n                </mrow>\n                <mn>3</mn>\n              </mrow>\n            </msup>\n            <mo stretchy="false">(</mo>\n            <mi>log</mi>\n            <mo>⁡<!-- \xe2\x81\xa1 --></mo>\n            <mi>n</mi>\n            <msup>\n              <mo stretchy="false">)</mo>\n              <mrow class="MJX-TeXAtom-ORD">\n                <mn>1</mn>\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mo>/</mo>\n                </mrow>\n                <mn>3</mn>\n              </mrow>\n            </msup>\n            <mo stretchy="false">(</mo>\n            <mi>log</mi>\n            <mo>⁡<!-- \xe2\x81\xa1 --></mo>\n            <mi>log</mi>\n            <mo>⁡<!-- \xe2\x81\xa1 --></mo>\n            <mi>n</mi>\n            <msup>\n              <mo stretchy="false">)</mo>\n              <mrow class="MJX-TeXAtom-ORD">\n                <mn>2</mn>\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mo>/</mo>\n                </mrow>\n                <mn>3</mn>\n              </mrow>\n            </msup>\n          </mrow>\n        </msup>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle O(e^{\\left({\\frac {64}{9}}\\right)^{1/3}(\\log n)^{1/3}(\\log \\log n)^{2/3}})}</annotation>\n  </semantics>\n</math></span><img alt="{\\displaystyle O(e^{\\left({\\frac {64}{9}}\\right)^{1/3}(\\log n)^{1/3}(\\log \\log n)^{2/3}})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/174a23aeebf125d4fed8534465afe997526d8f82" style="vertical-align: -0.838ex; width:27.527ex; height:4.843ex;"/></span><sup class="reference" id="cite_ref-12"><a href="#cite_note-12">[12]</a></sup> to factor an integer <i>n</i>. However, the best known <a href="/wiki/Quantum_algorithm" title="Quantum algorithm">quantum algorithm</a> for this problem, <a href="/wiki/Shor%27s_algorithm" title="Shor's algorithm">Shor\'s algorithm</a>, does run in polynomial time. Unfortunately, this fact doesn\'t say much about where the problem lies with respect to non-quantum complexity classes.\n</p>, <p>Many known complexity classes are suspected to be unequal, but this has not been proved. For instance <b>P</b> \xe2\x8a\x86 <b>NP</b> \xe2\x8a\x86 <b><a href="/wiki/PP_(complexity)" title="PP (complexity)">PP</a></b> \xe2\x8a\x86 <b>PSPACE</b>, but it is possible that <b>P</b> = <b>PSPACE</b>. If <b>P</b> is not equal to <b>NP</b>, then <b>P</b> is not equal to <b>PSPACE</b> either. Since there are many known complexity classes between <b>P</b> and <b>PSPACE</b>, such as <b>RP</b>, <b>BPP</b>, <b>PP</b>, <b>BQP</b>, <b>MA</b>, <b>PH</b>, etc., it is possible that all these complexity classes collapse to one class. Proving that any of these classes are unequal would be a major breakthrough in complexity theory.\n</p>, <p>Along the same lines, <b><a href="/wiki/Co-NP" title="Co-NP">co-NP</a></b> is the class containing the <a href="/wiki/Complement_(complexity)" title="Complement (complexity)">complement</a> problems (i.e. problems with the <i>yes</i>/<i>no</i> answers reversed) of <b>NP</b> problems. It is believed<sup class="reference" id="cite_ref-13"><a href="#cite_note-13">[13]</a></sup> that <b>NP</b> is not equal to <b>co-NP</b>; however, it has not yet been proven. It is clear that if these two complexity classes are not equal then <b>P</b> is not equal to <b>NP</b>, since if <b>P</b>=<b>NP</b> we would also have <b>P</b>=<b>co-NP</b>, since problems in <b>NP</b> are dual to those in <b>co-NP</b>.\n</p>, <p>Similarly, it is not known if <b>L</b> (the set of all problems that can be solved in logarithmic space) is strictly contained in <b>P</b> or equal to <b>P</b>. Again, there are many complexity classes between the two, such as <b>NL</b> and <b>NC</b>, and it is not known if they are distinct or equal classes.\n</p>, <p>It is suspected that <b>P</b> and <b>BPP</b> are equal. However, it is currently open if <b>BPP</b> = <b>NEXP</b>.\n</p>, <p>A problem that can be solved in theory (e.g. given large but finite resources, especially time), but for which in practice <i>any</i> solution takes too many resources to be useful, is known as an <b><span id="intractable_problem">intractable problem</span></b>.<sup class="reference" id="cite_ref-14"><a href="#cite_note-14">[14]</a></sup> Conversely, a problem that can be solved in practice is called a <b><span id="tractable_problem">tractable problem</span></b>, literally "a problem that can be handled". The term <i><a class="extiw" href="https://en.wiktionary.org/wiki/infeasible" title="wikt:infeasible">infeasible</a></i> (literally "cannot be done") is sometimes used interchangeably with <i><a class="extiw" href="https://en.wiktionary.org/wiki/intractable" title="wikt:intractable">intractable</a></i>,<sup class="reference" id="cite_ref-15"><a href="#cite_note-15">[15]</a></sup> though this risks confusion with a <a class="mw-redirect" href="/wiki/Feasible_solution" title="Feasible solution">feasible solution</a> in <a href="/wiki/Mathematical_optimization" title="Mathematical optimization">mathematical optimization</a>.<sup class="reference" id="cite_ref-16"><a href="#cite_note-16">[16]</a></sup>\n</p>, <p>Tractable problems are frequently identified with problems that have polynomial-time solutions (<b>P</b>, <b>PTIME</b>); this is known as the <a class="mw-redirect" href="/wiki/Cobham%E2%80%93Edmonds_thesis" title="Cobham\xe2\x80\x93Edmonds thesis">Cobham\xe2\x80\x93Edmonds thesis</a>. Problems that are known to be intractable in this sense include those that are <a href="/wiki/EXPTIME" title="EXPTIME">EXPTIME</a>-hard. If NP is not the same as P, then <a class="mw-redirect" href="/wiki/NP-hard" title="NP-hard">NP-hard</a> problems are also intractable in this sense.\n</p>, <p>However, this identification is inexact: a polynomial-time solution with large exponent or large constant term grows quickly, and may be impractical for practical size problems; conversely, an exponential-time solution that grows slowly may be practical on realistic input, or a solution that takes a long time in the worst case may take a short time in most cases or the average case, and thus still be practical. Saying that a problem is not in P does not imply that all large cases of the problem are hard or even that most of them are. For example, the decision problem in <a href="/wiki/Presburger_arithmetic" title="Presburger arithmetic">Presburger arithmetic</a> has been shown not to be in P, yet algorithms have been written that solve the problem in reasonable times in most cases. Similarly, algorithms can solve the NP-complete <a href="/wiki/Knapsack_problem" title="Knapsack problem">knapsack problem</a> over a wide range of sizes in less than quadratic time and <a class="mw-redirect" href="/wiki/SAT_solver" title="SAT solver">SAT solvers</a> routinely handle large instances of the NP-complete <a href="/wiki/Boolean_satisfiability_problem" title="Boolean satisfiability problem">Boolean satisfiability problem</a>.\n</p>, <p>To see why exponential-time algorithms are generally unusable in practice, consider a program that makes 2<sup><i>n</i></sup> operations before halting. For small <i>n</i>, say 100, and assuming for the sake of example that the computer does 10<sup>12</sup> operations each second, the program would run for about 4 \xc3\x97 10<sup>10</sup> years, which is the same order of magnitude as the <a href="/wiki/Age_of_the_universe" title="Age of the universe">age of the universe</a>. Even with a much faster computer, the program would only be useful for very small instances and in that sense the intractability of a problem is somewhat independent of technological progress. However, an exponential-time algorithm that takes 1.0001<sup><i>n</i></sup> operations is practical until <i>n</i> gets relatively large.\n</p>, <p>Similarly, a polynomial time algorithm is not always practical. If its running time is, say, <i>n</i><sup>15</sup>, it is unreasonable to consider it efficient and it is still useless except on small instances. Indeed, in practice even <i>n</i><sup>3</sup> or <i>n</i><sup>2</sup> algorithms are often impractical on realistic sizes of problems.\n</p>, <p>An early example of algorithm complexity analysis is the running time analysis of the <a href="/wiki/Euclidean_algorithm" title="Euclidean algorithm">Euclidean algorithm</a> done by <a href="/wiki/Gabriel_Lam%C3%A9" title="Gabriel Lam\xc3\xa9">Gabriel Lam\xc3\xa9</a> in 1844.\n</p>, <p>Before the actual research explicitly devoted to the complexity of algorithmic problems started off, numerous foundations were laid out by various researchers. Most influential among these was the definition of Turing machines by <a href="/wiki/Alan_Turing" title="Alan Turing">Alan Turing</a> in 1936, which turned out to be a very robust and flexible simplification of a computer.\n</p>, <p>The beginning of systematic studies in computational complexity is attributed to the seminal 1965 paper "On the Computational Complexity of Algorithms" by <a href="/wiki/Juris_Hartmanis" title="Juris Hartmanis">Juris Hartmanis</a> and <a href="/wiki/Richard_E._Stearns" title="Richard E. Stearns">Richard E. Stearns</a>, which laid out the definitions of <a href="/wiki/Time_complexity" title="Time complexity">time complexity</a> and <a class="mw-redirect" href="/wiki/Space_complexity" title="Space complexity">space complexity</a>, and proved the hierarchy theorems.<sup class="reference" id="cite_ref-Fortnow_2003_17-0"><a href="#cite_note-Fortnow_2003-17">[17]</a></sup> In addition, in 1965 <a href="/wiki/Jack_Edmonds" title="Jack Edmonds">Edmonds</a> suggested to consider a "good" algorithm to be one with running time bounded by a polynomial of the input size.<sup class="reference" id="cite_ref-18"><a href="#cite_note-18">[18]</a></sup>\n</p>, <p>Earlier papers studying problems solvable by Turing machines with specific bounded resources include<sup class="reference" id="cite_ref-Fortnow_2003_17-1"><a href="#cite_note-Fortnow_2003-17">[17]</a></sup> <a href="/wiki/John_Myhill" title="John Myhill">John Myhill</a>\'s definition of <a class="mw-redirect" href="/wiki/Linear_bounded_automata" title="Linear bounded automata">linear bounded automata</a> (Myhill 1960), <a href="/wiki/Raymond_Smullyan" title="Raymond Smullyan">Raymond Smullyan</a>\'s study of rudimentary sets (1961), as well as <a href="/wiki/Hisao_Yamada" title="Hisao Yamada">Hisao Yamada</a>\'s paper<sup class="reference" id="cite_ref-19"><a href="#cite_note-19">[19]</a></sup> on real-time computations (1962). Somewhat earlier, <a href="/wiki/Boris_Trakhtenbrot" title="Boris Trakhtenbrot">Boris Trakhtenbrot</a> (1956), a pioneer in the field from the USSR, studied another specific complexity measure.<sup class="reference" id="cite_ref-20"><a href="#cite_note-20">[20]</a></sup> As he remembers:\n</p>, <p>However, [my] initial interest [in automata theory] was increasingly set aside in favor of computational complexity, an exciting fusion of combinatorial methods, inherited from <a class="mw-redirect" href="/wiki/Switching_theory" title="Switching theory">switching theory</a>, with the conceptual arsenal of the theory of algorithms. These ideas had occurred to me earlier in 1955 when I coined the term "signalizing function", which is nowadays commonly known as "complexity measure".<sup class="reference" id="cite_ref-21"><a href="#cite_note-21">[21]</a></sup>\n</p>, <p>In 1967, <a href="/wiki/Manuel_Blum" title="Manuel Blum">Manuel Blum</a> formulated a set of <a href="/wiki/Axiom" title="Axiom">axioms</a> (now known as <a href="/wiki/Blum_axioms" title="Blum axioms">Blum axioms</a>) specifying desirable properties of complexity measures on the set of computable functions and proved an important result, the so-called <a href="/wiki/Blum%27s_speedup_theorem" title="Blum's speedup theorem">speed-up theorem</a>. The field began to flourish in 1971 when the <a href="/wiki/Stephen_Cook" title="Stephen Cook">Stephen Cook</a> and <a href="/wiki/Leonid_Levin" title="Leonid Levin">Leonid Levin</a> <a href="/wiki/Cook%E2%80%93Levin_theorem" title="Cook\xe2\x80\x93Levin theorem">proved</a> the existence of practically relevant problems that are <a class="mw-redirect" href="/wiki/NP-complete" title="NP-complete">NP-complete</a>. In 1972, <a class="mw-redirect" href="/wiki/Richard_Karp" title="Richard Karp">Richard Karp</a> took this idea a leap forward with his landmark paper, "Reducibility Among Combinatorial Problems", in which he showed that 21 diverse <a href="/wiki/Combinatorics" title="Combinatorics">combinatorial</a> and <a href="/wiki/Graph_theory" title="Graph theory">graph theoretical</a> problems, each infamous for its computational intractability, are NP-complete.<sup class="reference" id="cite_ref-22"><a href="#cite_note-22">[22]</a></sup>\n</p>, <p>In the 1980s, much work was done on the average difficulty of solving NP-complete problems\xe2\x80\x94both exactly and approximately. At that time, computational complexity theory was at its height, and it was widely believed that if a problem turned out to be NP-complete, then there was little chance of being able to work with the problem in a practical situation. However, it became increasingly clear that this is not always the case, and some authors claimed that general asymptotic results are often unimportant for typical problems arising in practice.<sup class="reference" id="cite_ref-23"><a href="#cite_note-23">[23]</a></sup>\n</p>]