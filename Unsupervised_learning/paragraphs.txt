Unsupervisedlearningisabranchofmachinelearningthatlearnsfromtestdatathathasnotbeenlabeled,classifiedorcategorized.Insteadofrespondingtofeedback,unsupervisedlearningidentifiescommonalitiesinthedataandreactsbasedonthepresenceorabsenceofsuchcommonalitiesineachnewpieceofdata.Alternativesincludesupervisedlearningandreinforcementlearning.\nAcentralapplicationofunsupervisedlearningisinthefieldofdensityestimationinstatistics,[1]thoughunsupervisedlearningencompassesmanyotherdomainsinvolvingsummarizingandexplainingdatafeatures.Itcouldbecontrastedwithsupervisedlearningbysayingthatwhereassupervisedlearningintendstoinferaconditionalprobabilitydistribution\n\n\n\n\np\n\nX\n\n\n(\nx\n\n\n|\n\n\ny\n)\n\n\n{\\textstylep_{X}(x\\,|\\,y)}\n\nconditionedonthelabel\n\n\n\ny\n\n\n{\\textstyley}\n\nofinputdata;unsupervisedlearningintendstoinferanaprioriprobabilitydistribution\n\n\n\n\np\n\nX\n\n\n(\nx\n)\n\n\n{\\textstylep_{X}(x)}\n\n.\nComparedtosupervisedlearningwheretrainingdataislabeledwiththeappropriateclassifications,modelsusingunsupervisedlearningmustlearnrelationshipsbetweenelementsinadatasetandclassifytherawdatawithout"help."Thishuntforrelationshipscantakemanydifferentalgorithmicforms,Â butallmodelshavethesamegoalofmimickinghumanlogicbysearchingforindirecthiddenstructures,patternsorfeaturestoanalyzenewdata.[2]\nSomeofthemostcommonalgorithmsusedinunsupervisedlearninginclude:\nTheclassicalexampleofunsupervisedlearninginthestudyofneuralnetworksisDonaldHebb\'sprinciple,thatis,neuronsthatfiretogetherwiretogether.InHebbianlearning,theconnectionisreinforcedirrespectiveofanerror,butisexclusivelyafunctionofthecoincidencebetweenactionpotentialsbetweenthetwoneurons.Asimilarversionthatmodifiessynapticweightstakesintoaccountthetimebetweentheactionpotentials(spike-timing-dependentplasticityorSTDP).HebbianLearninghasbeenhypothesizedtounderliearangeofcognitivefunctions,suchaspatternrecognitionandexperientiallearning.\nAmongneuralnetworkmodels,theself-organizingmap(SOM)andadaptiveresonancetheory(ART)arecommonlyusedinunsupervisedlearningalgorithms.TheSOMisatopographicorganizationinwhichnearbylocationsinthemaprepresentinputswithsimilarproperties.TheARTmodelallowsthenumberofclusterstovarywithproblemsizeandletstheusercontrolthedegreeofsimilaritybetweenmembersofthesameclustersbymeansofauser-definedconstantcalledthevigilanceparameter.ARTnetworksareusedformanypatternrecognitiontasks,suchasautomatictargetrecognitionandseismicsignalprocessing.[4]\nOneofthestatisticalapproachesforunsupervisedlearningisthemethodofmoments.Inthemethodofmoments,theunknownparameters(ofinterest)inthemodelarerelatedtothemomentsofoneormorerandomvariables,andthus,theseunknownparameterscanbeestimatedgiventhemoments.Themomentsareusuallyestimatedfromsamplesempirically.Thebasicmomentsarefirstandsecondordermoments.Forarandomvector,thefirstordermomentisthemeanvector,andthesecondordermomentisthecovariancematrix(whenthemeaniszero).Higherordermomentsareusuallyrepresentedusingtensorswhicharethegeneralizationofmatricestohigherordersasmulti-dimensionalarrays.\nInparticular,themethodofmomentsisshowntobeeffectiveinlearningtheparametersoflatentvariablemodels.[5]\nLatentvariablemodelsarestatisticalmodelswhereinadditiontotheobservedvariables,asetoflatentvariablesalsoexistswhichisnotobserved.Ahighlypracticalexampleoflatentvariablemodelsinmachinelearningisthetopicmodelingwhichisastatisticalmodelforgeneratingthewords(observedvariables)inthedocumentbasedonthetopic(latentvariable)ofthedocument.Inthetopicmodeling,thewordsinthedocumentaregeneratedaccordingtodifferentstatisticalparameterswhenthetopicofthedocumentischanged.Itisshownthatmethodofmoments(tensordecompositiontechniques)consistentlyrecovertheparametersofalargeclassoflatentvariablemodelsundersomeassumptions.[5]\nTheExpectation\xe2\x80\x93maximizationalgorithm(EM)isalsooneofthemostpracticalmethodsforlearninglatentvariablemodels.However,itcangetstuckinlocaloptima,anditisnotguaranteedthatthealgorithmwillconvergetothetrueunknownparametersofthemodel.Incontrast,forthemethodofmoments,theglobalconvergenceisguaranteedundersomeconditions.[5]\n