[<p>In <a href="/wiki/Computing" title="Computing">computing</a>, <b>hardware acceleration</b> is the use of <a href="/wiki/Computer_hardware" title="Computer hardware">computer hardware</a> specially made to perform some functions more efficiently than is possible in <a href="/wiki/Software" title="Software">software</a> running on a general-purpose <abbr title="central processing unit"><a class="mw-redirect" href="/wiki/CPU" title="CPU">CPU</a></abbr>.  Any <a href="/wiki/Function_(mathematics)" title="Function (mathematics)">transformation</a> of <a href="/wiki/Data_(computing)" title="Data (computing)">data</a> or <a href="/wiki/Subroutine" title="Subroutine">routine</a> that can be <a href="/wiki/Computable_function" title="Computable function">computed</a>, can be calculated purely in <a href="/wiki/Software" title="Software">software</a> running on a generic CPU, purely in custom-made <a href="/wiki/Computer_hardware" title="Computer hardware">hardware</a>, or in some mix of both.  An operation can be computed faster in <a href="/wiki/Application-specific_integrated_circuit" title="Application-specific integrated circuit">application-specific hardware</a> designed or <a href="/wiki/Field-programmable_gate_array" title="Field-programmable gate array">programmed</a> to compute the operation than specified in software and performed on a general-purpose <a href="/wiki/Processor_(computing)" title="Processor (computing)">computer processor</a>.  Each approach has advantages and disadvantages.  The implementation of <a href="/wiki/Task_(computing)" title="Task (computing)">computing tasks</a> in hardware to decrease <a href="/wiki/Latency_(engineering)" title="Latency (engineering)">latency</a> and increase <a href="/wiki/Throughput#Integrated_Circuits" title="Throughput">throughput</a> is known as <b>hardware acceleration</b>.\n</p>, <p>Typical advantages of software include more rapid <a href="/wiki/Software_development_process" title="Software development process">development</a> (leading to faster <a href="/wiki/Time_to_market" title="Time to market">times to market</a>), lower <a href="/wiki/Non-recurring_engineering" title="Non-recurring engineering">non-recurring engineering</a> costs, heightened <a href="/wiki/Software_portability" title="Software portability">portability</a>, and ease of <a href="/wiki/Software_release_life_cycle" title="Software release life cycle">updating features</a> or <a href="/wiki/Patch_(computing)" title="Patch (computing)">patching</a> <a href="/wiki/Software_bug" title="Software bug">bugs</a>, at the cost of <a href="/wiki/Overhead_(computing)" title="Overhead (computing)">overhead</a> to <a href="/wiki/Instruction_cycle" title="Instruction cycle">compute</a> general operations. Advantages of hardware include <a href="/wiki/Speedup" title="Speedup">speedup</a>, reduced <a href="/wiki/Electric_energy_consumption" title="Electric energy consumption">power consumption</a>,<sup class="reference" id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup> lower <a href="/wiki/Latency_(engineering)" title="Latency (engineering)">latency</a>, increased <a href="/wiki/Parallel_computing" title="Parallel computing">parallelism</a><sup class="reference" id="cite_ref-2"><a href="#cite_note-2">[2]</a></sup> and <a href="/wiki/Bandwidth_(computing)" title="Bandwidth (computing)">bandwidth</a>, and <a href="/wiki/Circuit_underutilization" title="Circuit underutilization">better utilization</a> of area and <a class="mw-redirect" href="/wiki/Functional_unit" title="Functional unit">functional components</a> available on an <a href="/wiki/Integrated_circuit" title="Integrated circuit">integrated circuit</a>; at the cost of lower ability to update designs once <a href="/wiki/Semiconductor_device_fabrication" title="Semiconductor device fabrication">etched onto silicon</a> and higher costs of <a href="/wiki/Functional_verification" title="Functional verification">functional verification</a> and times to market. In the hierarchy of digital computing systems ranging from general-purpose processors to <a href="/wiki/Full_custom" title="Full custom">fully customized</a> hardware, there is a tradeoff between flexibility and efficiency, with efficiency increasing by <a href="/wiki/Computer_performance_by_orders_of_magnitude" title="Computer performance by orders of magnitude">orders of magnitude</a> when any given application is implemented higher up that hierarchy.<sup class="reference" id="cite_ref-3"><a href="#cite_note-3">[3]</a></sup><sup class="reference" id="cite_ref-4"><a href="#cite_note-4">[4]</a></sup>  This hierarchy includes general-purpose processors such as CPUs, more <a href="/wiki/Application-specific_instruction_set_processor" title="Application-specific instruction set processor">specialized processors</a> such as GPUs, <a href="/wiki/Fixed-function" title="Fixed-function">fixed-function</a> implemented on <a href="/wiki/Field-programmable_gate_array" title="Field-programmable gate array">field-programmable gate arrays</a> (FPGAs), and fixed-function implemented on <a href="/wiki/Application-specific_integrated_circuit" title="Application-specific integrated circuit">application-specific integrated circuit</a> (ASICs). \n</p>, <p>Hardware acceleration is advantageous for <a href="/wiki/Computer_performance" title="Computer performance">performance</a>, and practical when the <a href="/wiki/Fixed-function" title="Fixed-function">functions are fixed</a> so updates are not as needed as in software solutions.  With the advent of <a href="/wiki/Reconfigurable_computing" title="Reconfigurable computing">reprogrammable</a> <a href="/wiki/Programmable_logic_device" title="Programmable logic device">logic devices</a> such as FPGAs, the restriction of hardware acceleration to fully fixed algorithms has eased since 2010, allowing hardware acceleration to be applied to problem domains requiring modification to algorithms and processing <a href="/wiki/Control_flow" title="Control flow">control flow</a>.<sup class="reference" id="cite_ref-fpgacnn_5-0"><a href="#cite_note-fpgacnn-5">[5]</a></sup><sup class="reference" id="cite_ref-BingFPGA_6-0"><a href="#cite_note-BingFPGA-6">[6]</a></sup><sup class="reference" id="cite_ref-ProjCatapult_7-0"><a href="#cite_note-ProjCatapult-7">[7]</a></sup> \n</p>, <p>Integrated circuits can be created to perform arbitrary operations on <a href="/wiki/Analog_signal" title="Analog signal">analog</a> and <a href="/wiki/Digital_signal" title="Digital signal">digital</a> signals. Most often in computing, signals are digital and can be interpreted as <a href="/wiki/Binary_number" title="Binary number">binary number</a> data. Computer hardware and software operate on information in binary representation to perform <a href="/wiki/Computing" title="Computing">computing</a>; this is accomplished by calculating <a href="/wiki/Boolean_function" title="Boolean function">boolean functions</a> on the <a href="/wiki/Bit" title="Bit">bits</a> of input and outputting the result to some <a href="/wiki/Output_device" title="Output device">output device</a> downstream for <a href="/wiki/Computer_data_storage" title="Computer data storage">storage</a> or <a class="mw-redirect" href="/wiki/Pipelining_(computing)" title="Pipelining (computing)">further processing</a>.\n</p>, <p>Either software or hardware can compute any <a href="/wiki/Computable_function" title="Computable function">computable function</a>. Custom hardware offers higher <a href="/wiki/Performance_per_watt" title="Performance per watt">performance per watt</a> for the same functions that can be specified in software. <a href="/wiki/Hardware_description_language" title="Hardware description language">Hardware description languages</a> (HDLs) such as <a href="/wiki/Verilog" title="Verilog">Verilog</a> and <a href="/wiki/VHDL" title="VHDL">VHDL</a> can model the same <a href="/wiki/Semantics_(computer_science)" title="Semantics (computer science)">semantics</a> as software and <a href="/wiki/Logic_synthesis" title="Logic synthesis">synthesize</a> the design into a <a href="/wiki/Netlist" title="Netlist">netlist</a> that can be programmed to an <a href="/wiki/Field-programmable_gate_array" title="Field-programmable gate array">FPGA</a> or composed into <a href="/wiki/Logic_gate" title="Logic gate">logic gates</a> of an <a href="/wiki/Application-specific_integrated_circuit" title="Application-specific integrated circuit">application-specific integrated circuit</a>.\n</p>, <p>The vast majority of software-based computing occurs on machines implementing the <a href="/wiki/Von_Neumann_architecture" title="Von Neumann architecture">von Neumann architecture</a>, collectively known as <a href="/wiki/Stored-program_computer" title="Stored-program computer">stored-program computers</a>. <a href="/wiki/Computer_program" title="Computer program">Computer programs</a> are <a class="mw-redirect" href="/wiki/Stored_program" title="Stored program">stored as data</a> and <a href="/wiki/Execution_(computing)" title="Execution (computing)">executed</a> by <a href="/wiki/Processor_(computing)" title="Processor (computing)">processors</a>, typically one or more <a class="mw-redirect" href="/wiki/CPU_core" title="CPU core">CPU cores</a>.  Such processors must <a class="mw-redirect" href="/wiki/Instruction_fetch" title="Instruction fetch">fetch</a> and <a href="/wiki/Instruction_cycle#Decode_step" title="Instruction cycle">decode</a> instructions <a class="mw-redirect" href="/wiki/Load/store_architecture" title="Load/store architecture">as well as data operands</a> from <a href="/wiki/Computer_memory" title="Computer memory">memory</a> as part of the <a href="/wiki/Instruction_cycle" title="Instruction cycle">instruction cycle</a> to execute the instructions constituting the software program.  Relying on a common <a href="/wiki/CPU_cache" title="CPU cache">cache</a> for code and data leads to the <a class="mw-redirect" href="/wiki/Von_Neumann_bottleneck" title="Von Neumann bottleneck">von Neumann bottleneck</a>, a fundamental limitation on the throughput of software on processors implementing the von Neumann architecture.  Even in the <a href="/wiki/Modified_Harvard_architecture" title="Modified Harvard architecture">modified Harvard architecture</a>, where instructions and data have separate <a href="/wiki/Cache_(computing)" title="Cache (computing)">caches</a> in the <a href="/wiki/Memory_hierarchy" title="Memory hierarchy">memory hierarchy</a>, there is <a href="/wiki/Overhead_(computing)" title="Overhead (computing)">overhead</a> to decoding instruction <a href="/wiki/Opcode" title="Opcode">opcodes</a> and <a href="/wiki/Multiplexer" title="Multiplexer">multiplexing</a> available <a href="/wiki/Execution_unit" title="Execution unit">execution units</a> on a <a href="/wiki/Microprocessor" title="Microprocessor">microprocessor</a> or <a href="/wiki/Microcontroller" title="Microcontroller">microcontroller</a>, leading to <a href="/wiki/Circuit_underutilization" title="Circuit underutilization">low circuit utilization</a>.  <a href="/wiki/Intel" title="Intel">Intel</a>\'s <a href="/wiki/Hyper-threading" title="Hyper-threading">hyper-threading</a> technology provides <a href="/wiki/Simultaneous_multithreading" title="Simultaneous multithreading">simultaneous multithreading</a> by exploiting under-utilization of available processor functional units and <a class="mw-redirect" href="/wiki/Instruction_level_parallelism" title="Instruction level parallelism">instruction level parallelism</a> between different <a class="mw-redirect" href="/wiki/Hardware_thread" title="Hardware thread">hardware threads</a>.\n</p>, <p>Hardware <a href="/wiki/Execution_unit" title="Execution unit">execution units</a> do not in general rely on the von Neumann or modified Harvard architectures and do not need to perform the instruction fetch and decode steps of an <a href="/wiki/Instruction_cycle" title="Instruction cycle">instruction cycle</a> and incur those stages\' overhead. If needed calculations are specified in a <a class="mw-redirect" href="/wiki/Register_transfer_level" title="Register transfer level">register transfer level</a> hardware design, the time and circuit area costs that would be incurred by instruction fetch and decoding stages can be reclaimed and put to other uses. \n</p>, <p>This reclamation saves time, power and circuit area in computation. The reclaimed resources can be used for increased parallel computation, other functions, communication or memory, as well as increased <a href="/wiki/Input/output" title="Input/output">input/output</a> capabilities. This comes at the <a href="/wiki/Opportunity_cost" title="Opportunity cost">opportunity cost</a> of less general-purpose utility. \n</p>, <p>Greater RTL customization of hardware designs allows emerging architectures such as <a href="/wiki/In-memory_processing" title="In-memory processing">in-memory computing</a>, <a href="/wiki/Transport_triggered_architecture" title="Transport triggered architecture">transport triggered architectures</a> (TTA) and <a href="/wiki/Network_on_a_chip" title="Network on a chip">networks-on-chip</a> (NoC) to further benefit from increased <a href="/wiki/Locality_of_reference" title="Locality of reference">locality</a> of data to execution context, thereby reducing computing and <a class="mw-redirect" href="/wiki/Communication_latency" title="Communication latency">communication latency</a> between <a href="/wiki/Modularity" title="Modularity">modules</a> and functional units. \n</p>, <p>Custom hardware is limited in <a href="/wiki/Parallel_processing_(DSP_implementation)" title="Parallel processing (DSP implementation)">parallel processing</a> capability only by the area and <a href="/wiki/Logic_block" title="Logic block">logic blocks</a> available on the <a href="/wiki/Die_(integrated_circuit)" title="Die (integrated circuit)">integrated circuit die</a>.<sup class="reference" id="cite_ref-8"><a href="#cite_note-8">[8]</a></sup>  Therefore, hardware is much more free to offer <a class="mw-redirect" href="/wiki/Massive_parallelism_(computing)" title="Massive parallelism (computing)">massive parallelism</a> than software on general-purpose processors, offering a possibility of implementing the <a href="/wiki/Parallel_random-access_machine" title="Parallel random-access machine">parallel random-access machine</a> (PRAM) model.\n</p>, <p>It is common to build <a href="/wiki/Multi-core_processor" title="Multi-core processor">multicore</a> and <a href="/wiki/Manycore_processor" title="Manycore processor">manycore</a> processing units out of <a href="/wiki/Soft_microprocessor" title="Soft microprocessor">microprocessor IP core schematics</a> on a single FPGA or ASIC.<sup class="reference" id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup><sup class="reference" id="cite_ref-10"><a href="#cite_note-10">[10]</a></sup><sup class="reference" id="cite_ref-11"><a href="#cite_note-11">[11]</a></sup><sup class="reference" id="cite_ref-12"><a href="#cite_note-12">[12]</a></sup><sup class="reference" id="cite_ref-13"><a href="#cite_note-13">[13]</a></sup> Similarly, specialized <a href="/wiki/Execution_unit" title="Execution unit">functional units</a> can be composed in parallel as <a href="/wiki/Parallel_processing_(DSP_implementation)" title="Parallel processing (DSP implementation)">in digital signal processing</a> without being embedded in a processor <a href="/wiki/Semiconductor_intellectual_property_core" title="Semiconductor intellectual property core">IP core</a>.  Therefore, hardware acceleration is often employed for repetitive, fixed <a href="/wiki/Task_(computing)" title="Task (computing)">tasks</a> involving little <a href="/wiki/Conditional_(computer_programming)" title="Conditional (computer programming)">conditional branching</a>, especially on large amounts of data. This is how <a href="/wiki/Nvidia" title="Nvidia">Nvidia</a>\'s <a href="/wiki/CUDA" title="CUDA">CUDA</a> line of <a href="/wiki/Graphics_processing_unit" title="Graphics processing unit">GPUs</a> are implemented.\n</p>, <p>As device mobility has increased, the relative performance of specific acceleration protocols has required new metricizations, considering the characteristics such as physical hardware dimensions, power consumption and operations throughput. These can be summarized into three categories: task efficiency, implementation efficiency, and flexibility. Appropriate metrics consider the area of the hardware along with both the corresponding operations throughput and energy consumed.<sup class="reference" id="cite_ref-14"><a href="#cite_note-14">[14]</a></sup>\n</p>, <p>\nSuppose we wish to compute the sum of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\displaystyle 2^{20}=1,048,576}" xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <msup>\n          <mn>2</mn>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mn>20</mn>\n          </mrow>\n        </msup>\n        <mo>=</mo>\n        <mn>1</mn>\n        <mo>,</mo>\n        <mn>048</mn>\n        <mo>,</mo>\n        <mn>576</mn>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle 2^{20}=1,048,576}</annotation>\n  </semantics>\n</math></span><img alt="{\\displaystyle 2^{20}=1,048,576}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a89d939253f6aad9cca2aaccf8883e5c3846cff9" style="vertical-align: -0.671ex; width:16.342ex; height:3.009ex;"/></span> <a href="/wiki/Integer_(computer_science)" title="Integer (computer science)">integers</a>. Assuming <a href="/wiki/Arbitrary-precision_arithmetic" title="Arbitrary-precision arithmetic">large integers</a> are available as <code>bignum</code> <a href="/wiki/Precision_(computer_science)" title="Precision (computer science)">large enough</a> to hold the sum, this can be done in software by specifying (here, in <a href="/wiki/C%2B%2B" title="C++">C++</a>): </p>, <p>This algorithm runs in <a class="mw-redirect" href="/wiki/Linear_time" title="Linear time">linear time</a>, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\textstyle {\\mathcal {O}}\\left(n\\right)}" xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="false" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi class="MJX-tex-caligraphic" mathvariant="script">O</mi>\n          </mrow>\n        </mrow>\n        <mrow>\n          <mo>(</mo>\n          <mi>n</mi>\n          <mo>)</mo>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\textstyle {\\mathcal {O}}\\left(n\\right)}</annotation>\n  </semantics>\n</math></span><img alt="{\\textstyle {\\mathcal {O}}\\left(n\\right)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c3208fffca5cbfaa40b24603202a71f933c60645" style="vertical-align: -0.838ex; width:5.441ex; height:2.843ex;"/></span> in <a href="/wiki/Big_O_notation" title="Big O notation">Big O notation</a>. In hardware, with sufficient area on <a href="/wiki/Integrated_circuit" title="Integrated circuit">chip</a>, calculation can be parallelized to take only 20 <a class="mw-redirect" href="/wiki/Clock_cycle" title="Clock cycle">time steps</a> using the <a href="/wiki/Prefix_sum" title="Prefix sum">prefix sum</a> algorithm.<sup class="reference" id="cite_ref-hs1986_15-0"><a href="#cite_note-hs1986-15">[15]</a></sup> The algorithm requires only <a class="mw-redirect" href="/wiki/Logarithmic_time" title="Logarithmic time">logarithmic time</a>, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\textstyle {\\mathcal {O}}\\left(\\log {n}\\right)}" xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="false" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi class="MJX-tex-caligraphic" mathvariant="script">O</mi>\n          </mrow>\n        </mrow>\n        <mrow>\n          <mo>(</mo>\n          <mrow>\n            <mi>log</mi>\n            <mo>⁡<!-- \xe2\x81\xa1 --></mo>\n            <mrow class="MJX-TeXAtom-ORD">\n              <mi>n</mi>\n            </mrow>\n          </mrow>\n          <mo>)</mo>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\textstyle {\\mathcal {O}}\\left(\\log {n}\\right)}</annotation>\n  </semantics>\n</math></span><img alt="{\\textstyle {\\mathcal {O}}\\left(\\log {n}\\right)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/aa859edcafecab87c7eaaf8da32562d51f6f55bb" style="vertical-align: -0.838ex; width:8.8ex; height:2.843ex;"/></span>, and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\\textstyle {\\mathcal {O}}\\left(1\\right)}" xmlns="http://www.w3.org/1998/Math/MathML">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="false" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi class="MJX-tex-caligraphic" mathvariant="script">O</mi>\n          </mrow>\n        </mrow>\n        <mrow>\n          <mo>(</mo>\n          <mn>1</mn>\n          <mo>)</mo>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\textstyle {\\mathcal {O}}\\left(1\\right)}</annotation>\n  </semantics>\n</math></span><img alt="{\\textstyle {\\mathcal {O}}\\left(1\\right)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e56df1b5ccdf566b792f896c8c6b490fc9f40162" style="vertical-align: -0.838ex; width:5.209ex; height:2.843ex;"/></span> <a class="mw-redirect" href="/wiki/Space_complexity" title="Space complexity">space</a> as an <a href="/wiki/In-place_algorithm" title="In-place algorithm">in-place algorithm</a>:</p>, <p>This example takes advantage of the greater parallel resources available in application-specific hardware than most software and general-purpose <a class="mw-redirect" href="/wiki/Computing_paradigm" title="Computing paradigm">computing paradigms</a> and <a href="/wiki/Computer_architecture" title="Computer architecture">architectures</a>.\n</p>, <p>Hardware acceleration can be applied to <a href="/wiki/Stream_processing" title="Stream processing">stream processing</a>.\n</p>, <p>Examples of hardware acceleration include <a href="/wiki/Bit_blit" title="Bit blit">bit blit</a> acceleration functionality in <a href="/wiki/Graphics_processing_unit" title="Graphics processing unit">graphics processing units</a> (GPUs), use of <a href="/wiki/Memristor" title="Memristor">memristors</a> for accelerating <a href="/wiki/Artificial_neural_network" title="Artificial neural network">neural networks</a><sup class="reference" id="cite_ref-PIM_NN_Survey_16-0"><a href="#cite_note-PIM_NN_Survey-16">[16]</a></sup> and <a href="/wiki/Regular_expression" title="Regular expression">regular expression</a> hardware acceleration for <a href="/wiki/Anti-spam_techniques" title="Anti-spam techniques">spam control</a> in the <a href="/wiki/Server_(computing)" title="Server (computing)">server</a> industry, intended to prevent <a class="mw-redirect" href="/wiki/Regular_expression_Denial_of_Service" title="Regular expression Denial of Service">regular expression denial of service</a> (ReDoS) attacks.<sup class="reference" id="cite_ref-wellho_17-0"><a href="#cite_note-wellho-17">[17]</a></sup> The hardware that performs the acceleration may be part of a general-purpose CPU, or a separate unit. In the second case, it is referred to as a <b>hardware accelerator</b>, or often more specifically as a <a class="mw-redirect" href="/wiki/3D_accelerator" title="3D accelerator">3D accelerator</a>, <a href="/wiki/Cryptographic_accelerator" title="Cryptographic accelerator">cryptographic accelerator</a>, etc.\n</p>, <p>Traditionally, <a href="/wiki/Processor_(computing)" title="Processor (computing)">processors</a> were sequential (<a class="mw-redirect" href="/wiki/Instruction_(computing)" title="Instruction (computing)">instructions</a> are executed one by one), and were designed to run general purpose algorithms controlled by <a class="mw-redirect" href="/wiki/Instruction_fetch" title="Instruction fetch">instruction fetch</a> (for example moving temporary results <a class="mw-redirect" href="/wiki/Load/store_architecture" title="Load/store architecture">to and from</a> a <a href="/wiki/Register_file" title="Register file">register file</a>). Hardware accelerators improve the execution of a specific algorithm by allowing greater <a href="/wiki/Concurrency_(computer_science)" title="Concurrency (computer science)">concurrency</a>, having specific <a href="/wiki/Datapath" title="Datapath">datapaths</a> for their <a href="/wiki/Temporary_variable" title="Temporary variable">temporary variables</a>, and reducing the overhead of instruction control in the <a class="mw-redirect" href="/wiki/Fetch-decode-execute_cycle" title="Fetch-decode-execute cycle">fetch-decode-execute cycle</a>.\n</p>, <p>Modern processors are <a href="/wiki/Multi-core_processor" title="Multi-core processor">multi-core</a> and often feature parallel "single-instruction; multiple data" (<a href="/wiki/SIMD" title="SIMD">SIMD</a>) units. Even so, hardware acceleration still yields benefits. Hardware acceleration is suitable for any computation-intensive algorithm which is executed frequently in a task or program. Depending upon the granularity, hardware acceleration can vary from a small functional unit, to a large functional block (like <a href="/wiki/Motion_estimation" title="Motion estimation">motion estimation</a> in <a href="/wiki/MPEG-2" title="MPEG-2">MPEG-2</a>).\n</p>]