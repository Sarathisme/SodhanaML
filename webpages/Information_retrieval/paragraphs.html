[<p><b>Information retrieval</b> (<b>IR</b>) is the activity of obtaining <a href="/wiki/Information_system" title="Information system">information system</a> resources relevant to an information need from a collection of information resources.  Searches can be based on <a href="/wiki/Full-text_search" title="Full-text search">full-text</a> or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for <a href="/wiki/Metadata" title="Metadata">metadata</a> that describe data, and for databases of texts, images or sounds.\n</p>, <p>Automated information retrieval systems are used to reduce what has been called <a href="/wiki/Information_overload" title="Information overload">information overload</a>. An IR system is a software that provide access to books, journals and other documents, stores them and manages the document. <a href="/wiki/Web_search_engine" title="Web search engine">Web search engines</a> are the most visible <a href="/wiki/Information_retrieval_applications" title="Information retrieval applications">IR applications</a>.\n</p>, <p>An information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of <a href="/wiki/Relevance_(information_retrieval)" title="Relevance (information retrieval)">relevancy</a>.\n</p>, <p>An object is an entity that is represented by information in a content collection or <a href="/wiki/Database" title="Database">database</a>. User queries are matched against the database information. However, as opposed to classical SQL queries of a database, in information retrieval the results returned may or may not match the query, so results are typically ranked. This ranking of results is a key difference of information retrieval searching compared to database searching.<sup class="reference" id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup>\n</p>, <p>Depending on the <a href="/wiki/Information_retrieval_applications" title="Information retrieval applications">application</a> the data objects may be, for example, text documents, images,<sup class="reference" id="cite_ref-goodron2000_2-0"><a href="#cite_note-goodron2000-2">[2]</a></sup> audio,<sup class="reference" id="cite_ref-Foote99_3-0"><a href="#cite_note-Foote99-3">[3]</a></sup> <a class="mw-redirect" href="/wiki/Mind_maps" title="Mind maps">mind maps</a><sup class="reference" id="cite_ref-Beel2009_4-0"><a href="#cite_note-Beel2009-4">[4]</a></sup> or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or <a href="/wiki/Metadata" title="Metadata">metadata</a>.\n</p>, <p>Most IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.<sup class="reference" id="cite_ref-Frakes1992_5-0"><a href="#cite_note-Frakes1992-5">[5]</a></sup>\n</p>, <p>The idea of using computers to search for relevant pieces of information was popularized in the article <i><a href="/wiki/As_We_May_Think" title="As We May Think">As We May Think</a></i> by <a href="/wiki/Vannevar_Bush" title="Vannevar Bush">Vannevar Bush</a> in 1945.<sup class="reference" id="cite_ref-Singhal2001_6-0"><a href="#cite_note-Singhal2001-6">[6]</a></sup> It would appear that Bush was inspired by patents for a \'statistical machine\' - filed by <a href="/wiki/Emanuel_Goldberg" title="Emanuel Goldberg">Emanuel Goldberg</a> in the 1920s and \'30s - that searched for documents stored on film.<sup class="reference" id="cite_ref-Sanderson2012_7-0"><a href="#cite_note-Sanderson2012-7">[7]</a></sup> The first description of a computer searching for information was described by Holmstrom in 1948,<sup class="reference" id="cite_ref-Holmstrom1948_8-0"><a href="#cite_note-Holmstrom1948-8">[8]</a></sup> detailing an early mention of the <a class="mw-redirect" href="/wiki/Univac" title="Univac">Univac</a> computer. Automated information retrieval systems were introduced in the 1950s: one even featured in the 1957 romantic comedy, <a href="/wiki/Desk_Set" title="Desk Set">Desk Set</a>. In the 1960s, the first large information retrieval research group was formed by <a href="/wiki/Gerard_Salton" title="Gerard Salton">Gerard Salton</a> at Cornell. By the 1970s several different retrieval techniques had been shown to perform well on small <a class="mw-redirect" href="/wiki/Text_corpora" title="Text corpora">text corpora</a> such as the Cranfield collection (several thousand documents).<sup class="reference" id="cite_ref-Singhal2001_6-1"><a href="#cite_note-Singhal2001-6">[6]</a></sup> Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s.\n</p>, <p>In 1992, the US Department of Defense along with the <a href="/wiki/National_Institute_of_Standards_and_Technology" title="National Institute of Standards and Technology">National Institute of Standards and Technology</a> (NIST), cosponsored the <a href="/wiki/Text_Retrieval_Conference" title="Text Retrieval Conference">Text Retrieval Conference</a> (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that <a href="/wiki/Scalability" title="Scalability">scale</a> to huge corpora. The introduction of <a href="/wiki/Web_search_engine" title="Web search engine">web search engines</a> has boosted the need for very large scale retrieval systems even further.\n</p>, <p>For effectively retrieving relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model.\n</p>, <p>The evaluation of an information retrieval system\' is the process of assessing how well a system meets the information needs of its users. In general, measurement considers a collection of documents to be searched and a search query. Traditional evaluation metrics, designed for <a class="mw-redirect" href="/wiki/Standard_Boolean_model" title="Standard Boolean model">Boolean retrieval</a><sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="The text near this tag may need clarification or removal of jargon. (June 2018)">clarification needed</span></a></i>]</sup> or top-k retrieval, include <a href="/wiki/Precision_and_recall" title="Precision and recall">precision and recall</a>. All measures assume a <a href="/wiki/Ground_truth" title="Ground truth">ground truth</a> notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be <a class="mw-redirect" href="/wiki/Ill-posed" title="Ill-posed">ill-posed</a> and there may be different shades of relevancy.\n</p>]