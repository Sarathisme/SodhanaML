[<p>In <a href="/wiki/Computer_engineering" title="Computer engineering">computer engineering</a>, <b>computer architecture</b> is a set of rules and methods that describe the functionality, organization, and implementation of <a href="/wiki/Computer" title="Computer">computer</a> systems. Some definitions of architecture define it as describing the capabilities and programming model of a computer but not a particular implementation.<sup class="reference" id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup> In other definitions computer architecture involves <a href="/wiki/Instruction_set_architecture" title="Instruction set architecture">instruction set architecture</a> design, <a href="/wiki/Microarchitecture" title="Microarchitecture">microarchitecture</a> design, <a class="mw-redirect" href="/wiki/Logic_design" title="Logic design">logic design</a>, and <a href="/wiki/Implementation" title="Implementation">implementation</a>.<sup class="reference" id="cite_ref-2"><a href="#cite_note-2">[2]</a></sup>\n</p>, <p>The first documented computer architecture was in the correspondence between <a href="/wiki/Charles_Babbage" title="Charles Babbage">Charles Babbage</a> and <a href="/wiki/Ada_Lovelace" title="Ada Lovelace">Ada Lovelace</a>, describing the <a class="mw-redirect" href="/wiki/Analytical_engine" title="Analytical engine">analytical engine</a>. When building the computer <a href="/wiki/Z1_(computer)" title="Z1 (computer)">Z1</a> in 1936, <a href="/wiki/Konrad_Zuse" title="Konrad Zuse">Konrad Zuse</a> described in two patent applications for his future projects that machine instructions could be stored in the same storage used for data, i.e. the <a href="/wiki/Stored-program_computer" title="Stored-program computer">stored-program</a> concept.<sup class="reference" id="cite_ref-3"><a href="#cite_note-3">[3]</a></sup><sup class="reference" id="cite_ref-4"><a href="#cite_note-4">[4]</a></sup> Two other early and important examples are:\n</p>, <p>The term \xe2\x80\x9carchitecture\xe2\x80\x9d in computer literature can be traced to the work of Lyle R. Johnson and <a href="/wiki/Fred_Brooks" title="Fred Brooks">Frederick P. Brooks, Jr.</a>, members of the Machine Organization department in IBM\xe2\x80\x99s main research center in 1959. Johnson had the opportunity to write a proprietary research communication about the <a href="/wiki/IBM_7030_Stretch" title="IBM 7030 Stretch">Stretch</a>, an IBM-developed <a href="/wiki/Supercomputer" title="Supercomputer">supercomputer</a> for <a href="/wiki/Los_Alamos_National_Laboratory" title="Los Alamos National Laboratory">Los Alamos National Laboratory</a> (at the time known as Los Alamos Scientific Laboratory). To describe the level of detail for discussing the luxuriously embellished computer, he noted that his description of formats, instruction types, hardware parameters, and speed enhancements were at the level of \xe2\x80\x9csystem architecture\xe2\x80\x9d \xe2\x80\x93 a term that seemed more useful than \xe2\x80\x9cmachine organization.\xe2\x80\x9d<sup class="reference" id="cite_ref-7"><a href="#cite_note-7">[7]</a></sup>\n</p>, <p>\nSubsequently, Brooks, a Stretch designer, started Chapter 2 of a book (Planning a Computer System: Project Stretch, ed. W. Buchholz, 1962) by writing,<sup class="reference" id="cite_ref-8"><a href="#cite_note-8">[8]</a></sup> </p>, <p>Computer architecture, like other architecture, is the art of determining the needs of the user of a structure and then designing to meet those needs as effectively as possible within economic and technological constraints.\n</p>, <p>Brooks went on to help develop the <a href="/wiki/IBM_System/360" title="IBM System/360">IBM System/360</a> (now called the <a class="mw-redirect" href="/wiki/IBM_zSeries" title="IBM zSeries">IBM zSeries</a>) line of computers, in which \xe2\x80\x9carchitecture\xe2\x80\x9d became a noun defining \xe2\x80\x9cwhat the user needs to know\xe2\x80\x9d.<sup class="reference" id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup> Later, computer users came to use the term in many less-explicit ways.<sup class="reference" id="cite_ref-10"><a href="#cite_note-10">[10]</a></sup>\n</p>, <p>The earliest computer architectures were designed on paper and then directly built into the final hardware form.<sup class="reference" id="cite_ref-11"><a href="#cite_note-11">[11]</a></sup>\nLater, computer architecture prototypes were physically built in the form of a <a href="/wiki/Transistor%E2%80%93transistor_logic" title="Transistor\xe2\x80\x93transistor logic">transistor\xe2\x80\x93transistor logic</a> (TTL) computer\xe2\x80\x94such as the prototypes of the <a href="/wiki/Motorola_6800#Development_team" title="Motorola 6800">6800</a> and the <a href="/wiki/PA-RISC" title="PA-RISC">PA-RISC</a>\xe2\x80\x94tested, and tweaked, before committing to the final hardware form.\nAs of the 1990s, new computer architectures are typically "built", tested, and tweaked\xe2\x80\x94inside some other computer architecture in a <a href="/wiki/Computer_architecture_simulator" title="Computer architecture simulator">computer architecture simulator</a>; or inside a FPGA as a <a href="/wiki/Soft_microprocessor" title="Soft microprocessor">soft microprocessor</a>; or both\xe2\x80\x94before committing to the final hardware form.<sup class="reference" id="cite_ref-12"><a href="#cite_note-12">[12]</a></sup>\n</p>, <p>The discipline of computer architecture has three main subcategories:<sup class="reference" id="cite_ref-13"><a href="#cite_note-13">[13]</a></sup>\n</p>, <p>There are other types of computer architecture. The following types are used in bigger companies like Intel, and count for 1% of all of computer architecture\n</p>, <p>The purpose is to design a computer that maximizes performance while keeping power consumption in check, costs low relative to the amount of expected performance, and is also very reliable.\nFor this, many aspects are to be considered which includes instruction set design, functional organization, logic design, and implementation.\nThe implementation involves integrated circuit design, packaging, power, and cooling. Optimization of the design requires familiarity with compilers, operating systems to logic design, and packaging.<sup class="reference" id="cite_ref-15"><a href="#cite_note-15">[15]</a></sup>\n</p>, <p>An instruction set architecture (ISA) is the interface between the computer\'s software and hardware and also can be viewed as the programmer\'s view of the machine. Computers do not understand <a href="/wiki/High-level_programming_language" title="High-level programming language">high-level programming languages</a> such as Java, C++, or most programming languages used. A processor only understands instructions encoded in some numerical fashion, usually as <a class="mw-redirect" href="/wiki/Binary_numeral_system" title="Binary numeral system">binary numbers</a>. Software tools, such as <a href="/wiki/Compiler" title="Compiler">compilers</a>, translate those high level languages into instructions that the processor can understand.\n</p>, <p>Besides instructions, the ISA defines items in the computer that are available to a program—e.g. <a href="/wiki/Data_type" title="Data type">data types</a>, <a href="/wiki/Processor_register" title="Processor register">registers</a>, <a href="/wiki/Addressing_mode" title="Addressing mode">addressing modes</a>, and memory.  Instructions locate these available items with register indexes (or names) and memory addressing modes.\n</p>, <p>The ISA of a computer is usually described in a small instruction manual, which describes how the instructions are encoded. Also, it may define short (vaguely) mnemonic names for the instructions. The names can be recognized by a software development tool called an <a class="mw-redirect" href="/wiki/Assembler_(computer_programming)" title="Assembler (computer programming)">assembler</a>.  An assembler is a computer program that translates a human-readable form of the ISA into a computer-readable form.  <a href="/wiki/Disassembler" title="Disassembler">Disassemblers</a> are also widely available, usually in <a href="/wiki/Debugger" title="Debugger">debuggers</a> and software programs to isolate and correct malfunctions in binary computer programs.\n</p>, <p>ISAs vary in quality and completeness.  A good ISA compromises between programmer convenience (how easy the code is to understand), size of the code (how much code is required to do a specific action), cost of the computer to interpret the instructions (more complexity means more hardware needed to decode and execute the instructions), and speed of the computer (with more complex decoding hardware comes longer decode time).  Memory organization defines how instructions interact with the memory, and how memory interacts with itself.\n</p>, <p>During design <a href="/wiki/Emulator" title="Emulator">emulation</a> software (emulators) can run programs written in a proposed instruction set. Modern emulators can measure size, cost, and speed to determine if a particular ISA is meeting its goals.\n</p>, <p>Computer organization helps optimize performance-based products. For example, software engineers need to know the processing power of processors. They may need to optimize software in order to gain the most performance for the lowest price. This can require quite detailed analysis of the computer\'s organization.  For example, in a SD card, the designers might need to arrange the card so that the most data can be processed in the fastest possible way.\n</p>, <p>Computer organization also helps plan the selection of a processor for a particular project. Multimedia projects may need very rapid data access, while virtual machines may need fast interrupts. Sometimes certain tasks need additional components as well.  For example, a computer capable of running a virtual machine needs <a href="/wiki/Virtual_memory" title="Virtual memory">virtual memory</a> hardware so that the memory of different virtual computers can be kept separated. Computer organization and features also affect power consumption and processor cost.\n</p>, <p>Once an instruction set and micro-architecture are designed, a practical machine must be developed. This design process is called the <i>implementation</i>. Implementation is usually not considered architectural design, but rather hardware <a href="/wiki/Engineering_design_process" title="Engineering design process">design engineering</a>. Implementation can be further broken down into several steps:\n</p>, <p>For <a href="/wiki/Central_processing_unit" title="Central processing unit">CPUs</a>, the entire implementation process is organized differently and is often referred to as <a class="mw-redirect" href="/wiki/CPU_design" title="CPU design">CPU design</a>.\n</p>, <p>The exact form of a computer system depends on the constraints and goals. Computer architectures usually trade off standards, power versus performance, cost, memory capacity, <a href="/wiki/Latency_(engineering)" title="Latency (engineering)">latency</a> (latency is the amount of time that it takes for information from one node to travel to the source) and throughput. Sometimes other considerations, such as features, size, weight, reliability, and expandability are also factors.\n</p>, <p>The most common scheme does an in depth power analysis and figures out how to keep power consumption low, while maintaining adequate performance.\n</p>, <p>Modern computer performance is often described in IPC (instructions per <a href="/wiki/Clock_rate" title="Clock rate">cycle</a>).  This measures the efficiency of the architecture at any clock frequency.  Since a faster rate can make a faster computer, this is a useful measurement. Older computers had IPC counts as low as 0.1 <a href="/wiki/Instructions_per_cycle" title="Instructions per cycle">instructions per cycle</a>.  Simple modern processors easily reach near 1.  <a class="mw-redirect" href="/wiki/Superscalar" title="Superscalar">Superscalar</a> processors may reach three to five IPC by executing several instructions per clock cycle.\n</p>, <p>Counting machine language instructions would be misleading because they can do varying amounts of work in different ISAs. The "instruction" in the standard measurements is not a count of the ISA\'s actual machine language instructions, but a unit of measurement, usually based on the speed of the <a href="/wiki/VAX" title="VAX">VAX</a> computer architecture.\n</p>, <p>Many people used to measure a computer\'s speed by the clock rate (usually in MHz or GHz). This refers to the cycles per second of the main clock of the CPU. However, this metric is somewhat misleading, as a machine with a higher clock rate may not necessarily have greater performance. As a result, manufacturers have moved away from clock speed as a measure of performance.\n</p>, <p>Other factors influence speed, such as the mix of <a class="mw-redirect" href="/wiki/Functional_unit" title="Functional unit">functional units</a>, <a class="mw-redirect" href="/wiki/Computer_bus" title="Computer bus">bus</a> speeds, available memory, and the type and order of instructions in the programs.\n</p>, <p>There are two main types of speed: latency and throughput. Latency is the time between the start of a process and its completion. Throughput is the amount of work done per unit time.  <a href="/wiki/Interrupt_latency" title="Interrupt latency">Interrupt latency</a> is the guaranteed maximum response time of the system to an electronic event (like when the disk drive finishes moving some data).\n</p>, <p>Performance is affected by a very wide range of design choices \xe2\x80\x94 for example, <a href="/wiki/Pipeline_(computing)" title="Pipeline (computing)">pipelining</a> a processor usually makes latency worse, but makes throughput better. Computers that control machinery usually need low interrupt latencies. These computers operate in a <a href="/wiki/Real-time_computing" title="Real-time computing">real-time</a> environment and fail if an operation is not completed in a specified amount of time. For example, computer-controlled anti-lock brakes must begin braking within a predictable, short time after the brake pedal is sensed or else failure of the brake will occur.\n</p>, <p><a href="/wiki/Benchmark_(computing)" title="Benchmark (computing)">Benchmarking</a> takes all these factors into account by measuring the time a computer takes to run through a series of test programs. Although benchmarking shows strengths, it shouldn\'t be how you choose a computer. Often the measured machines split on different measures. For example, one system might handle scientific applications quickly, while another might render video games more smoothly. Furthermore, designers may target and add special features to their products, through hardware or software, that permit a specific benchmark to execute quickly but don\'t offer similar advantages to general tasks.\n</p>, <p>Power efficiency is another important measurement in modern computers. A higher power efficiency can often be traded for lower speed or higher cost. The typical measurement when referring to power consumption in computer architecture is MIPS/W (millions of instructions per second per watt).\n</p>, <p>Modern circuits have less power required per transistor as the number of transistors per chip grows.<sup class="reference" id="cite_ref-16"><a href="#cite_note-16">[16]</a></sup> This is because each transistor that is put in a new chip requires its own power supply and requires new pathways to be built to power it. However the number of transistors per chip is starting to increase at a slower rate. Therefore, power efficiency is starting to become as important, if not more important than fitting more and more transistors into a single chip. Recent processor designs have shown this emphasis as they put more focus on power efficiency rather than cramming as many transistors into a single chip as possible.<sup class="reference" id="cite_ref-17"><a href="#cite_note-17">[17]</a></sup> In the world of embedded computers, power efficiency has long been an important goal next to throughput and latency.\n</p>, <p>Increases in publicly released refresh rates have grown slowly over the past few years, with respect to vast leaps in power consumption reduction and miniaturization demand. This has led to a new demand for longer battery life and reductions in size due to the mobile technology being produced at a greater rate. This change in focus from greater refresh rates to power consumption and miniaturization can be shown by the significant reductions in power consumption, as much as 50%, that were  reported by Intel in their release of the <a href="/wiki/Haswell_(microarchitecture)" title="Haswell (microarchitecture)">Haswell microarchitecture</a>; where they dropped their power consumption benchmark from 30-40 watts down to 10-20 watts.<sup class="reference" id="cite_ref-18"><a href="#cite_note-18">[18]</a></sup> Comparing this to the processing speed increase of 3 GHz to 4 GHz (2002 to 2006)<sup class="reference" id="cite_ref-19"><a href="#cite_note-19">[19]</a></sup> it can be seen that the focus in research and development are shifting away from refresh rates and moving towards consuming less power and taking up less space.\n</p>]