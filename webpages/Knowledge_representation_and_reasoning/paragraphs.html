[<p><b>Knowledge representation and reasoning</b> (<b>KR</b>, <b>KR\xc2\xb2</b>, <b>KR&amp;R</b>) is the field of <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a> (AI) dedicated to representing information about the world in a form that a computer system can utilize to solve complex tasks such as <a href="/wiki/Computer-aided_diagnosis" title="Computer-aided diagnosis">diagnosing a medical condition</a> or <a class="mw-redirect" href="/wiki/Natural_language_user_interface" title="Natural language user interface">having a dialog in a natural language</a>. Knowledge representation incorporates findings from psychology<sup class="reference" id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup> about how humans solve problems and represent knowledge in order to design <a class="mw-redirect" href="/wiki/Formalism_(mathematics)" title="Formalism (mathematics)">formalisms</a> that will make complex systems easier to design and build.  Knowledge representation and reasoning also incorporates findings from <a href="/wiki/Logic" title="Logic">logic</a> to automate various kinds of <i>reasoning</i>, such as the application of rules or the relations of <a href="/wiki/Set_theory" title="Set theory">sets</a> and <a href="/wiki/Subset" title="Subset">subsets</a>.<sup class="reference" id="cite_ref-2"><a href="#cite_note-2">[2]</a></sup>\n</p>, <p>Examples of knowledge representation formalisms include <a href="/wiki/Semantic_network" title="Semantic network">semantic nets</a>, <a href="/wiki/Systems_architecture" title="Systems architecture">systems architecture</a>, <a href="/wiki/Frame_(artificial_intelligence)" title="Frame (artificial intelligence)">frames</a>, rules, and <a href="/wiki/Ontology_(information_science)" title="Ontology (information science)">ontologies</a>. Examples of <a href="/wiki/Automated_reasoning" title="Automated reasoning">automated reasoning</a> engines include <a href="/wiki/Inference_engine" title="Inference engine">inference engines</a>, <a href="/wiki/Automated_theorem_proving" title="Automated theorem proving">theorem provers</a>, and classifiers.\n</p>, <p>The KR conference series was established to share ideas and progress on this challenging field.<sup class="reference" id="cite_ref-3"><a href="#cite_note-3">[3]</a></sup>\n</p>, <p>The earliest work in computerized knowledge representation was focused on general problem solvers such as the <a href="/wiki/General_Problem_Solver" title="General Problem Solver">General Problem Solver</a> (GPS) system developed by <a href="/wiki/Allen_Newell" title="Allen Newell">Allen Newell</a> and <a href="/wiki/Herbert_A._Simon" title="Herbert A. Simon">Herbert A. Simon</a> in 1959. These systems featured data structures for planning and decomposition. The system would begin with a goal. It would then decompose that goal into sub-goals and then set out to construct strategies that could accomplish each subgoal.\n</p>, <p>In these early days of AI, general search algorithms such as <a class="mw-redirect" href="/wiki/A*" title="A*">A*</a> were also developed. However, the amorphous problem definitions for systems such as GPS meant that they worked only for very constrained toy domains (e.g. the "<a href="/wiki/Blocks_world" title="Blocks world">blocks world</a>"). In order to tackle non-toy problems, AI researchers such as <a class="mw-redirect" href="/wiki/Ed_Feigenbaum" title="Ed Feigenbaum">Ed Feigenbaum</a> and <a href="/wiki/Rick_Hayes-Roth" title="Rick Hayes-Roth">Frederick Hayes-Roth</a> realized that it was necessary to focus systems on more constrained problems.\n</p>, <p>It was the failure of these efforts that led to the <a href="/wiki/Cognitive_revolution" title="Cognitive revolution">cognitive revolution</a> in psychology and to the phase of AI focused on knowledge representation that resulted in <a class="mw-redirect" href="/wiki/Expert_systems" title="Expert systems">expert systems</a> in the 1970s and 80s, <a href="/wiki/Production_system_(computer_science)" title="Production system (computer science)">production systems</a>, <a href="/wiki/Frame_language" title="Frame language">frame languages</a>, etc. Rather than general problem solvers, AI changed its focus to expert systems that could match human competence on a specific task, such as medical diagnosis.\n</p>, <p>Expert systems gave us the terminology still in use today where AI systems are divided into a Knowledge Base with facts about the world and rules and an inference engine that applies the rules to the <a href="/wiki/Knowledge_base" title="Knowledge base">knowledge base</a> in order to answer questions and solve problems. In these early systems the knowledge base tended to be a fairly flat structure, essentially assertions about the values of variables used by the rules.<sup class="reference" id="cite_ref-4"><a href="#cite_note-4">[4]</a></sup>\n</p>, <p>In addition to expert systems, other researchers developed the concept of <a href="/wiki/Frame_language" title="Frame language">frame based languages</a> in the mid 1980s. A frame is similar to an object class: It is an abstract description of a category describing things in the world, problems, and potential solutions. Frames were originally used on systems geared toward human interaction, e.g. <a class="mw-redirect" href="/wiki/Natural_language_understanding" title="Natural language understanding">understanding natural language</a> and the social settings in which various default expectations such as ordering food in a restaurant narrow the search space and allow the system to choose appropriate responses to dynamic situations.\n</p>, <p>It wasn\'t long before the frame communities and the rule-based researchers realized that there was synergy between their approaches. Frames were good for representing the real world, described as  classes, subclasses, slots (data values) with various constraints on possible values. Rules were good for representing and utilizing complex logic such as the process to make a medical diagnosis. Integrated systems were developed that combined Frames and Rules. One of the most powerful and well known was the 1983 <a href="/wiki/Knowledge_Engineering_Environment" title="Knowledge Engineering Environment">Knowledge Engineering Environment</a> (KEE) from <a href="/wiki/IntelliCorp_(software)" title="IntelliCorp (software)">Intellicorp</a>. KEE had a complete rule engine with <a href="/wiki/Forward_chaining" title="Forward chaining">forward</a> and <a href="/wiki/Backward_chaining" title="Backward chaining">backward chaining</a>. It also had a complete frame based knowledge base with triggers, slots (data values), inheritance, and message passing. Although message passing originated in the object-oriented community rather than AI it was quickly embraced by AI researchers as well in environments such as KEE and in the operating systems for Lisp machines from <a href="/wiki/Symbolics" title="Symbolics">Symbolics</a>, <a href="/wiki/Xerox" title="Xerox">Xerox</a>, and <a href="/wiki/Texas_Instruments" title="Texas Instruments">Texas Instruments</a>.<sup class="reference" id="cite_ref-5"><a href="#cite_note-5">[5]</a></sup>\n</p>, <p>The integration of Frames, rules, and object-oriented programming was significantly driven by commercial ventures such as KEE and Symbolics spun off from various research projects. At the same time as this was occurring, there was another strain of research which was less commercially focused and was driven by mathematical logic and automated theorem proving.  One of the most influential languages in this research was the <a href="/wiki/KL-ONE" title="KL-ONE">KL-ONE</a> language of the mid 80\'s. KL-ONE was a <a href="/wiki/Frame_language" title="Frame language">frame language</a> that had a rigorous semantics, formal definitions for concepts such as an <a href="/wiki/Is-a" title="Is-a">Is-A relation</a>.<sup class="reference" id="cite_ref-6"><a href="#cite_note-6">[6]</a></sup> KL-ONE and languages that were influenced by it such as Loom had an automated reasoning engine that was based on formal logic rather than on IF-THEN rules. This reasoner is called the classifier. A classifier can analyze a set of declarations and infer new assertions, for example, redefine a class to be a subclass or superclass of some other class that wasn\'t formally specified. In this way the classifier can function as an inference engine, deducing new facts from an existing knowledge base. The classifier can also provide consistency checking on a knowledge base (which in the case of KL-ONE languages is also referred to as an Ontology).<sup class="reference" id="cite_ref-7"><a href="#cite_note-7">[7]</a></sup>\n</p>, <p>Another area of knowledge representation research was the problem of common sense reasoning. One of the first realizations learned from trying to make software that can function with human natural language was that humans regularly draw on an extensive foundation of knowledge about the real world that we simply take for granted but that is not at all obvious to an artificial agent. Basic principles of common sense physics, causality, intentions, etc. An example is the <a href="/wiki/Frame_problem" title="Frame problem">frame problem</a>, that in an event driven logic there need to be axioms that state things maintain position from one moment to the next unless they are moved by some external force. In order to make a true artificial intelligence agent that can <a class="mw-redirect" href="/wiki/Natural_language_user_interface" title="Natural language user interface">converse with humans using natural language</a> and can process basic statements and questions about the world, it is essential to represent this kind of knowledge. One of the most ambitious programs to tackle this problem was Doug Lenat\'s <a href="/wiki/Cyc" title="Cyc">Cyc</a> project. Cyc established its own Frame language and had large numbers of analysts document various areas of common sense reasoning in that language. The knowledge recorded in Cyc included common sense models of time, causality, physics, intentions, and many others.<sup class="reference" id="cite_ref-8"><a href="#cite_note-8">[8]</a></sup>\n</p>, <p>The starting point for knowledge representation is the <i>knowledge representation hypothesis</i> first formalized by <a href="/wiki/Brian_Cantwell_Smith" title="Brian Cantwell Smith">Brian C. Smith</a> in 1985:<sup class="reference" id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup>\n</p>, <p>Any mechanically embodied intelligent process will be comprised of structural ingredients that a) we as external observers naturally take to represent a propositional account of the knowledge that the overall process exhibits, and b) independent of such external semantic attribution, play a formal but causal and essential role in engendering the behavior that manifests that knowledge.</p>, <p>Currently one of the most active areas of knowledge representation research are projects associated with the <a class="mw-redirect" href="/wiki/Semantic_web" title="Semantic web">semantic web</a>. The semantic web seeks to add a layer of semantics (meaning) on top of the current Internet. Rather than indexing web sites and pages via keywords, the semantic web creates large <a class="mw-redirect" href="/wiki/Ontologies" title="Ontologies">ontologies</a> of concepts. Searching for a concept will be more effective than traditional text only searches. Frame languages and automatic classification play a big part in the vision for the future semantic web. The automatic classification gives developers technology to provide order on a constantly evolving network of knowledge. Defining ontologies that are static and incapable of evolving on the fly would be very limiting for Internet-based systems. The classifier technology provides the ability to deal with the dynamic environment of the Internet.\n</p>, <p>Recent projects funded primarily by the <a class="mw-redirect" href="/wiki/Defense_Advanced_Research_Projects_Agency" title="Defense Advanced Research Projects Agency">Defense Advanced Research Projects Agency</a> (DARPA) have integrated frame languages and classifiers with markup languages based on XML. The <a href="/wiki/Resource_Description_Framework" title="Resource Description Framework">Resource Description Framework</a> (RDF) provides the basic capability to define classes, subclasses, and properties of objects. The <a href="/wiki/Web_Ontology_Language" title="Web Ontology Language">Web Ontology Language</a> (OWL) provides additional levels of semantics and enables integration with classification engines.<sup class="reference" id="cite_ref-10"><a href="#cite_note-10">[10]</a></sup><sup class="reference" id="cite_ref-11"><a href="#cite_note-11">[11]</a></sup>\n</p>, <p>Knowledge-representation is the field of artificial intelligence that focuses on designing computer representations that capture information about the world that can be used to solve complex problems.\n</p>, <p>The justification for knowledge representation is that conventional <a class="mw-redirect" href="/wiki/Procedural_code" title="Procedural code">procedural code</a> is not the best formalism to use to solve complex problems. Knowledge representation makes complex software easier to define and maintain than procedural code and can be used in <a class="mw-redirect" href="/wiki/Expert_systems" title="Expert systems">expert systems</a>.\n</p>, <p>For example, talking to experts in terms of business rules rather than code lessens the semantic gap between users and developers and makes development of complex systems more practical.\n</p>, <p>Knowledge representation goes hand in hand with <a href="/wiki/Automated_reasoning" title="Automated reasoning">automated reasoning</a> because one of the main purposes of explicitly representing knowledge is to be able to reason about that knowledge, to make inferences, assert new knowledge, etc. Virtually all <a class="mw-redirect" href="/wiki/Knowledge_representation_language" title="Knowledge representation language">knowledge representation languages</a> have a reasoning or inference engine as part of the system.<sup class="reference" id="cite_ref-12"><a href="#cite_note-12">[12]</a></sup>\n</p>, <p>A key trade-off in the design of a knowledge representation formalism is that between expressivity and practicality. The ultimate knowledge representation formalism in terms of expressive power and compactness is <a class="mw-redirect" href="/wiki/First_Order_Logic" title="First Order Logic">First Order Logic</a> (FOL). There is no more powerful formalism than that used by mathematicians to define general propositions about the world. However, FOL has two drawbacks as a knowledge representation formalism: ease of use and practicality of implementation. First order logic can be intimidating even for many software developers. Languages which do not have the complete formal power of FOL can still provide close to the same expressive power with a user interface that is more practical for the average developer to understand. The issue of practicality of implementation is that FOL in some ways is too expressive. With FOL it is possible to create statements (e.g. quantification over infinite sets) that would cause a system to never terminate if it attempted to verify them.\n</p>, <p>Thus, a subset of FOL can be both easier to use and more practical to implement. This was a driving motivation behind rule-based expert systems. IF-THEN rules provide a subset of FOL but a very useful one that is also very intuitive. The history of most of the early AI knowledge representation formalisms; from databases to semantic nets to theorem provers and production systems can be viewed as various design decisions on whether to emphasize expressive power or computability and efficiency.<sup class="reference" id="cite_ref-13"><a href="#cite_note-13">[13]</a></sup>\n</p>, <p>In a key 1993 paper on the topic, Randall Davis of <a href="/wiki/Massachusetts_Institute_of_Technology" title="Massachusetts Institute of Technology">MIT</a> outlined five distinct roles to analyze a knowledge representation framework:<sup class="reference" id="cite_ref-14"><a href="#cite_note-14">[14]</a></sup>\n</p>, <p>Knowledge representation and reasoning are a key enabling technology for the <a class="mw-redirect" href="/wiki/Semantic_web" title="Semantic web">Semantic web</a>. Languages based on the Frame model with automatic classification provide a layer of semantics on top of the existing Internet. Rather than searching via text strings as is typical today, it will be possible to define logical queries and find pages that map to those queries.<sup class="reference" id="cite_ref-15"><a href="#cite_note-15">[15]</a></sup> The automated reasoning component in these systems is an engine known as the classifier. Classifiers focus on the <a class="mw-redirect" href="/wiki/Subsumption_relation" title="Subsumption relation">subsumption</a> relations in a knowledge base rather than rules. A classifier can infer new classes and dynamically change the ontology as new information becomes available. This capability is ideal for the ever-changing and evolving information space of the Internet.<sup class="reference" id="cite_ref-16"><a href="#cite_note-16">[16]</a></sup>\n</p>, <p>The Semantic web integrates concepts from knowledge representation and reasoning with markup languages based on XML.  The <a href="/wiki/Resource_Description_Framework" title="Resource Description Framework">Resource Description Framework</a> (RDF) provides the basic capabilities to define knowledge-based objects on the Internet with basic features such as Is-A relations and object properties. The <a href="/wiki/Web_Ontology_Language" title="Web Ontology Language">Web Ontology Language</a> (OWL) adds additional semantics and integrates with automatic classification reasoners.<sup class="reference" id="cite_ref-17"><a href="#cite_note-17">[17]</a></sup>\n</p>, <p>In 1985, Ron Brachman categorized the core issues for knowledge representation as follows:<sup class="reference" id="cite_ref-18"><a href="#cite_note-18">[18]</a></sup>\n</p>, <p>In the early years of knowledge-based systems the knowledge-bases were fairly small. The knowledge-bases that were meant to actually solve real problems rather than do proof of concept demonstrations needed to focus on well defined problems. So for example, not just medical diagnosis as a whole topic, but medical diagnosis of certain kinds of diseases.\n</p>, <p>As knowledge-based technology scaled up, the need for larger knowledge bases and for modular knowledge bases that could communicate and integrate with each other became apparent. This gave rise to the discipline of ontology engineering, designing and building large knowledge bases that could be used by multiple projects. One of the leading research projects in this area was the <a href="/wiki/Cyc" title="Cyc">Cyc</a> project. Cyc was an attempt to build a huge encyclopedic knowledge base that would contain not just expert knowledge but common sense knowledge. In designing an artificial intelligence agent, it was soon realized that representing common sense knowledge, knowledge that humans simply take for granted, was essential to make an AI that could interact with humans using natural language. Cyc was meant to address this problem. The language they defined was known as <a href="/wiki/CycL" title="CycL">CycL</a>.\n</p>, <p>After CycL, a number of <a href="/wiki/Ontology_language" title="Ontology language">ontology languages</a> have been developed. Most are <a class="mw-redirect" href="/wiki/Declarative_language" title="Declarative language">declarative languages</a>, and are either <a href="/wiki/Frame_language" title="Frame language">frame languages</a>, or are based on <a href="/wiki/First-order_logic" title="First-order logic">first-order logic</a>. Modularity\xe2\x80\x94the ability to define boundaries around specific domains and problem spaces\xe2\x80\x94is essential for these languages because as stated by Tom Gruber, "Every ontology is a treaty- a social agreement among people with common motive in sharing." There are always many competing and differing views that make any general purpose ontology impossible. A general purpose ontology would have to be applicable in any domain and different areas of knowledge need to be unified.<sup class="reference" id="cite_ref-22"><a href="#cite_note-22">[22]</a></sup>\n</p>, <p>There is a long history of work attempting to build ontologies for a variety of task domains, e.g., an ontology for liquids,<sup class="reference" id="cite_ref-23"><a href="#cite_note-23">[23]</a></sup> the <a href="/wiki/Lumped_element_model" title="Lumped element model">lumped element model</a> widely used in representing electronic circuits (e.g.,<sup class="reference" id="cite_ref-24"><a href="#cite_note-24">[24]</a></sup>), as well as ontologies for time, belief, and even programming itself. Each of these offers a way to see some part of the world.\n</p>, <p>The lumped element model, for instance, suggests that we think of circuits in terms of components with connections between them, with signals flowing instantaneously along the connections. This is a useful view, but not the only possible one. A different ontology arises if we need to attend to the electrodynamics in the device: Here signals propagate at finite speed and an object (like a resistor) that was previously viewed as a single component with an I/O behavior may now have to be thought of as an extended medium through which an electromagnetic wave flows.\n</p>, <p>Ontologies can of course be written down in a wide variety of languages and notations (e.g., logic, LISP, etc.); the essential information is not the form of that language but the content, i.e., the set of concepts offered as a way of thinking about the world. Simply put, the important part is notions like connections and components, not the choice between writing them as predicates or LISP constructs.\n</p>, <p>The commitment made selecting one or another ontology can produce a sharply different view of the task at hand. Consider the difference that arises in selecting the lumped element view of a circuit rather than the electrodynamic view of the same device. As a second example, medical diagnosis viewed in terms of rules (e.g., <a class="mw-redirect" href="/wiki/MYCIN" title="MYCIN">MYCIN</a>) looks substantially different from the same task viewed in terms of frames (e.g., INTERNIST). Where MYCIN sees the medical world as made up of empirical associations connecting symptom to disease, INTERNIST sees a set of prototypes, in particular prototypical diseases, to be matched against the case at hand.\n</p>, <p><i>See also: <a href="/wiki/Artificial_intelligence_in_fiction#Logic_machines" title="Artificial intelligence in fiction">Logic machines in fiction</a> and <a href="/wiki/List_of_fictional_computers" title="List of fictional computers">List of fictional computers</a></i>\n</p>]