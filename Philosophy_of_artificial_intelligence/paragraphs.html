[<p><a href="/wiki/Artificial_intelligence" title="Artificial intelligence">Artificial intelligence</a> has close connections with <a href="/wiki/Philosophy" title="Philosophy">philosophy</a> because both share several concepts and these include intelligence, action, <a href="/wiki/Consciousness" title="Consciousness">consciousness</a>, <a href="/wiki/Epistemology" title="Epistemology">epistemology</a>, and even <a href="/wiki/Free_will" title="Free will">free will</a>.<sup class="reference" id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup> Furthermore, the technology is concerned with the creation of artificial animals or artificial people (or, at least, artificial creatures) so the discipline is of considerable interest to philosophers.<sup class="reference" id="cite_ref-sep_2-0"><a href="#cite_note-sep-2">[2]</a></sup> These factors contributed to the emergence of the <b>philosophy of artificial intelligence.</b> Some scholars argue that the AI community\'s dismissal of philosophy is detrimental.\n</p>, <p>The philosophy of artificial intelligence attempts to answer such questions as follows:<sup class="reference" id="cite_ref-3"><a href="#cite_note-3">[3]</a></sup>\n</p>, <p>These three questions reflect the divergent interests of <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">AI researchers</a>, <a href="/wiki/Linguistics" title="Linguistics">linguists</a>, <a href="/wiki/Cognitive_science" title="Cognitive science">cognitive scientists</a> and <a href="/wiki/Philosophy" title="Philosophy">philosophers</a> respectively. The scientific answers to these questions depend on the definition of "intelligence" and "consciousness" and exactly which "machines" are under discussion.\n</p>, <p>Important <a href="/wiki/Proposition" title="Proposition">propositions</a> in the philosophy of AI include:\n</p>, <p>Is it possible to create a machine that can solve <i>all</i> the problems humans solve using their intelligence? This question defines the scope of what machines will be able to do in the future and guides the direction of AI research. It only concerns the <i>behavior</i> of machines and ignores the issues of interest to <a href="/wiki/Psychology" title="Psychology">psychologists</a>, cognitive scientists and <a href="/wiki/Philosophy" title="Philosophy">philosophers</a>; to answer this question, it does not matter whether a machine is <i>really</i> thinking (as a person thinks) or is just <i>acting like</i> it is thinking.<sup class="reference" id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup>\n</p>, <p>The basic position of most AI researchers is summed up in this statement, which appeared in the proposal for the <a href="/wiki/Dartmouth_workshop" title="Dartmouth workshop">Dartmouth workshop</a> of 1956:\n</p>, <p>Arguments against the basic premise must show that building a working AI system is impossible, because there is some practical limit to the abilities of computers or that there is some special quality of the human mind that is necessary for thinking and yet cannot be duplicated by a machine (or by the methods of current AI research).  Arguments in favor of the basic premise must show that such a system is possible.\n</p>, <p>The first step to answering the question is to clearly define "intelligence".\n</p>, <p><a href="/wiki/Alan_Turing" title="Alan Turing">Alan Turing</a><sup class="reference" id="cite_ref-11"><a href="#cite_note-11">[11]</a></sup> reduced the problem of defining intelligence to a simple question about conversation. He suggests that: if a machine can answer <i>any</i> question put to it, using the same words that an ordinary person would, then we may call that machine intelligent. A modern version of his experimental design would use an online <a href="/wiki/Chat_room" title="Chat room">chat room</a>, where one of the participants is a real person and one of the participants is a computer program. The program passes the test if no one can tell which of the two participants is human.<sup class="reference" id="cite_ref-T_4-1"><a href="#cite_note-T-4">[4]</a></sup> Turing notes that no one (except philosophers) ever asks the question "can people think?" He writes "instead of arguing continually over this point, it is usual to have a polite convention that everyone thinks".<sup class="reference" id="cite_ref-12"><a href="#cite_note-12">[12]</a></sup> Turing\'s test extends this polite convention to machines:\n</p>, <p>One criticism of the <a href="/wiki/Turing_test" title="Turing test">Turing test</a> is that it is explicitly <a class="mw-redirect" href="/wiki/Anthropomorphic" title="Anthropomorphic">anthropomorphic</a><sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (April 2017)">citation needed</span></a></i>]</sup>. If our ultimate goal is to create machines that are <i>more</i> intelligent than people, why should we insist that our machines must closely <i>resemble</i> people?<sup class="noprint Inline-Template" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Inline_citation#When_you_must_use_inline_citations" title="Wikipedia:Inline citation"><span title="The text near this tag needs a citation. (April 2017)">This quote needs a citation</span></a></i>]</sup> <a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Russell</a> and <a href="/wiki/Peter_Norvig" title="Peter Norvig">Norvig</a> write that "aeronautical engineering texts do not define the goal of their field as \'making machines that fly so exactly like pigeons that they can fool other pigeons\'".<sup class="reference" id="cite_ref-13"><a href="#cite_note-13">[13]</a></sup>\n</p>, <p>Recent A.I. research defines intelligence in terms of <a href="/wiki/Intelligent_agent" title="Intelligent agent">intelligent agents</a>. An "agent" is something which perceives and acts in an environment. A "performance measure" defines what counts as success for the agent.<sup class="reference" id="cite_ref-14"><a href="#cite_note-14">[14]</a></sup> \n</p>, <p>Definitions like this one try to capture the essence of intelligence. They have the advantage that, unlike the Turing test, they do not also test for human traits that we<sup class="noprint Inline-Template" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Manual_of_Style/Words_to_watch#Unsupported_attributions" title="Wikipedia:Manual of Style/Words to watch"><span title="The material near this tag possibly uses too-vague attribution or weasel words. (April 2017)">who?</span></a></i>]</sup> may not want to consider intelligent, like the ability to be insulted or the temptation to lie<sup class="noprint Inline-Template" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Accuracy_dispute#Disputed_statement" title="Wikipedia:Accuracy dispute"><span title="The material near this tag is possibly inaccurate or nonfactual. (April 2017)">dubious</span></a> <span class="metadata"> â€“ <a href="/wiki/Talk:Philosophy_of_artificial_intelligence#Dubious" title="Talk:Philosophy of artificial intelligence">discuss</a></span></i>]</sup>. They have the disadvantage that they fail to make the commonsense<sup class="noprint Inline-Template" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Manual_of_Style/Words_to_watch" title="Wikipedia:Manual of Style/Words to watch"><span title="The preceding text may be too imprecise or indirect for Wikipedia's standards. (April 2017)">when defined as?</span></a></i>]</sup> differentiation between "things that think" and "things that do not". By this definition, even a thermostat has a rudimentary intelligence.<sup class="reference" id="cite_ref-16"><a href="#cite_note-16">[16]</a></sup>\n</p>, <p><a href="/wiki/Hubert_Dreyfus" title="Hubert Dreyfus">Hubert Dreyfus</a> describes this argument as claiming that "if the nervous system obeys the laws of physics and chemistry, which we have every reason to suppose it does, then .... we ... ought to be able to reproduce the behavior of the nervous system with some physical device".<sup class="reference" id="cite_ref-Dreyfus_1972_p=106_17-0"><a href="#cite_note-Dreyfus_1972_p=106-17">[17]</a></sup> This argument, first introduced as early as 1943<sup class="reference" id="cite_ref-18"><a href="#cite_note-18">[18]</a></sup> and vividly described by <a href="/wiki/Hans_Moravec" title="Hans Moravec">Hans Moravec</a> in 1988,<sup class="reference" id="cite_ref-19"><a href="#cite_note-19">[19]</a></sup> is now associated with futurist <a href="/wiki/Ray_Kurzweil" title="Ray Kurzweil">Ray Kurzweil</a>, who estimates that computer power will be sufficient for a complete brain simulation by the year 2029.<sup class="reference" id="cite_ref-20"><a href="#cite_note-20">[20]</a></sup> A non-real-time simulation of a thalamocortical model that has the size of the human brain (10<sup>11</sup> neurons) was performed in 2005<sup class="reference" id="cite_ref-21"><a href="#cite_note-21">[21]</a></sup> and it took 50 days to simulate 1 second of brain dynamics on a cluster of 27 processors.\n</p>, <p>Few<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Manual_of_Style/Dates_and_numbers" title="Wikipedia:Manual of Style/Dates and numbers"><span title="This term requires quantification. (April 2017)">quantify</span></a></i>]</sup> disagree that a brain simulation is possible in theory,<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (April 2017)">citation needed</span></a></i>]</sup><sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Manual_of_Style/Words_to_watch#Unsupported_attributions" title="Wikipedia:Manual of Style/Words to watch"><span title="The material near this tag may use weasel words or too-vague attribution. (April 2017)">according to whom?</span></a></i>]</sup> even critics of AI such as Hubert Dreyfus and John Searle.<sup class="reference" id="cite_ref-22"><a href="#cite_note-22">[22]</a></sup>\nHowever, Searle points out that, in principle, <i>anything</i> can be simulated by a computer; thus, bringing the definition to its breaking point leads to the conclusion that any process at all can technically be considered "computation". "What we wanted to know is what distinguishes the mind from thermostats and livers," he writes.<sup class="reference" id="cite_ref-23"><a href="#cite_note-23">[23]</a></sup> Thus, merely mimicking the functioning of a brain would in itself be an admission of ignorance regarding intelligence and the nature of the mind<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (April 2017)">citation needed</span></a></i>]</sup>.\n</p>, <p>In 1963, <a href="/wiki/Allen_Newell" title="Allen Newell">Allen Newell</a> and <a href="/wiki/Herbert_A._Simon" title="Herbert A. Simon">Herbert A. Simon</a> proposed that "symbol manipulation" was the essence of both human and machine intelligence. They wrote: \n</p>, <p>This claim is very strong: it implies both that human thinking is a kind of symbol manipulation (because a symbol system is <i>necessary</i> for intelligence) and that machines can be intelligent (because a symbol system is <i>sufficient</i> for intelligence).<sup class="reference" id="cite_ref-24"><a href="#cite_note-24">[24]</a></sup> Another version of this position was described by philosopher Hubert Dreyfus, who called it "the psychological assumption":\n</p>, <p>A distinction is usually made<sup class="noprint Inline-Template" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Manual_of_Style/Words_to_watch#Unsupported_attributions" title="Wikipedia:Manual of Style/Words to watch"><span title="The material near this tag may use weasel words or too-vague attribution. (April 2017)">by whom?</span></a></i>]</sup> between the kind of high level symbols that directly correspond with objects in the world, such as &lt;dog&gt; and &lt;tail&gt; and the more complex "symbols" that are present in a machine like a <a href="/wiki/Neural_network" title="Neural network">neural network</a>. Early research into AI, called "good old fashioned artificial intelligence" (<a class="mw-redirect" href="/wiki/GOFAI" title="GOFAI">GOFAI</a>) by <a href="/wiki/John_Haugeland" title="John Haugeland">John Haugeland</a>, focused on these kind of high level symbols.<sup class="reference" id="cite_ref-26"><a href="#cite_note-26">[26]</a></sup>\n</p>, <p>These arguments show that human thinking does not consist (solely) of high level symbol manipulation. They do <i>not</i> show that artificial intelligence is impossible, only that more than symbol processing is required.\n</p>, <p>In 1931, <a href="/wiki/Kurt_G%C3%B6del" title="Kurt G\xc3\xb6del">Kurt G\xc3\xb6del</a> proved with an <a class="mw-redirect" href="/wiki/Incompleteness_theorem" title="Incompleteness theorem">incompleteness theorem</a> that it is always possible to construct a "G\xc3\xb6del <a href="/wiki/Statement_(logic)" title="Statement (logic)">statement</a>" that a given consistent <a href="/wiki/Formal_system" title="Formal system">formal system</a> of logic (such as a high-level symbol manipulation program) could not prove. Despite being a true statement, the constructed G\xc3\xb6del statement is unprovable in the given system. (The truth of the constructed G\xc3\xb6del statement is contingent on the consistency of the given system; applying the same process to a subtly inconsistent system will appear to succeed, but will actually yield a false "G\xc3\xb6del statement" instead.)<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (April 2017)">citation needed</span></a></i>]</sup> More speculatively, G\xc3\xb6del conjectured that the human mind can correctly eventually determine the truth or falsity of any well-grounded mathematical statement (including any possible G\xc3\xb6del statement), and that therefore the human mind\'s power is not reducible to a <i><a href="/wiki/Mechanism_(philosophy)" title="Mechanism (philosophy)">mechanism</a></i>.<sup class="reference" id="cite_ref-27"><a href="#cite_note-27">[27]</a></sup> Philosopher <a href="/wiki/John_Lucas_(philosopher)" title="John Lucas (philosopher)">John Lucas</a> (since 1961) and <a href="/wiki/Roger_Penrose" title="Roger Penrose">Roger Penrose</a> (since 1989) have championed this philosophical anti-mechanist argument.<sup class="reference" id="cite_ref-L_28-0"><a href="#cite_note-L-28">[28]</a></sup> G\xc3\xb6delian anti-mechanist arguments tend to rely on the innocuous-seeming claim that a system of human mathematicians (or some idealization of human mathematicians) is both consistent (completely free of error) and believes fully in its own consistency (and can make all logical inferences that follow from its own consistency, including belief in its G\xc3\xb6del statement)<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (April 2017)">citation needed</span></a></i>]</sup>. This is provably impossible for a Turing machine<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="The text near this tag may need clarification or removal of jargon. (April 2017)">clarification needed</span></a></i>]</sup> (and, by an informal extension, any known type of mechanical computer) to do; therefore, the G\xc3\xb6delian concludes that human reasoning is too powerful to be captured in a machine<sup class="noprint Inline-Template" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Accuracy_dispute#Disputed_statement" title="Wikipedia:Accuracy dispute"><span title="The material near this tag is possibly inaccurate or nonfactual. (April 2017)">dubious</span></a> <span class="metadata"> â€“ <a href="/wiki/Talk:Philosophy_of_artificial_intelligence#Dubious" title="Talk:Philosophy of artificial intelligence">discuss</a></span></i>]</sup>.\n</p>, <p>However, the modern consensus in the scientific and mathematical community is that actual human reasoning is inconsistent; that any consistent "idealized version" <i>H</i> of human reasoning would logically be forced to adopt a healthy but counter-intuitive open-minded skepticism about the consistency of <i>H</i> (otherwise <i>H</i> is provably inconsistent); and that G\xc3\xb6del\'s theorems do not lead to any valid argument that humans have mathematical reasoning capabilities beyond what a machine could ever duplicate.<sup class="reference" id="cite_ref-29"><a href="#cite_note-29">[29]</a></sup><sup class="reference" id="cite_ref-30"><a href="#cite_note-30">[30]</a></sup><sup class="reference" id="cite_ref-31"><a href="#cite_note-31">[31]</a></sup> This consensus that G\xc3\xb6delian anti-mechanist arguments are doomed to failure is laid out strongly in <i><a href="/wiki/Artificial_Intelligence_(journal)" title="Artificial Intelligence (journal)">Artificial Intelligence</a></i>: "<i>any</i> attempt to utilize (G\xc3\xb6del\'s incompleteness results) to attack the <a class="mw-redirect" href="/wiki/Computationalism" title="Computationalism">computationalist</a> thesis is bound to be illegitimate, since these results are quite consistent with the computationalist thesis."<sup class="reference" id="cite_ref-laforte_32-0"><a href="#cite_note-laforte-32">[32]</a></sup>\n</p>, <p>More pragmatically, Russell and Norvig note that G\xc3\xb6del\'s argument only applies to what can theoretically be proved, given an infinite amount of memory and time. In practice, real machines (including humans) have finite resources and will have difficulty proving many theorems. It is not necessary to prove everything in order to be intelligent<sup class="noprint Inline-Template" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Manual_of_Style/Words_to_watch" title="Wikipedia:Manual of Style/Words to watch"><span title="The preceding text may be too imprecise or indirect for Wikipedia's standards. (April 2017)">when defined as?</span></a></i>]</sup>.<sup class="reference" id="cite_ref-33"><a href="#cite_note-33">[33]</a></sup>\n</p>, <p>Less formally, <a href="/wiki/Douglas_Hofstadter" title="Douglas Hofstadter">Douglas Hofstadter</a>, in his <a class="mw-redirect" href="/wiki/Pulitzer_prize" title="Pulitzer prize">Pulitzer prize</a> winning book <i><a class="mw-redirect" href="/wiki/G%C3%B6del,_Escher,_Bach:_An_Eternal_Golden_Braid" title="G\xc3\xb6del, Escher, Bach: An Eternal Golden Braid">G\xc3\xb6del, Escher, Bach: An Eternal Golden Braid</a>,</i> states that these "G\xc3\xb6del-statements" always refer to the system itself, drawing an analogy to the way the <a href="/wiki/Epimenides_paradox" title="Epimenides paradox">Epimenides paradox</a> uses statements that refer to themselves, such as "this statement is false" or "I am lying".<sup class="reference" id="cite_ref-34"><a href="#cite_note-34">[34]</a></sup> But, of course, the <a href="/wiki/Epimenides_paradox" title="Epimenides paradox">Epimenides paradox</a> applies to anything that makes statements, whether they are machines <i>or</i> humans, even Lucas himself. Consider:\n</p>, <p>This statement is true but cannot be asserted by Lucas. This shows that Lucas himself is subject to the same limits that he describes for machines, as are all people, and so <a href="/wiki/John_Lucas_(philosopher)" title="John Lucas (philosopher)">Lucas</a>\'s argument is pointless.<sup class="reference" id="cite_ref-36"><a href="#cite_note-36">[36]</a></sup>\n</p>, <p>After concluding that human reasoning is non-computable, Penrose went on to controversially speculate that some kind of hypothetical non-computable processes involving the collapse of <a class="mw-redirect" href="/wiki/Quantum_mechanical" title="Quantum mechanical">quantum mechanical</a> states give humans a special advantage over existing computers. Existing quantum computers are only capable of reducing the complexity of Turing computable tasks and are still restricted to tasks within the scope of Turing machines.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (April 2017)">citation needed</span></a></i>]</sup><sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="The text near this tag may need clarification or removal of jargon. (April 2017)">clarification needed</span></a></i>]</sup>. By Penrose and Lucas\'s arguments, existing quantum computers are not sufficient<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (April 2017)">citation needed</span></a></i>]</sup><sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="The text near this tag may need clarification or removal of jargon. (April 2017)">clarification needed</span></a></i>]</sup><sup class="noprint Inline-Template" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="The reason for this is unclear. (April 2017)">why?</span></a></i>]</sup>, so Penrose seeks for some other process involving new physics, for instance quantum gravity which might manifest new physics at the scale of the <a href="/wiki/Planck_mass" title="Planck mass">Planck mass</a> via spontaneous quantum collapse of the wave function. These states, he suggested, occur both within neurons and also spanning more than one neuron.<sup class="reference" id="cite_ref-37"><a href="#cite_note-37">[37]</a></sup> However, other scientists point out that there is no plausible organic mechanism in the brain for harnessing any sort of quantum computation, and furthermore that the timescale of quantum decoherence seems too fast to influence neuron firing.<sup class="reference" id="cite_ref-38"><a href="#cite_note-38">[38]</a></sup>\n</p>, <p>Hubert Dreyfus <a class="new" href="/w/index.php?title=Argued_that_human_intelligence&amp;action=edit&amp;redlink=1" title="Argued that human intelligence (page does not exist)">Hubert Dreyfus\'s views on artificial intelligence</a> and expertise depended primarily on implicit skill  rather than explicit symbolic manipulation, and argued that these skills would never be captured in formal rules.<sup class="reference" id="cite_ref-D_39-0"><a href="#cite_note-D-39">[39]</a></sup>\n</p>, <p><a href="/wiki/Hubert_Dreyfus" title="Hubert Dreyfus">Dreyfus</a>\'s argument had been anticipated by Turing in his 1950 paper <a class="mw-redirect" href="/wiki/Computing_machinery_and_intelligence" title="Computing machinery and intelligence">Computing machinery and intelligence</a>, where he had classified this as the "argument from the informality of behavior."<sup class="reference" id="cite_ref-40"><a href="#cite_note-40">[40]</a></sup> Turing argued in response that, just because we do not know the rules that govern a complex behavior, this does not mean that no such rules exist. He wrote: "we cannot so easily convince ourselves of the absence of complete laws of behaviour ... The only way we know of for finding such laws is scientific observation, and we certainly know of no circumstances under which we could say, \'We have searched enough. There are no such laws.\'"<sup class="reference" id="cite_ref-41"><a href="#cite_note-41">[41]</a></sup>\n</p>, <p>Russell and <a href="/wiki/Peter_Norvig" title="Peter Norvig">Norvig</a> point out that, in the years since Dreyfus published his critique, progress has been made towards discovering the "rules" that govern unconscious reasoning.<sup class="reference" id="cite_ref-42"><a href="#cite_note-42">[42]</a></sup> The <a href="/wiki/Situated" title="Situated">situated</a> movement in <a href="/wiki/Robotics" title="Robotics">robotics</a> research attempts to capture our unconscious skills at perception and attention.<sup class="reference" id="cite_ref-43"><a href="#cite_note-43">[43]</a></sup> <a href="/wiki/Computational_intelligence" title="Computational intelligence">Computational intelligence</a> paradigms, such as <a class="mw-redirect" href="/wiki/Neural_net" title="Neural net">neural nets</a>, <a href="/wiki/Evolutionary_algorithm" title="Evolutionary algorithm">evolutionary algorithms</a> and so on are mostly directed at simulated unconscious reasoning and learning. <a href="/wiki/Artificial_intelligence#Statistical" title="Artificial intelligence">Statistical approaches to AI</a> can make predictions which approach the accuracy of human intuitive guesses. Research into <a class="mw-redirect" href="/wiki/Commonsense_knowledge" title="Commonsense knowledge">commonsense knowledge</a> has focused on reproducing the "background" or context of knowledge. In fact, AI research in general has moved away from high level symbol manipulation or "<a class="mw-redirect" href="/wiki/GOFAI" title="GOFAI">GOFAI</a>", towards new models that are intended to capture more of our <i>unconscious</i> reasoning<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Manual_of_Style/Words_to_watch#Unsupported_attributions" title="Wikipedia:Manual of Style/Words to watch"><span title="The material near this tag may use weasel words or too-vague attribution. (April 2017)">according to whom?</span></a></i>]</sup>. Historian and AI researcher <a href="/wiki/Daniel_Crevier" title="Daniel Crevier">Daniel Crevier</a> wrote that "time has proven the accuracy and perceptiveness of some of Dreyfus\'s comments. Had he formulated them less aggressively, constructive actions they suggested might have been taken much earlier."<sup class="reference" id="cite_ref-Crevier_1993_p=125_44-0"><a href="#cite_note-Crevier_1993_p=125-44">[44]</a></sup>\n</p>, <p>This is a philosophical question, related to the <a href="/wiki/Problem_of_other_minds" title="Problem of other minds">problem of other minds</a> and the <a href="/wiki/Hard_problem_of_consciousness" title="Hard problem of consciousness">hard problem of consciousness</a>. The question revolves around a position defined by <a href="/wiki/John_Searle" title="John Searle">John Searle</a> as "strong AI":\n</p>, <p>Searle distinguished this position from what he called "weak AI":\n</p>, <p>Searle introduced the terms to isolate strong AI from weak AI so he could focus on what he thought was the more interesting and debatable issue. He argued that <i>even if we assume</i> that we had a computer program that acted exactly like a human mind, there would still be a difficult philosophical question that needed to be answered.<sup class="reference" id="cite_ref-SWAI_7-3"><a href="#cite_note-SWAI-7">[7]</a></sup>\n</p>, <p>Neither of Searle\'s two positions are of great concern to AI research, since they do not directly answer the question "can a machine display general intelligence?" (unless it can also be shown that consciousness is <i>necessary</i> for intelligence). Turing wrote "I do not wish to give the impression that I think there is no mystery about consciousness\xe2\x80\xa6 [b]ut I do not think these mysteries necessarily need to be solved before we can answer the question [of whether machines can think]."<sup class="reference" id="cite_ref-T4_45-0"><a href="#cite_note-T4-45">[45]</a></sup> <a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Russell</a> and Norvig agree: "Most AI researchers take the weak AI hypothesis for granted, and don\'t care about the strong AI hypothesis."<sup class="reference" id="cite_ref-46"><a href="#cite_note-46">[46]</a></sup>\n</p>, <p>There are a few researchers who believe that consciousness is an essential element in intelligence, such as <a href="/wiki/Igor_Aleksander" title="Igor Aleksander">Igor Aleksander</a>, <a href="/wiki/Stan_Franklin" title="Stan Franklin">Stan Franklin</a>, <a href="/wiki/Ron_Sun" title="Ron Sun">Ron Sun</a>, and <a href="/wiki/Artificial_consciousness#Haikonen's_cognitive_architecture" title="Artificial consciousness">Pentti Haikonen</a>, although their definition of "consciousness" strays very close to "intelligence." (See <a href="/wiki/Artificial_consciousness" title="Artificial consciousness">artificial consciousness</a>.)\n</p>, <p>Before we can answer this question, we must be clear what we mean by "minds", "mental states" and "consciousness".\n</p>, <p>The words "<a href="/wiki/Mind" title="Mind">mind</a>" and "<a href="/wiki/Consciousness" title="Consciousness">consciousness</a>" are used by different communities in different ways. Some <a class="mw-redirect" href="/wiki/New_age" title="New age">new age</a> thinkers, for example, use the word "consciousness" to describe something similar to <a class="mw-redirect" href="/wiki/Bergson" title="Bergson">Bergson</a>\'s "<a href="/wiki/%C3%89lan_vital" title="\xc3\x89lan vital">\xc3\xa9lan vital</a>": an invisible, energetic fluid that permeates life and especially the mind. <a href="/wiki/Science_fiction" title="Science fiction">Science fiction</a> writers use the word to describe some <a href="/wiki/Essentialism" title="Essentialism">essential</a> property that makes us human: a machine or alien that is "conscious" will be presented as a fully human character, with intelligence, desires, will, insight, pride and so on. (Science fiction writers also use the words "sentience", "sapience," "self-awareness" or "ghost" - as in the <i><a href="/wiki/Ghost_in_the_Shell" title="Ghost in the Shell">Ghost in the Shell</a></i> manga and anime series - to describe this essential human property). For others<sup class="noprint Inline-Template" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Manual_of_Style/Words_to_watch#Unsupported_attributions" title="Wikipedia:Manual of Style/Words to watch"><span title="The material near this tag possibly uses too-vague attribution or weasel words. (April 2017)">who?</span></a></i>]</sup>, the words "mind" or "consciousness" are used as a kind of secular synonym for the <a href="/wiki/Soul" title="Soul">soul</a>.\n</p>, <p>For <a href="/wiki/Philosophy" title="Philosophy">philosophers</a>, <a href="/wiki/Neuroscience" title="Neuroscience">neuroscientists</a> and <a href="/wiki/Cognitive_science" title="Cognitive science">cognitive scientists</a>, the words are used in a way that is both more precise and more mundane: they refer to the familiar, everyday experience of having a "thought in your head", like a perception, a dream, an intention or a plan, and to the way we <i>know</i> something, or <i>mean</i> something or <i>understand</i> something<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (April 2017)">citation needed</span></a></i>]</sup>. "It\'s not hard to give a commonsense definition of consciousness" observes philosopher John Searle.<sup class="reference" id="cite_ref-47"><a href="#cite_note-47">[47]</a></sup> What is mysterious and fascinating is not so much <i>what</i> it is but <i>how</i> it is: how does a lump of fatty tissue and electricity give rise to this (familiar) experience of perceiving, meaning or thinking?\n</p>, <p><a href="/wiki/Philosopher" title="Philosopher">Philosophers</a> call this the <a href="/wiki/Hard_problem_of_consciousness" title="Hard problem of consciousness">hard problem of consciousness</a>. It is the latest version of a classic problem in the <a href="/wiki/Philosophy_of_mind" title="Philosophy of mind">philosophy of mind</a> called the "<a class="mw-redirect" href="/wiki/Mind-body_problem" title="Mind-body problem">mind-body problem</a>."<sup class="reference" id="cite_ref-48"><a href="#cite_note-48">[48]</a></sup> A related problem is the problem of <i>meaning</i> or <i>understanding</i> (which philosophers call "<a href="/wiki/Intentionality" title="Intentionality">intentionality</a>"): what is the connection between our <i>thoughts</i> and <i>what we are thinking about</i> (i.e. objects and situations out in the world)? A third issue is the problem of <i>experience</i> (or "<a href="/wiki/Phenomenology_(philosophy)" title="Phenomenology (philosophy)">phenomenology</a>"): If two people see the same thing, do they have the same experience? Or are there things "inside their head" (called "<a href="/wiki/Qualia" title="Qualia">qualia</a>") that can be different from person to person?<sup class="reference" id="cite_ref-49"><a href="#cite_note-49">[49]</a></sup>\n</p>, <p><a class="mw-redirect" href="/wiki/Neurobiologist" title="Neurobiologist">Neurobiologists</a> believe all these problems will be solved as we begin to identify the <a href="/wiki/Neural_correlates_of_consciousness" title="Neural correlates of consciousness">neural correlates of consciousness</a>: the actual relationship between the machinery in our heads and its collective properties; such as the mind, experience and understanding. Some of the harshest critics of <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a> agree that the brain is just a machine, and that consciousness and intelligence are the result of physical processes in the brain.<sup class="reference" id="cite_ref-50"><a href="#cite_note-50">[50]</a></sup> The difficult philosophical question is this: can a computer program, running on a digital machine that shuffles the binary digits of zero and one, duplicate the ability of the <a href="/wiki/Neural_correlates_of_consciousness" title="Neural correlates of consciousness">neurons</a> to create minds, with mental states (like understanding or perceiving), and ultimately, the experience of <a href="/wiki/Consciousness" title="Consciousness">consciousness</a>?\n</p>, <p><a href="/wiki/John_Searle" title="John Searle">John Searle</a> asks us to consider a <a href="/wiki/Thought_experiment" title="Thought experiment">thought experiment</a>: suppose we have written a computer program that passes the Turing test and demonstrates "general intelligent action." Suppose, specifically that the program can converse in fluent Chinese. Write the program on 3x5 cards and give them to an ordinary person who does not speak Chinese. Lock the person into a room and have him follow the instructions on the cards. He will copy out Chinese characters and pass them in and out of the room through a slot. From the outside, it will appear that the Chinese room contains a fully intelligent person who speaks Chinese. The question is this: is there anyone (or anything) in the room that understands Chinese? That is, is there anything that has the mental state of <a href="/wiki/Understanding" title="Understanding">understanding</a>, or which has <a href="/wiki/Consciousness" title="Consciousness">conscious</a> <a href="/wiki/Awareness" title="Awareness">awareness</a> of what is being discussed in Chinese? The man is clearly not aware. The room cannot be aware. The <i>cards</i> certainly aren\'t aware. Searle concludes that the <a href="/wiki/Chinese_room" title="Chinese room">Chinese room</a>, or <i>any</i> other physical symbol system, cannot have a mind.<sup class="reference" id="cite_ref-51"><a href="#cite_note-51">[51]</a></sup>\n</p>, <p>Searle goes on to argue that actual mental states and <a href="/wiki/Consciousness" title="Consciousness">consciousness</a> require (yet to be described) "actual physical-chemical properties of actual human brains."<sup class="reference" id="cite_ref-52"><a href="#cite_note-52">[52]</a></sup> He argues there are special "causal properties" of <a href="/wiki/Brain" title="Brain">brains</a> and <a href="/wiki/Neuron" title="Neuron">neurons</a> that gives rise to <a href="/wiki/Mind" title="Mind">minds</a>: in his words "brains cause minds."<sup class="reference" id="cite_ref-53"><a href="#cite_note-53">[53]</a></sup>\n</p>, <p><a class="mw-redirect" href="/wiki/Gottfried_Leibniz" title="Gottfried Leibniz">Gottfried Leibniz</a> made essentially the same argument as Searle in 1714, using the thought experiment of expanding the brain until it was the size of a <a class="mw-redirect" href="/wiki/Mill_(factory)" title="Mill (factory)">mill</a>.<sup class="reference" id="cite_ref-54"><a href="#cite_note-54">[54]</a></sup> In 1974, <a class="mw-redirect" href="/wiki/Lawrence_Davis" title="Lawrence Davis">Lawrence Davis</a> imagined duplicating the brain using telephone lines and offices staffed by people, and in 1978 <a href="/wiki/Ned_Block" title="Ned Block">Ned Block</a> envisioned the entire population of China involved in such a brain simulation. This thought experiment is called "the Chinese Nation" or "the Chinese Gym".<sup class="reference" id="cite_ref-55"><a href="#cite_note-55">[55]</a></sup> Ned Block also proposed his <a class="mw-redirect" href="/wiki/Blockhead_argument" title="Blockhead argument">Blockhead argument</a>, which is a version of the <a href="/wiki/Chinese_room" title="Chinese room">Chinese room</a> in which the program has been <a href="/wiki/Code_refactoring" title="Code refactoring">re-factored</a> into a simple set of rules of the form "see this, do that", removing all mystery from the program.\n</p>, <p>Responses to the Chinese room emphasize several different points. \n</p>, <p>The <a href="/wiki/Computational_theory_of_mind" title="Computational theory of mind">computational theory of mind</a> or "<a class="mw-redirect" href="/wiki/Computationalism" title="Computationalism">computationalism</a>" claims that the relationship between mind and brain is similar (if not identical) to the relationship between a <i>running program</i> and a computer. The idea has philosophical roots in <a class="mw-redirect" href="/wiki/Hobbes" title="Hobbes">Hobbes</a> (who claimed reasoning was "nothing more than reckoning"), <a href="/wiki/Gottfried_Wilhelm_Leibniz" title="Gottfried Wilhelm Leibniz">Leibniz</a> (who attempted to create a logical calculus of all human ideas), <a href="/wiki/David_Hume" title="David Hume">Hume</a> (who thought perception could be reduced to "atomic impressions") and even <a href="/wiki/Immanuel_Kant" title="Immanuel Kant">Kant</a> (who analyzed all experience as controlled by formal rules).<sup class="reference" id="cite_ref-62"><a href="#cite_note-62">[62]</a></sup> The latest version is associated with philosophers <a href="/wiki/Hilary_Putnam" title="Hilary Putnam">Hilary Putnam</a> and <a href="/wiki/Jerry_Fodor" title="Jerry Fodor">Jerry Fodor</a>.<sup class="reference" id="cite_ref-63"><a href="#cite_note-63">[63]</a></sup>\n</p>, <p>This question bears on our earlier questions: if the human brain is a kind of computer then computers can be both intelligent and conscious, answering both the practical and philosophical questions of AI. In terms of the practical question of AI ("Can a machine display general intelligence?"), some versions of computationalism make the claim that (as <a class="mw-redirect" href="/wiki/Hobbes" title="Hobbes">Hobbes</a> wrote):\n</p>, <p>In other words, our intelligence derives from a form of <i>calculation</i>, similar to <a href="/wiki/Arithmetic" title="Arithmetic">arithmetic</a>. This is the <a href="/wiki/Physical_symbol_system" title="Physical symbol system">physical symbol system</a> hypothesis discussed above, and it implies that artificial intelligence is possible. In terms of the philosophical question of AI ("Can a machine have mind, mental states and consciousness?"), most versions of <a class="mw-redirect" href="/wiki/Computationalism" title="Computationalism">computationalism</a> claim that (as <a href="/wiki/Stevan_Harnad" title="Stevan Harnad">Stevan Harnad</a> characterizes it):\n</p>, <p>This is John Searle\'s "strong AI" discussed above, and it is the real target of the <a href="/wiki/Chinese_room" title="Chinese room">Chinese room</a> argument (according to <a href="/wiki/Stevan_Harnad" title="Stevan Harnad">Harnad</a>).<sup class="reference" id="cite_ref-HARNAD_64-1"><a href="#cite_note-HARNAD-64">[64]</a></sup>\n</p>, <p>Alan Turing noted that there are many arguments of the form "a machine will never do X", where X can be many things, such as:\n</p>, <p>Be kind, resourceful, beautiful, friendly, have initiative, have a sense of humor, tell right from wrong, make mistakes, fall in love, enjoy strawberries and cream, make someone fall in love with it, learn from experience, use words properly, be the subject of its own thought, have as much diversity of behaviour as a man, do something really new.<sup class="reference" id="cite_ref-T5_65-0"><a href="#cite_note-T5-65">[65]</a></sup></p>, <p>Turing argues that these objections are often based on naive assumptions about the versatility of machines or are "disguised forms of the argument from consciousness". Writing a program that exhibits one of these behaviors "will not make much of an impression."<sup class="reference" id="cite_ref-T5_65-1"><a href="#cite_note-T5-65">[65]</a></sup> All of these arguments are tangential to the basic premise of AI, unless it can be shown that one of these traits is essential for general intelligence.\n</p>, <p>If "emotions" are defined only in terms of their effect on <a href="/wiki/Behaviorism" title="Behaviorism">behavior</a> or on how they <a href="/wiki/Function_(biology)" title="Function (biology)">function</a> inside an organism, then emotions can be viewed as a mechanism that an <a href="/wiki/Intelligent_agent" title="Intelligent agent">intelligent agent</a> uses to maximize the <a class="mw-redirect" href="/wiki/Utility_(economics)" title="Utility (economics)">utility</a> of its actions. Given this definition of emotion, <a href="/wiki/Hans_Moravec" title="Hans Moravec">Hans Moravec</a> believes that "robots in general will be quite emotional about being nice people".<sup class="reference" id="cite_ref-CQ266_66-0"><a href="#cite_note-CQ266-66">[66]</a></sup> Fear is a source of urgency. Empathy is a necessary component of good <a class="mw-redirect" href="/wiki/Human_computer_interaction" title="Human computer interaction">human computer interaction</a>. He says robots "will try to please you in an apparently selfless manner because it will get a thrill out of this positive reinforcement. You can interpret this as a kind of love."<sup class="reference" id="cite_ref-CQ266_66-1"><a href="#cite_note-CQ266-66">[66]</a></sup> <a href="/wiki/Daniel_Crevier" title="Daniel Crevier">Daniel Crevier</a> writes "Moravec\'s point is that emotions are just devices for channeling behavior in a direction beneficial to the survival of one\'s species."<sup class="reference" id="cite_ref-67"><a href="#cite_note-67">[67]</a></sup>\n</p>, <p>However, emotions can also be defined in terms of their <a href="/wiki/Subjectivity" title="Subjectivity">subjective</a> quality, of what it <i>feels like</i> to have an emotion. The question of whether the machine <i>actually feels</i> an emotion, or whether it merely <i>acts as if</i> it is feeling an emotion is the philosophical question, "can a machine be conscious?" in another form.<sup class="reference" id="cite_ref-T4_45-1"><a href="#cite_note-T4-45">[45]</a></sup>\n</p>, <p>"Self awareness", as noted above, is sometimes used by <a href="/wiki/Science_fiction" title="Science fiction">science fiction</a> writers as a name for the <a href="/wiki/Essentialism" title="Essentialism">essential</a> human property that makes a character fully human. <a href="/wiki/Alan_Turing" title="Alan Turing">Turing</a> strips away all other properties of human beings and reduces the question to "can a machine be the subject of its own thought?" Can it <i>think about itself</i>? Viewed in this way, a program can be written that can report on its own internal states, such as a <a href="/wiki/Debugger" title="Debugger">debugger</a>.<sup class="reference" id="cite_ref-T5_65-2"><a href="#cite_note-T5-65">[65]</a></sup>  Though arguably self-awareness often presumes a bit more capability; a machine that can ascribe meaning in some way to not only its own state but in general postulating questions without solid answers: the contextual nature of its existence now; how it compares to past states or plans for the future, the limits and value of its work product, how it perceives its performance to be valued-by or compared to others.\n</p>, <p>Turing reduces this to the question of whether a machine can "take us by surprise" and argues that this is obviously true, as any programmer can attest.<sup class="reference" id="cite_ref-68"><a href="#cite_note-68">[68]</a></sup> He notes that, with enough storage capacity, a computer can behave in an astronomical number of different ways.<sup class="reference" id="cite_ref-69"><a href="#cite_note-69">[69]</a></sup> It must be possible, even trivial, for a computer that can represent ideas to combine them in new ways. (<a href="/wiki/Douglas_Lenat" title="Douglas Lenat">Douglas Lenat</a>\'s <a href="/wiki/Automated_Mathematician" title="Automated Mathematician">Automated Mathematician</a>, as one example, combined ideas to discover new mathematical truths.) <a href="/wiki/Andreas_Kaplan" title="Andreas Kaplan">Kaplan</a> and Haenlein suggest that machines can display scientific creativity, while it seems likely that humans will have the upper hand where artistic creativity is concerned.<sup class="reference" id="cite_ref-70"><a href="#cite_note-70">[70]</a></sup>\n</p>, <p>In 2009, scientists at Aberystwyth University in Wales and the U.K\'s University of Cambridge designed a robot called Adam that they believe to be the first machine to independently come up with new scientific findings.<sup class="reference" id="cite_ref-71"><a href="#cite_note-71">[71]</a></sup> Also in 2009, researchers at <a class="mw-redirect" href="/wiki/Cornell" title="Cornell">Cornell</a> developed <a href="/wiki/Eureqa" title="Eureqa">Eureqa</a>, a computer program that extrapolates formulas to fit the data inputted, such as finding the laws of motion from a pendulum\'s motion.\n</p>, <p>This question (like many others in the philosophy of artificial intelligence) can be presented in two forms. "Hostility" can be defined in terms <a class="mw-redirect" href="/wiki/Functionalism_(philosophy)" title="Functionalism (philosophy)">function</a> or <a href="/wiki/Behaviorism" title="Behaviorism">behavior</a>, in which case "hostile" becomes synonymous with "dangerous". Or it can be defined in terms of intent: can a machine "deliberately" set out to do harm? The latter is the question "can a machine have conscious states?" (such as <a href="/wiki/Intention" title="Intention">intentions</a>) in another form.<sup class="reference" id="cite_ref-T4_45-2"><a href="#cite_note-T4-45">[45]</a></sup>\n</p>, <p>The question of whether highly intelligent and completely autonomous machines would be dangerous has been examined in detail by futurists (such as the <a class="mw-redirect" href="/wiki/Singularity_Institute" title="Singularity Institute">Singularity Institute</a>). (The obvious element of drama has also made the subject popular in <a href="/wiki/Science_fiction" title="Science fiction">science fiction</a>, which has considered many differently possible scenarios where intelligent machines pose a threat to mankind.)\n</p>, <p>One issue is that machines may acquire the autonomy and intelligence required to be dangerous very quickly. <a href="/wiki/Vernor_Vinge" title="Vernor Vinge">Vernor Vinge</a> has suggested that over just a few years, computers will suddenly become thousands or millions of times more intelligent than humans. He calls this "<a href="/wiki/Technological_singularity" title="Technological singularity">the Singularity</a>."<sup class="reference" id="cite_ref-nytimes_july09_72-0"><a href="#cite_note-nytimes_july09-72">[72]</a></sup>  He suggests that it may be somewhat or possibly very dangerous for humans.<sup class="reference" id="cite_ref-73"><a href="#cite_note-73">[73]</a></sup> This is discussed by a philosophy called <a href="/wiki/Singularitarianism" title="Singularitarianism">Singularitarianism</a>.\n</p>, <p>In 2009, academics and technical experts attended a conference to discuss the potential impact of robots and computers and the impact of the hypothetical possibility that they could become self-sufficient and able to make their own decisions. They discussed the possibility and the extent to which computers and robots might be able to acquire any level of autonomy, and to what degree they could use such abilities to possibly pose any threat or hazard. They noted that some machines have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved "cockroach intelligence." They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls.<sup class="reference" id="cite_ref-nytimes_july09_72-1"><a href="#cite_note-nytimes_july09-72">[72]</a></sup>\n</p>, <p>Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions.<sup class="reference" id="cite_ref-74"><a href="#cite_note-74">[74]</a></sup> The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions.<sup class="reference" id="cite_ref-75"><a href="#cite_note-75">[75]</a></sup><sup class="reference" id="cite_ref-76"><a href="#cite_note-76">[76]</a></sup>\n</p>, <p>The President of the <a href="/wiki/Association_for_the_Advancement_of_Artificial_Intelligence" title="Association for the Advancement of Artificial Intelligence">Association for the Advancement of Artificial Intelligence</a> has commissioned a study to look at this issue.<sup class="reference" id="cite_ref-77"><a href="#cite_note-77">[77]</a></sup> They point to programs like the <a href="/wiki/Language_Acquisition_Device_(computer)" title="Language Acquisition Device (computer)">Language Acquisition Device</a> which can emulate human interaction.\n</p>, <p>Some have suggested a need to build "<a class="mw-redirect" href="/wiki/Friendly_AI" title="Friendly AI">Friendly AI</a>", meaning that the advances which are already occurring with AI should also include an effort to make AI intrinsically friendly and humane.<sup class="reference" id="cite_ref-78"><a href="#cite_note-78">[78]</a></sup>\n</p>, <p>Finally, those who believe in the existence of a soul may argue that "Thinking is a function of man\'s immortal soul." Alan Turing called this "the theological objection". He writes\n</p>, <p>In attempting to construct such machines we should not be irreverently usurping His power of creating souls, any more than we are in the procreation of children: rather we are, in either case, instruments of His will providing mansions for the souls that He creates.<sup class="reference" id="cite_ref-79"><a href="#cite_note-79">[79]</a></sup></p>, <p>Some scholars argue that the AI community\'s dismissal of philosophy is detrimental. In the <i><a href="/wiki/Stanford_Encyclopedia_of_Philosophy" title="Stanford Encyclopedia of Philosophy">Stanford Encyclopedia of Philosophy</a></i>, some philosophers argue that  the role of philosophy in AI is underappreciated.<sup class="reference" id="cite_ref-sep_2-1"><a href="#cite_note-sep-2">[2]</a></sup> Physicist <a href="/wiki/David_Deutsch" title="David Deutsch">David Deutsch</a> argues that without an understanding of philosophy or its concepts, AI development would suffer from a lack of progress.<sup class="reference" id="cite_ref-80"><a href="#cite_note-80">[80]</a></sup>\n</p>, <p>The main bibliography on the subject, with several sub-sections, is on <a class="external text" href="https://philpapers.org/browse/philosophy-of-artificial-intelligence/" rel="nofollow">PhilPapers</a>\n</p>, <p>The main conference series on the issue is <a class="external text" href="https://www.pt-ai.org" rel="nofollow">"Philosophy and Theory of AI"</a> (PT-AI), run by <a class="external text" href="http://www.sophia.de" rel="nofollow">Vincent C. M\xc3\xbcller</a>\n</p>]