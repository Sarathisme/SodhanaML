[<p><b>Concurrent computing</b> is a form of <a href="/wiki/Computing" title="Computing">computing</a> in which several <a href="/wiki/Computation" title="Computation">computations</a> are executed during overlapping time periods\xe2\x80\x94<i><a href="/wiki/Concurrency_(computer_science)" title="Concurrency (computer science)">concurrently</a></i>\xe2\x80\x94instead of <i>sequentially</i> (one completing before the next starts). This is a property of a system\xe2\x80\x94this may be an individual <a href="/wiki/Computer_program" title="Computer program">program</a>, a <a href="/wiki/Computer" title="Computer">computer</a>, or a <a href="/wiki/Computer_network" title="Computer network">network</a>\xe2\x80\x94and there is a separate execution point or "thread of control" for each computation ("process"). A <i>concurrent system</i> is one where a computation can advance without waiting for all other computations to complete.<sup class="reference" id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup>\n</p>, <p>As a <a href="/wiki/Programming_paradigm" title="Programming paradigm">programming paradigm</a>, concurrent computing is a form of <a href="/wiki/Modular_programming" title="Modular programming">modular programming</a>, namely <a href="/wiki/Decomposition_(computer_science)" title="Decomposition (computer science)">factoring</a> an overall computation into subcomputations that may be executed concurrently. Pioneers in the field of concurrent computing include <a class="mw-redirect" href="/wiki/Edsger_Dijkstra" title="Edsger Dijkstra">Edsger Dijkstra</a>, <a href="/wiki/Per_Brinch_Hansen" title="Per Brinch Hansen">Per Brinch Hansen</a>, and <a class="mw-redirect" href="/wiki/C.A.R._Hoare" title="C.A.R. Hoare">C.A.R. Hoare</a>.\n</p>, <p>The concept of concurrent computing is frequently confused with the related but distinct concept of <a href="/wiki/Parallel_computing" title="Parallel computing">parallel computing</a>,<sup class="reference" id="cite_ref-waza_2-0"><a href="#cite_note-waza-2">[2]</a></sup><sup class="reference" id="cite_ref-3"><a href="#cite_note-3">[3]</a></sup> although both can be described as "multiple processes executing <i>during the same period of time</i>". In parallel computing, execution occurs at the same physical instant: for example, on separate <a href="/wiki/Central_processing_unit" title="Central processing unit">processors</a> of a <a class="mw-redirect" href="/wiki/Multi-processor" title="Multi-processor">multi-processor</a> machine, with the goal of speeding up computations\xe2\x80\x94parallel computing is impossible on a (<a href="/wiki/Multi-core_processor" title="Multi-core processor">one-core</a>) single processor, as only one computation can occur at any instant (during any single clock cycle).<sup class="reference" id="cite_ref-4"><a href="#cite_note-4">[a]</a></sup> By contrast, concurrent computing consists of process <i>lifetimes</i> overlapping, but execution need not happen at the same instant. The goal here is to model processes in the outside world that happen concurrently, such as multiple clients accessing a server at the same time. Structuring software systems as composed of multiple concurrent, communicating parts can be useful for tackling complexity, regardless of whether the parts can be executed in parallel.<sup class="reference" id="cite_ref-5"><a href="#cite_note-5">[4]</a></sup><sup class="reference" style="white-space:nowrap;">:<span>1</span></sup>\n</p>, <p>For example, concurrent processes can be executed on one core by interleaving the execution steps of each process via <a href="/wiki/Time-sharing" title="Time-sharing">time-sharing</a> slices: only one process runs at a time, and if it does not complete during its time slice, it is <i>paused</i>, another process begins or resumes, and then later the original process is resumed. In this way, multiple processes are part-way through execution at a single instant, but only one process is being executed at that instant.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (December 2016)">citation needed</span></a></i>]</sup>\n</p>, <p>Concurrent computations <i>may</i> be executed in parallel,<sup class="reference" id="cite_ref-waza_2-1"><a href="#cite_note-waza-2">[2]</a></sup><sup class="reference" id="cite_ref-benari2006_6-0"><a href="#cite_note-benari2006-6">[5]</a></sup> for example, by assigning each process to a separate processor or processor core, or <a href="/wiki/Distributed_computing" title="Distributed computing">distributing</a> a computation across a network. In general, however, the languages, tools, and techniques for parallel programming might not be suitable for concurrent programming, and vice versa.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (December 2016)">citation needed</span></a></i>]</sup>\n</p>, <p>The exact timing of when tasks in a concurrent system are executed depend on the <a href="/wiki/Schedule_(computer_science)" title="Schedule (computer science)">scheduling</a>, and tasks need not always be executed concurrently. For example, given two tasks, T1 and T2:<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (December 2016)">citation needed</span></a></i>]</sup>\n</p>, <p>The word "sequential" is used as an antonym for both "concurrent" and "parallel"; when these are explicitly distinguished, <i>concurrent/sequential</i> and <i>parallel/serial</i> are used as opposing pairs.<sup class="reference" id="cite_ref-FOOTNOTEPattersonHennessy2013503_7-0"><a href="#cite_note-FOOTNOTEPattersonHennessy2013503-7">[6]</a></sup> A schedule in which tasks execute one at a time (serially, no parallelism), without interleaving (sequentially, no concurrency: no task begins until the prior task ends) is called a <i>serial schedule</i>. A set of tasks that can be scheduled serially is <i><a href="/wiki/Serializability" title="Serializability">serializable</a></i>, which simplifies <a href="/wiki/Concurrency_control" title="Concurrency control">concurrency control</a>.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (December 2016)">citation needed</span></a></i>]</sup>\n</p>, <p>The main challenge in designing concurrent programs is <a href="/wiki/Concurrency_control" title="Concurrency control">concurrency control</a>: ensuring the correct sequencing of the interactions or communications between different computational executions, and coordinating access to resources that are shared among executions.<sup class="reference" id="cite_ref-benari2006_6-1"><a href="#cite_note-benari2006-6">[5]</a></sup> Potential problems include <a href="/wiki/Race_condition#Software" title="Race condition">race conditions</a>, <a href="/wiki/Deadlock" title="Deadlock">deadlocks</a>, and <a class="mw-redirect" href="/wiki/Resource_starvation" title="Resource starvation">resource starvation</a>. For example, consider the following algorithm to make withdrawals from a checking account represented by the shared resource <code>balance</code>:\n</p>, <p>Suppose <code>balance = 500</code>, and two concurrent <i>threads</i> make the calls <code>withdraw(300)</code> and <code>withdraw(350)</code>. If line 3 in both operations executes before line 5 both operations will find that <code>balance &gt;= withdrawal</code> evaluates to <code>true</code>, and execution will proceed to subtracting the withdrawal amount. However, since both processes perform their withdrawals, the total amount withdrawn will end up being more than the original balance. These sorts of problems with shared resources benefit from the use of <a href="/wiki/Concurrency_control" title="Concurrency control">concurrency control</a>, or <a href="/wiki/Non-blocking_algorithm" title="Non-blocking algorithm">non-blocking algorithms</a>.\n</p>, <p>Concurrent computing has the following advantages:\n</p>, <p>There are several models of concurrent computing, which can be used to understand and analyze concurrent systems. These models include:\n</p>, <p>A number of different methods can be used to implement concurrent programs, such as implementing each computational execution as an <a class="mw-redirect" href="/wiki/Process_(computer_science)" title="Process (computer science)">operating system process</a>, or implementing the computational processes as a set of <a class="mw-redirect" href="/wiki/Thread_(computer_science)" title="Thread (computer science)">threads</a> within a single operating system process.\n</p>, <p>In some concurrent computing systems, communication between the concurrent components is hidden from the programmer (e.g., by using <a class="mw-redirect" href="/wiki/Future_(programming)" title="Future (programming)">futures</a>), while in others it must be handled explicitly. Explicit communication can be divided into two classes:\n</p>, <p>Shared memory and message passing concurrency have different performance characteristics. Typically (although not always), the per-process memory overhead and task switching overhead is lower in a message passing system, but the overhead of message passing is greater than for a procedure call. These differences are often overwhelmed by other performance factors.\n</p>, <p>Concurrent computing developed out of earlier work on railroads and <a href="/wiki/Telegraphy" title="Telegraphy">telegraphy</a>, from the 19th and early 20th century, and some terms date to this period, such as semaphores. These arose to address the question of how to handle multiple trains on the same railroad system (avoiding collisions and maximizing efficiency) and how to handle multiple transmissions over a given set of wires (improving efficiency), such as via <a href="/wiki/Time-division_multiplexing" title="Time-division multiplexing">time-division multiplexing</a> (1870s).\n</p>, <p>The academic study of concurrent algorithms started in the 1960s, with <a href="#CITEREFDijkstra1965">Dijkstra (1965)</a> credited with being the first paper in this field, identifying and solving <a href="/wiki/Mutual_exclusion" title="Mutual exclusion">mutual exclusion</a>.<sup class="reference" id="cite_ref-8"><a href="#cite_note-8">[7]</a></sup>\n</p>, <p>Concurrency is pervasive in computing, occurring from low-level hardware on a single chip to worldwide networks. Examples follow.\n</p>, <p>At the programming language level:\n</p>, <p>At the operating system level:\n</p>, <p>At the network level, networked systems are generally concurrent by their nature, as they consist of separate devices.\n</p>, <p><a class="mw-redirect" href="/wiki/List_of_concurrent_programming_languages" title="List of concurrent programming languages">Concurrent programming languages</a> are programming languages that use language constructs for <a href="/wiki/Concurrency_(computer_science)" title="Concurrency (computer science)">concurrency</a>. These constructs may involve <a class="mw-redirect" href="/wiki/Thread_(computer_science)" title="Thread (computer science)">multi-threading</a>, support for <a href="/wiki/Distributed_computing" title="Distributed computing">distributed computing</a>, <a class="mw-redirect" href="/wiki/Message_passing_programming" title="Message passing programming">message passing</a>, <a href="/wiki/Sharing" title="Sharing">shared resources</a> (including <a class="mw-redirect" href="/wiki/Parallel_Random_Access_Machine" title="Parallel Random Access Machine">shared memory</a>) or <a href="/wiki/Futures_and_promises" title="Futures and promises">futures and promises</a>. Such languages are sometimes described as <i>concurrency-oriented languages</i> or <i>concurrency-oriented programming languages</i> (COPL).<sup class="reference" id="cite_ref-armstrong2003_9-0"><a href="#cite_note-armstrong2003-9">[8]</a></sup>\n</p>, <p>Today, the most commonly used programming languages that have specific constructs for concurrency are <a href="/wiki/Java_(programming_language)" title="Java (programming language)">Java</a> and <a href="/wiki/C_Sharp_(programming_language)" title="C Sharp (programming language)">C#</a>. Both of these languages fundamentally use a shared-memory concurrency model, with locking provided by <a href="/wiki/Monitor_(synchronization)" title="Monitor (synchronization)">monitors</a> (although message-passing models can and have been implemented on top of the underlying shared-memory model). Of the languages that use a message-passing concurrency model, <a href="/wiki/Erlang_(programming_language)" title="Erlang (programming language)">Erlang</a> is probably the most widely used in industry at present.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (August 2010)">citation needed</span></a></i>]</sup>\n</p>, <p>Many concurrent programming languages have been developed more as research languages (e.g. <a href="/wiki/Pict_(programming_language)" title="Pict (programming language)">Pict</a>) rather than as languages for production use. However, languages such as <a href="/wiki/Erlang_(programming_language)" title="Erlang (programming language)">Erlang</a>, <a href="/wiki/Limbo_(programming_language)" title="Limbo (programming language)">Limbo</a>, and <a href="/wiki/Occam_(programming_language)" title="Occam (programming language)">occam</a> have seen industrial use at various times in the last 20 years. Languages in which concurrency plays an important role include:\n</p>, <p>Many other languages provide support for concurrency in the form of libraries, at levels roughly comparable with the above list.\n</p>]